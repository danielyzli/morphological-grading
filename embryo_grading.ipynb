{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "embryo-grading.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "X1gN_XDMLzVL"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1XtKPu0njR4qQ_8FD0WsqcjyciFMjk_eE",
      "authorship_tag": "ABX9TyNoThrVMQXxO2HKkAS/lV+h",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielyzli/morphological-grading/blob/main/embryo_grading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzQ37HjdKgEX"
      },
      "source": [
        "# Dataset Exploration\n",
        "\n",
        "* ```data[index]```\n",
        "  * ```data[index]['index']``` - (type: int) true index (since there is index skipping)\n",
        "  * ```data[index]['img']```\n",
        "    * ```data[index]['img']['0']``` - (type: numpy array) focal 0 image\n",
        "    * ```data[index]['img']['-15']``` - (type: numpy array) focal -15 image\n",
        "    * ...and so on\n",
        "  * ```data[index]['label']```\n",
        "    * ```data[index]['label'][0]``` - (type: list of ints) labels based on multiple focal\n",
        "    * ```data[index]['label'][1]``` - (type: list of ints) labels based on single focal\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6afx0mlAi53"
      },
      "source": [
        "## Open dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZV_HzE9aqCK"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "with open('/content/drive/MyDrive/embryo-grading/multi-focal-project/morphology_grading_dataset_0606.pickle', 'rb') as handle:\n",
        "  data = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jiN8ol5AtNq"
      },
      "source": [
        "## Plot label distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "RlVA5J2o62dI",
        "outputId": "165c58f5-a41a-4286-eba2-ab9e668de016"
      },
      "source": [
        "labellist1, labellist2, labellist3 = [], [], []\n",
        "for i in range(len(data)):\n",
        "  labellist1.append(data[i]['label'][1][0])\n",
        "  labellist2.append(data[i]['label'][1][1])\n",
        "  labellist3.append(data[i]['label'][1][2])\n",
        "plt.figure()\n",
        "bins = [-1.5, -0.5, 0.5, 1.5, 2.5, 3.5, 4.5]\n",
        "plt.hist([labellist1, labellist2, labellist3], bins=bins, label=['label1', 'label2', 'label3'])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_8KOfV0A2Of"
      },
      "source": [
        "## Plot # images per key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "t_JIEKFzU2d_",
        "outputId": "864a21bc-6060-4c1b-b3f1-8aca431f12a8"
      },
      "source": [
        "lengthlist = []\n",
        "for i in range(len(data)):\n",
        "  lengthlist.append(len(data[i]['img'].keys()))\n",
        "plt.figure()\n",
        "plt.hist(lengthlist)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s91b3uw1P-N0"
      },
      "source": [
        "# Dummy Run Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK_ZOXBg-Hq3"
      },
      "source": [
        "## Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hhPt_2R-hXH"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import copy\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB3UaqjO-z9g"
      },
      "source": [
        "## class single_focal_net\n",
        "\n",
        "Resnet18 with three FCN outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dOo1OfVBgyh"
      },
      "source": [
        "class single_focal_net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(single_focal_net, self).__init__()\n",
        "        self.backbone_model = models.resnet18(pretrained=True)\n",
        "        self.backbone_model.fc = nn.Identity()\n",
        "        self.fc1 = nn.Linear(512, 4)\n",
        "        self.fc2 = nn.Linear(512, 3)\n",
        "        self.fc3 = nn.Linear(512, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone_model(x)\n",
        "        out1 = self.fc1(x)\n",
        "        out2 = self.fc2(x)\n",
        "        out3 = self.fc3(x)\n",
        "        return {'out1': out1, 'out2': out2, 'out3': out3}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PT4OpbhBlPf"
      },
      "source": [
        "## class mydata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A72fWWPBiH4"
      },
      "source": [
        "class mydata(Dataset):\n",
        "\n",
        "    def __init__(self, datafn, multi_image=False, transform=None):\n",
        "        # Load .mat data file\n",
        "        with open(datafn, 'rb') as handle:\n",
        "            self.data = pickle.load(handle)\n",
        "        self.multi_image_flag = multi_image\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        if not self.multi_image_flag:\n",
        "            image = self.data[idx]['img']['0']\n",
        "            labels = self.data[idx]['label'][1]\n",
        "        else:\n",
        "            # TODO\n",
        "            image = self.data[idx]['img']['0']\n",
        "            labels = self.data[idx]['label'][0]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        image = image.repeat(3, 1, 1) # turn into RGB because pretrained network expects RGB\n",
        "\n",
        "        return image, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GckHiIVFIIBW"
      },
      "source": [
        "## function focal_loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3beCQIlIg9H"
      },
      "source": [
        "def focal_loss(labels, logits, alpha, gamma):\n",
        "    bcloss = F.binary_cross_entropy_with_logits(input=logits, target=labels, reduction='none')\n",
        "    if gamma == 0.0:\n",
        "        modulator = 1.0\n",
        "    else:\n",
        "        modulator = torch.exp(-gamma * labels * logits - gamma * torch.log(1 + torch.exp(-1.0 * logits)))\n",
        "\n",
        "    loss = modulator * bcloss\n",
        "    weighted_loss = alpha * loss\n",
        "\n",
        "    return torch.sum(weighted_loss) / torch.sum(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPFYByrIIaV4"
      },
      "source": [
        "## function cb_loss\n",
        "\n",
        "Class balanced loss function wrapper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN9-Pvs0IWDo"
      },
      "source": [
        "def cb_loss(labels, logits, samples_per_class, loss_type, beta, gamma, device):\n",
        "    num_class = len(samples_per_class)\n",
        "    labels_oh = F.one_hot(labels, num_class).float()\n",
        "    weights = (1.0 - beta) / (np.array(1.0 - np.power(beta, samples_per_class)))\n",
        "    weights = weights / np.sum(weights) * num_class\n",
        "    weights = torch.tensor(weights).float()\n",
        "    weights = weights.to(device)\n",
        "    weights = weights.unsqueeze(0)\n",
        "    weights = weights.repeat(labels_oh.shape[0], 1) * labels_oh\n",
        "    weights = weights.sum(1)\n",
        "    weights = weights.unsqueeze(1)\n",
        "    weights = weights.repeat(1, num_class)\n",
        "\n",
        "    if loss_type == 'focal':\n",
        "        cb_loss = focal_loss(labels_oh, logits, weights, gamma)\n",
        "    elif loss_type == 'sigmoid':\n",
        "        cb_loss = F.binary_cross_entropy_with_logits(input=logits, target=labels_oh, weights=weights)\n",
        "    elif loss_type == 'softmax':\n",
        "        pred = logits.softmax(dim = 1)\n",
        "        cb_loss = F.binary_cross_entropy(input=pred, target=labels_oh, weight=weights)\n",
        "\n",
        "    return cb_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac-V7oXaB4hu"
      },
      "source": [
        "## function train_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2Jmffh8CIBi"
      },
      "source": [
        "def train_model(model, dataloaders, optimizer, criterion, lr_scheduler, num_epochs, samples_per_class, loss_type, loss_beta, loss_gamma, device):\n",
        "  since = time.time()\n",
        "\n",
        "  train_loss1_vec, train_loss2_vec, train_loss3_vec = [], [], []\n",
        "  val_loss1_vec, val_loss2_vec, val_loss3_vec = [], [], []\n",
        "  train_acc1_vec, train_acc2_vec, train_acc3_vec = [], [], []\n",
        "  val_acc1_vec, val_acc2_vec, val_acc3_vec = [], [], []\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    print('lr: {}'.format(optimizer.param_groups[0]['lr']))\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "      if phase == 'train':\n",
        "        model.train()  # Set model to training mode\n",
        "      else:\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "      running_loss1, running_loss2, running_loss3 = 0, 0, 0\n",
        "      running_corrects1, running_corrects2, running_corrects3 = 0, 0, 0\n",
        "\n",
        "      # Iterate over data.\n",
        "      for inputs, labels in dataloaders[phase]:\n",
        "        inputs = inputs.to(device)\n",
        "        label1 = torch.from_numpy(np.asarray(labels[0]).astype('long')).to(device)\n",
        "        label2 = torch.from_numpy(np.asarray(labels[1]).astype('long')).to(device)\n",
        "        label3 = torch.from_numpy(np.asarray(labels[2]).astype('long')).to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        # track history if only in train\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          outputs = model(inputs)\n",
        "          # loss1 = criterion(outputs['out1'], label1)\n",
        "          # loss2 = criterion(outputs['out2'], label2)\n",
        "          # loss3 = criterion(outputs['out3'], label3)\n",
        "          loss1 = cb_loss(label1, outputs['out1'], samples_per_class[0], loss_type, loss_beta, loss_gamma, device)\n",
        "          loss2 = cb_loss(label2, outputs['out2'], samples_per_class[1], loss_type, loss_beta, loss_gamma, device)\n",
        "          loss3 = cb_loss(label3, outputs['out3'], samples_per_class[2], loss_type, loss_beta, loss_gamma, device)\n",
        "          loss = (loss1 + loss2 + loss3)/3\n",
        "\n",
        "          _, preds1 = torch.max(outputs['out1'], 1)\n",
        "          _, preds2 = torch.max(outputs['out2'], 1)\n",
        "          _, preds3 = torch.max(outputs['out3'], 1)\n",
        "\n",
        "          # backward + optimize only if in training phase\n",
        "          if phase == 'train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss1 += loss1.item() * inputs.size(0)\n",
        "        running_loss2 += loss2.item() * inputs.size(0)\n",
        "        running_loss3 += loss3.item() * inputs.size(0)\n",
        "        running_corrects1 += torch.sum(preds1 == label1).cpu().numpy()\n",
        "        running_corrects2 += torch.sum(preds2 == label2).cpu().numpy()\n",
        "        running_corrects3 += torch.sum(preds3 == label3).cpu().numpy()\n",
        "\n",
        "      if phase == 'train':\n",
        "        lr_scheduler.step()\n",
        "\n",
        "      epoch_loss1 = running_loss1 / len(dataloaders[phase].sampler)\n",
        "      epoch_loss2 = running_loss2 / len(dataloaders[phase].sampler)\n",
        "      epoch_loss3 = running_loss3 / len(dataloaders[phase].sampler)\n",
        "      epoch_acc1 = running_corrects1 / len(dataloaders[phase].sampler)\n",
        "      epoch_acc2 = running_corrects2 / len(dataloaders[phase].sampler)\n",
        "      epoch_acc3 = running_corrects3 / len(dataloaders[phase].sampler)\n",
        "      print('({})\\nLoss1: {:.4f}   Loss2: {:.4f}   Loss3: {:.4f}\\nAcc1: {:.4f}   Acc2: {:.4f}   Acc3: {:.4f}'.format(phase, epoch_loss1, epoch_loss2, epoch_loss3, epoch_acc1, epoch_acc2, epoch_acc3))\n",
        "\n",
        "      # deep copy the model\n",
        "      if phase == 'val' and (epoch_acc1 + epoch_acc2 + epoch_acc3)/3 > best_acc:\n",
        "        best_acc = (epoch_acc1 + epoch_acc2 + epoch_acc3)/3\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      # record history\n",
        "      if phase == 'train':\n",
        "        train_loss1_vec.append(epoch_loss1)\n",
        "        train_loss2_vec.append(epoch_loss2)\n",
        "        train_loss3_vec.append(epoch_loss3)\n",
        "        train_acc1_vec.append(epoch_acc1)\n",
        "        train_acc2_vec.append(epoch_acc2)\n",
        "        train_acc3_vec.append(epoch_acc3)\n",
        "      else:\n",
        "        val_loss1_vec.append(epoch_loss1)\n",
        "        val_loss2_vec.append(epoch_loss2)\n",
        "        val_loss3_vec.append(epoch_loss3)\n",
        "        val_acc1_vec.append(epoch_acc1)\n",
        "        val_acc2_vec.append(epoch_acc2)\n",
        "        val_acc3_vec.append(epoch_acc3)\n",
        "\n",
        "    print()\n",
        "\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "  print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model, {'train_loss': [train_loss1_vec, train_loss2_vec, train_loss3_vec], 'train_acc': [train_acc1_vec, train_acc2_vec, train_acc3_vec], 'val_loss': [val_loss1_vec, val_loss2_vec, val_loss3_vec], 'val_acc': [val_acc1_vec, val_acc2_vec, val_acc3_vec]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twJRCU1jDF7Y"
      },
      "source": [
        "## function plot_training_curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cVfinamDV2Q"
      },
      "source": [
        "def plot_training_curve(history_dict, num_epochs):\n",
        "  titles = ['Label 1 (dev. stage)', 'Label 2 (inner cell mass)', 'Label 3 (trophectoderm)']\n",
        "  epochs = range(num_epochs)\n",
        "  for i in range(3):\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, history_dict['train_loss'][i], label='train loss')\n",
        "    plt.plot(epochs, history_dict['train_acc'][i], label='train acc')\n",
        "    plt.plot(epochs, history_dict['val_loss'][i], label='val loss')\n",
        "    plt.plot(epochs, history_dict['val_acc'][i], label='val acc')\n",
        "    plt.title(titles[i])\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epochs')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YtFW4QgDqUO"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1pg64NfD4tT"
      },
      "source": [
        "np_random_seed = 64\n",
        "datafn = '/content/drive/MyDrive/embryo-grading/multi-focal-project/morphology_grading_dataset_0606.pickle'\n",
        "batch_size = 32\n",
        "learning_rate = 0.1\n",
        "momentum = 0.9\n",
        "lr_step = 10\n",
        "lr_gamma = 0.5\n",
        "shuffle_dataset = True\n",
        "validation_split = 0.2\n",
        "trans = transforms.Compose([transforms.ToTensor(),\n",
        "                            transforms.Normalize([0.3927], [0.2723])])\n",
        "num_epochs = 100\n",
        "loss_type = 'focal'\n",
        "loss_beta = 0.001\n",
        "loss_gamma = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT1jEmgvEB5q"
      },
      "source": [
        "## Training script\n",
        "\n",
        "Dataloader structure\n",
        "* Each index of ```loader[0-(#batches-1)]``` contains:\n",
        "  * ```images[0-(batch_size-1)]``` \n",
        "  * ```labels[0,1,2]``` which contains ```label1[0-(batch_size-1)]```, ```label2[0-(batch_size-1)]```, and ```label3[0-(batch_size-1)]``` (one-hot)\n",
        "  * The last batch may not be complete i.e. have length shorter than ```batch_size```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjVSGqJtKkHW"
      },
      "source": [
        "# Dataset\n",
        "dataset = mydata(datafn, multi_image=False, transform=trans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kPZGWmz_Lx8O",
        "outputId": "93334977-2e97-450a-a069-6ca5f0684e12"
      },
      "source": [
        "# Split into train and test\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "if shuffle_dataset:\n",
        "  np.random.seed(np_random_seed)\n",
        "  np.random.shuffle(indices)\n",
        "train_indices = indices[split:]\n",
        "val_indices = indices[:split]\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "# Load data\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "# fortesting_loader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "samples_per_class = [[0, 0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
        "for _, labels in train_loader:\n",
        "  for i in range(3):\n",
        "    for j in range(len(samples_per_class[i])):\n",
        "      num_mask = np.asarray(labels[i]) == j\n",
        "      samples_per_class[i][j] += len(labels[i][num_mask])\n",
        "\n",
        "# Instantiate network\n",
        "model = single_focal_net()\n",
        "\n",
        "# Use CUDA\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# device = torch.device('cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Training setup\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_step, gamma=lr_gamma)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Run training\n",
        "model, history_dict = train_model(model, {'train': train_loader, 'val': val_loader}, optimizer, criterion, lr_scheduler, num_epochs, samples_per_class, loss_type, loss_beta, loss_gamma, device)\n",
        "\n",
        "# Visualize the training\n",
        "plot_training_curve(history_dict, num_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "lr: 0.001\n",
            "(train)\n",
            "Loss1: 1.2880   Loss2: 1.0902   Loss3: 0.8931\n",
            "Acc1: 0.7222   Acc2: 0.7296   Acc3: 0.7992\n",
            "(val)\n",
            "Loss1: 0.9149   Loss2: 0.9227   Loss3: 0.7547\n",
            "Acc1: 0.8050   Acc2: 0.7326   Acc3: 0.8161\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "lr: 0.001\n",
            "(train)\n",
            "Loss1: 0.8567   Loss2: 0.8659   Loss3: 0.7095\n",
            "Acc1: 0.8171   Acc2: 0.7575   Acc3: 0.8252\n",
            "(val)\n",
            "Loss1: 0.8093   Loss2: 0.8934   Loss3: 0.7145\n",
            "Acc1: 0.8139   Acc2: 0.7437   Acc3: 0.8235\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "lr: 0.001\n",
            "(train)\n",
            "Loss1: 0.7622   Loss2: 0.8242   Loss3: 0.6598\n",
            "Acc1: 0.8379   Acc2: 0.7724   Acc3: 0.8389\n",
            "(val)\n",
            "Loss1: 0.7418   Loss2: 0.8455   Loss3: 0.6745\n",
            "Acc1: 0.8353   Acc2: 0.7511   Acc3: 0.8486\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "lr: 0.001\n",
            "(train)\n",
            "Loss1: 0.7183   Loss2: 0.7961   Loss3: 0.6250\n",
            "Acc1: 0.8523   Acc2: 0.7781   Acc3: 0.8474\n",
            "(val)\n",
            "Loss1: 0.7210   Loss2: 0.8439   Loss3: 0.6546\n",
            "Acc1: 0.8412   Acc2: 0.7555   Acc3: 0.8545\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "lr: 0.001\n",
            "(train)\n",
            "Loss1: 0.6816   Loss2: 0.7691   Loss3: 0.5872\n",
            "Acc1: 0.8619   Acc2: 0.7914   Acc3: 0.8621\n",
            "(val)\n",
            "Loss1: 0.6727   Loss2: 0.8382   Loss3: 0.6341\n",
            "Acc1: 0.8479   Acc2: 0.7541   Acc3: 0.8641\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "lr: 0.001\n",
            "(train)\n",
            "Loss1: 0.6432   Loss2: 0.7542   Loss3: 0.5599\n",
            "Acc1: 0.8714   Acc2: 0.7966   Acc3: 0.8664\n",
            "(val)\n",
            "Loss1: 0.6505   Loss2: 0.8558   Loss3: 0.6419\n",
            "Acc1: 0.8575   Acc2: 0.7600   Acc3: 0.8464\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "lr: 0.001\n",
            "(train)\n",
            "Loss1: 0.5987   Loss2: 0.7289   Loss3: 0.5198\n",
            "Acc1: 0.8793   Acc2: 0.8023   Acc3: 0.8782\n",
            "(val)\n",
            "Loss1: 0.6304   Loss2: 0.8601   Loss3: 0.6081\n",
            "Acc1: 0.8663   Acc2: 0.7592   Acc3: 0.8582\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "lr: 0.001\n",
            "(train)\n",
            "Loss1: 0.5719   Loss2: 0.7128   Loss3: 0.4897\n",
            "Acc1: 0.8852   Acc2: 0.8058   Acc3: 0.8854\n",
            "(val)\n",
            "Loss1: 0.6306   Loss2: 0.8713   Loss3: 0.6431\n",
            "Acc1: 0.8612   Acc2: 0.7651   Acc3: 0.8530\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "lr: 0.001\n",
            "(train)\n",
            "Loss1: 0.5553   Loss2: 0.6946   Loss3: 0.4624\n",
            "Acc1: 0.8874   Acc2: 0.8138   Acc3: 0.8977\n",
            "(val)\n",
            "Loss1: 0.6266   Loss2: 0.8566   Loss3: 0.6551\n",
            "Acc1: 0.8575   Acc2: 0.7651   Acc3: 0.8508\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "lr: 0.001\n",
            "(train)\n",
            "Loss1: 0.5285   Loss2: 0.6634   Loss3: 0.4432\n",
            "Acc1: 0.8942   Acc2: 0.8298   Acc3: 0.8992\n",
            "(val)\n",
            "Loss1: 0.6025   Loss2: 0.8506   Loss3: 0.6148\n",
            "Acc1: 0.8671   Acc2: 0.7688   Acc3: 0.8604\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "lr: 0.0005\n",
            "(train)\n",
            "Loss1: 0.4937   Loss2: 0.6390   Loss3: 0.3998\n",
            "Acc1: 0.9013   Acc2: 0.8359   Acc3: 0.9155\n",
            "(val)\n",
            "Loss1: 0.5876   Loss2: 0.8175   Loss3: 0.5730\n",
            "Acc1: 0.8737   Acc2: 0.7703   Acc3: 0.8759\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "lr: 0.0005\n",
            "(train)\n",
            "Loss1: 0.4834   Loss2: 0.6215   Loss3: 0.3808\n",
            "Acc1: 0.9014   Acc2: 0.8424   Acc3: 0.9203\n",
            "(val)\n",
            "Loss1: 0.5978   Loss2: 0.8162   Loss3: 0.5880\n",
            "Acc1: 0.8678   Acc2: 0.7792   Acc3: 0.8744\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "lr: 0.0005\n",
            "(train)\n",
            "Loss1: 0.4718   Loss2: 0.6041   Loss3: 0.3638\n",
            "Acc1: 0.9053   Acc2: 0.8459   Acc3: 0.9256\n",
            "(val)\n",
            "Loss1: 0.5819   Loss2: 0.8249   Loss3: 0.5848\n",
            "Acc1: 0.8722   Acc2: 0.7718   Acc3: 0.8730\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "lr: 0.0005\n",
            "(train)\n",
            "Loss1: 0.4529   Loss2: 0.5877   Loss3: 0.3481\n",
            "Acc1: 0.9094   Acc2: 0.8544   Acc3: 0.9276\n",
            "(val)\n",
            "Loss1: 0.5922   Loss2: 0.8222   Loss3: 0.6005\n",
            "Acc1: 0.8700   Acc2: 0.7725   Acc3: 0.8715\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "lr: 0.0005\n",
            "(train)\n",
            "Loss1: 0.4435   Loss2: 0.5725   Loss3: 0.3313\n",
            "Acc1: 0.9092   Acc2: 0.8601   Acc3: 0.9348\n",
            "(val)\n",
            "Loss1: 0.5842   Loss2: 0.8403   Loss3: 0.5855\n",
            "Acc1: 0.8708   Acc2: 0.7806   Acc3: 0.8774\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "lr: 0.0005\n",
            "(train)\n",
            "Loss1: 0.4187   Loss2: 0.5636   Loss3: 0.3103\n",
            "Acc1: 0.9166   Acc2: 0.8619   Acc3: 0.9422\n",
            "(val)\n",
            "Loss1: 0.5754   Loss2: 0.8259   Loss3: 0.5881\n",
            "Acc1: 0.8708   Acc2: 0.7777   Acc3: 0.8752\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "lr: 0.0005\n",
            "(train)\n",
            "Loss1: 0.4040   Loss2: 0.5426   Loss3: 0.3012\n",
            "Acc1: 0.9234   Acc2: 0.8762   Acc3: 0.9450\n",
            "(val)\n",
            "Loss1: 0.5891   Loss2: 0.8442   Loss3: 0.6017\n",
            "Acc1: 0.8708   Acc2: 0.7836   Acc3: 0.8730\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "lr: 0.0005\n",
            "(train)\n",
            "Loss1: 0.3919   Loss2: 0.5307   Loss3: 0.2928\n",
            "Acc1: 0.9240   Acc2: 0.8793   Acc3: 0.9452\n",
            "(val)\n",
            "Loss1: 0.5938   Loss2: 0.8559   Loss3: 0.6290\n",
            "Acc1: 0.8693   Acc2: 0.7806   Acc3: 0.8663\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "lr: 0.0005\n",
            "(train)\n",
            "Loss1: 0.3740   Loss2: 0.5063   Loss3: 0.2736\n",
            "Acc1: 0.9302   Acc2: 0.8904   Acc3: 0.9511\n",
            "(val)\n",
            "Loss1: 0.5920   Loss2: 0.8613   Loss3: 0.6265\n",
            "Acc1: 0.8671   Acc2: 0.7814   Acc3: 0.8715\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "lr: 0.0005\n",
            "(train)\n",
            "Loss1: 0.3516   Loss2: 0.4932   Loss3: 0.2570\n",
            "Acc1: 0.9380   Acc2: 0.8944   Acc3: 0.9559\n",
            "(val)\n",
            "Loss1: 0.6007   Loss2: 0.8749   Loss3: 0.6261\n",
            "Acc1: 0.8604   Acc2: 0.7799   Acc3: 0.8663\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "lr: 0.00025\n",
            "(train)\n",
            "Loss1: 0.3238   Loss2: 0.4704   Loss3: 0.2388\n",
            "Acc1: 0.9457   Acc2: 0.9057   Acc3: 0.9609\n",
            "(val)\n",
            "Loss1: 0.5846   Loss2: 0.8835   Loss3: 0.6372\n",
            "Acc1: 0.8730   Acc2: 0.7777   Acc3: 0.8656\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "lr: 0.00025\n",
            "(train)\n",
            "Loss1: 0.3165   Loss2: 0.4640   Loss3: 0.2294\n",
            "Acc1: 0.9452   Acc2: 0.9061   Acc3: 0.9647\n",
            "(val)\n",
            "Loss1: 0.5740   Loss2: 0.8773   Loss3: 0.6264\n",
            "Acc1: 0.8722   Acc2: 0.7725   Acc3: 0.8685\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "lr: 0.00025\n",
            "(train)\n",
            "Loss1: 0.3067   Loss2: 0.4528   Loss3: 0.2314\n",
            "Acc1: 0.9505   Acc2: 0.9105   Acc3: 0.9633\n",
            "(val)\n",
            "Loss1: 0.5802   Loss2: 0.8690   Loss3: 0.6205\n",
            "Acc1: 0.8722   Acc2: 0.7777   Acc3: 0.8708\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "lr: 0.00025\n",
            "(train)\n",
            "Loss1: 0.2930   Loss2: 0.4510   Loss3: 0.2202\n",
            "Acc1: 0.9574   Acc2: 0.9081   Acc3: 0.9670\n",
            "(val)\n",
            "Loss1: 0.5930   Loss2: 0.8744   Loss3: 0.6311\n",
            "Acc1: 0.8685   Acc2: 0.7806   Acc3: 0.8663\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "lr: 0.00025\n",
            "(train)\n",
            "Loss1: 0.2794   Loss2: 0.4361   Loss3: 0.2124\n",
            "Acc1: 0.9583   Acc2: 0.9175   Acc3: 0.9666\n",
            "(val)\n",
            "Loss1: 0.6008   Loss2: 0.9644   Loss3: 0.6688\n",
            "Acc1: 0.8678   Acc2: 0.7725   Acc3: 0.8619\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "lr: 0.00025\n",
            "(train)\n",
            "Loss1: 0.2638   Loss2: 0.4332   Loss3: 0.2119\n",
            "Acc1: 0.9607   Acc2: 0.9217   Acc3: 0.9699\n",
            "(val)\n",
            "Loss1: 0.5794   Loss2: 0.8854   Loss3: 0.6340\n",
            "Acc1: 0.8700   Acc2: 0.7799   Acc3: 0.8715\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "lr: 0.00025\n",
            "(train)\n",
            "Loss1: 0.2522   Loss2: 0.4223   Loss3: 0.2074\n",
            "Acc1: 0.9675   Acc2: 0.9223   Acc3: 0.9675\n",
            "(val)\n",
            "Loss1: 0.5916   Loss2: 0.9070   Loss3: 0.6408\n",
            "Acc1: 0.8700   Acc2: 0.7799   Acc3: 0.8648\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "lr: 0.00025\n",
            "(train)\n",
            "Loss1: 0.2440   Loss2: 0.4107   Loss3: 0.1973\n",
            "Acc1: 0.9695   Acc2: 0.9302   Acc3: 0.9725\n",
            "(val)\n",
            "Loss1: 0.5961   Loss2: 0.9088   Loss3: 0.6883\n",
            "Acc1: 0.8715   Acc2: 0.7784   Acc3: 0.8663\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "lr: 0.00025\n",
            "(train)\n",
            "Loss1: 0.2300   Loss2: 0.4052   Loss3: 0.1856\n",
            "Acc1: 0.9686   Acc2: 0.9302   Acc3: 0.9738\n",
            "(val)\n",
            "Loss1: 0.5823   Loss2: 0.8848   Loss3: 0.6667\n",
            "Acc1: 0.8722   Acc2: 0.7733   Acc3: 0.8634\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "lr: 0.00025\n",
            "(train)\n",
            "Loss1: 0.2246   Loss2: 0.3924   Loss3: 0.1834\n",
            "Acc1: 0.9725   Acc2: 0.9360   Acc3: 0.9753\n",
            "(val)\n",
            "Loss1: 0.5946   Loss2: 0.9082   Loss3: 0.6605\n",
            "Acc1: 0.8700   Acc2: 0.7777   Acc3: 0.8634\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "lr: 0.000125\n",
            "(train)\n",
            "Loss1: 0.2096   Loss2: 0.3848   Loss3: 0.1695\n",
            "Acc1: 0.9788   Acc2: 0.9400   Acc3: 0.9790\n",
            "(val)\n",
            "Loss1: 0.5882   Loss2: 0.9070   Loss3: 0.6686\n",
            "Acc1: 0.8722   Acc2: 0.7762   Acc3: 0.8634\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "lr: 0.000125\n",
            "(train)\n",
            "Loss1: 0.2076   Loss2: 0.3754   Loss3: 0.1664\n",
            "Acc1: 0.9775   Acc2: 0.9433   Acc3: 0.9795\n",
            "(val)\n",
            "Loss1: 0.6050   Loss2: 0.9137   Loss3: 0.6527\n",
            "Acc1: 0.8722   Acc2: 0.7762   Acc3: 0.8693\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "lr: 0.000125\n",
            "(train)\n",
            "Loss1: 0.2000   Loss2: 0.3730   Loss3: 0.1672\n",
            "Acc1: 0.9797   Acc2: 0.9419   Acc3: 0.9793\n",
            "(val)\n",
            "Loss1: 0.5995   Loss2: 0.9431   Loss3: 0.6946\n",
            "Acc1: 0.8744   Acc2: 0.7784   Acc3: 0.8619\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "lr: 0.000125\n",
            "(train)\n",
            "Loss1: 0.1916   Loss2: 0.3658   Loss3: 0.1607\n",
            "Acc1: 0.9806   Acc2: 0.9485   Acc3: 0.9801\n",
            "(val)\n",
            "Loss1: 0.5978   Loss2: 0.9187   Loss3: 0.6592\n",
            "Acc1: 0.8752   Acc2: 0.7755   Acc3: 0.8634\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "lr: 0.000125\n",
            "(train)\n",
            "Loss1: 0.1818   Loss2: 0.3629   Loss3: 0.1567\n",
            "Acc1: 0.9836   Acc2: 0.9461   Acc3: 0.9827\n",
            "(val)\n",
            "Loss1: 0.6089   Loss2: 0.9484   Loss3: 0.6834\n",
            "Acc1: 0.8715   Acc2: 0.7747   Acc3: 0.8626\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "lr: 0.000125\n",
            "(train)\n",
            "Loss1: 0.1733   Loss2: 0.3587   Loss3: 0.1588\n",
            "Acc1: 0.9858   Acc2: 0.9483   Acc3: 0.9825\n",
            "(val)\n",
            "Loss1: 0.6052   Loss2: 0.9364   Loss3: 0.6896\n",
            "Acc1: 0.8730   Acc2: 0.7762   Acc3: 0.8626\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "lr: 0.000125\n",
            "(train)\n",
            "Loss1: 0.1801   Loss2: 0.3521   Loss3: 0.1525\n",
            "Acc1: 0.9841   Acc2: 0.9518   Acc3: 0.9858\n",
            "(val)\n",
            "Loss1: 0.6093   Loss2: 0.9366   Loss3: 0.6715\n",
            "Acc1: 0.8752   Acc2: 0.7770   Acc3: 0.8671\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "lr: 0.000125\n",
            "(train)\n",
            "Loss1: 0.1764   Loss2: 0.3443   Loss3: 0.1474\n",
            "Acc1: 0.9839   Acc2: 0.9498   Acc3: 0.9832\n",
            "(val)\n",
            "Loss1: 0.6077   Loss2: 0.9341   Loss3: 0.6742\n",
            "Acc1: 0.8722   Acc2: 0.7784   Acc3: 0.8656\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "lr: 0.000125\n",
            "(train)\n",
            "Loss1: 0.1664   Loss2: 0.3397   Loss3: 0.1439\n",
            "Acc1: 0.9858   Acc2: 0.9544   Acc3: 0.9860\n",
            "(val)\n",
            "Loss1: 0.6097   Loss2: 0.9423   Loss3: 0.6871\n",
            "Acc1: 0.8774   Acc2: 0.7784   Acc3: 0.8626\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "lr: 0.000125\n",
            "(train)\n",
            "Loss1: 0.1628   Loss2: 0.3370   Loss3: 0.1421\n",
            "Acc1: 0.9876   Acc2: 0.9561   Acc3: 0.9871\n",
            "(val)\n",
            "Loss1: 0.6229   Loss2: 0.9512   Loss3: 0.6976\n",
            "Acc1: 0.8715   Acc2: 0.7792   Acc3: 0.8619\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "lr: 6.25e-05\n",
            "(train)\n",
            "Loss1: 0.1583   Loss2: 0.3329   Loss3: 0.1388\n",
            "Acc1: 0.9874   Acc2: 0.9563   Acc3: 0.9871\n",
            "(val)\n",
            "Loss1: 0.6091   Loss2: 0.9622   Loss3: 0.6997\n",
            "Acc1: 0.8744   Acc2: 0.7806   Acc3: 0.8619\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "lr: 6.25e-05\n",
            "(train)\n",
            "Loss1: 0.1562   Loss2: 0.3250   Loss3: 0.1381\n",
            "Acc1: 0.9891   Acc2: 0.9598   Acc3: 0.9865\n",
            "(val)\n",
            "Loss1: 0.6097   Loss2: 0.9515   Loss3: 0.6928\n",
            "Acc1: 0.8722   Acc2: 0.7799   Acc3: 0.8604\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "lr: 6.25e-05\n",
            "(train)\n",
            "Loss1: 0.1526   Loss2: 0.3269   Loss3: 0.1335\n",
            "Acc1: 0.9908   Acc2: 0.9572   Acc3: 0.9882\n",
            "(val)\n",
            "Loss1: 0.6255   Loss2: 0.9468   Loss3: 0.6839\n",
            "Acc1: 0.8737   Acc2: 0.7770   Acc3: 0.8648\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "lr: 6.25e-05\n",
            "(train)\n",
            "Loss1: 0.1527   Loss2: 0.3224   Loss3: 0.1331\n",
            "Acc1: 0.9884   Acc2: 0.9572   Acc3: 0.9874\n",
            "(val)\n",
            "Loss1: 0.6058   Loss2: 0.9516   Loss3: 0.6811\n",
            "Acc1: 0.8730   Acc2: 0.7799   Acc3: 0.8678\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "lr: 6.25e-05\n",
            "(train)\n",
            "Loss1: 0.1527   Loss2: 0.3209   Loss3: 0.1267\n",
            "Acc1: 0.9891   Acc2: 0.9601   Acc3: 0.9897\n",
            "(val)\n",
            "Loss1: 0.6216   Loss2: 0.9394   Loss3: 0.6815\n",
            "Acc1: 0.8759   Acc2: 0.7806   Acc3: 0.8663\n",
            "\n",
            "Epoch 45/99\n",
            "----------\n",
            "lr: 6.25e-05\n",
            "(train)\n",
            "Loss1: 0.1537   Loss2: 0.3155   Loss3: 0.1321\n",
            "Acc1: 0.9898   Acc2: 0.9607   Acc3: 0.9895\n",
            "(val)\n",
            "Loss1: 0.6143   Loss2: 0.9473   Loss3: 0.6880\n",
            "Acc1: 0.8774   Acc2: 0.7814   Acc3: 0.8663\n",
            "\n",
            "Epoch 46/99\n",
            "----------\n",
            "lr: 6.25e-05\n",
            "(train)\n",
            "Loss1: 0.1441   Loss2: 0.3128   Loss3: 0.1306\n",
            "Acc1: 0.9908   Acc2: 0.9623   Acc3: 0.9882\n",
            "(val)\n",
            "Loss1: 0.6117   Loss2: 0.9446   Loss3: 0.6889\n",
            "Acc1: 0.8774   Acc2: 0.7784   Acc3: 0.8612\n",
            "\n",
            "Epoch 47/99\n",
            "----------\n",
            "lr: 6.25e-05\n",
            "(train)\n",
            "Loss1: 0.1468   Loss2: 0.3117   Loss3: 0.1261\n",
            "Acc1: 0.9897   Acc2: 0.9599   Acc3: 0.9900\n",
            "(val)\n",
            "Loss1: 0.6087   Loss2: 0.9620   Loss3: 0.6948\n",
            "Acc1: 0.8730   Acc2: 0.7792   Acc3: 0.8634\n",
            "\n",
            "Epoch 48/99\n",
            "----------\n",
            "lr: 6.25e-05\n",
            "(train)\n",
            "Loss1: 0.1451   Loss2: 0.3076   Loss3: 0.1249\n",
            "Acc1: 0.9915   Acc2: 0.9653   Acc3: 0.9889\n",
            "(val)\n",
            "Loss1: 0.6262   Loss2: 0.9480   Loss3: 0.6916\n",
            "Acc1: 0.8737   Acc2: 0.7792   Acc3: 0.8648\n",
            "\n",
            "Epoch 49/99\n",
            "----------\n",
            "lr: 6.25e-05\n",
            "(train)\n",
            "Loss1: 0.1413   Loss2: 0.3052   Loss3: 0.1227\n",
            "Acc1: 0.9917   Acc2: 0.9657   Acc3: 0.9898\n",
            "(val)\n",
            "Loss1: 0.6250   Loss2: 0.9432   Loss3: 0.6904\n",
            "Acc1: 0.8744   Acc2: 0.7792   Acc3: 0.8693\n",
            "\n",
            "Epoch 50/99\n",
            "----------\n",
            "lr: 3.125e-05\n",
            "(train)\n",
            "Loss1: 0.1388   Loss2: 0.3047   Loss3: 0.1151\n",
            "Acc1: 0.9910   Acc2: 0.9618   Acc3: 0.9915\n",
            "(val)\n",
            "Loss1: 0.6169   Loss2: 0.9395   Loss3: 0.6874\n",
            "Acc1: 0.8804   Acc2: 0.7814   Acc3: 0.8656\n",
            "\n",
            "Epoch 51/99\n",
            "----------\n",
            "lr: 3.125e-05\n",
            "(train)\n",
            "Loss1: 0.1402   Loss2: 0.2966   Loss3: 0.1170\n",
            "Acc1: 0.9908   Acc2: 0.9671   Acc3: 0.9917\n",
            "(val)\n",
            "Loss1: 0.6228   Loss2: 0.9595   Loss3: 0.7072\n",
            "Acc1: 0.8715   Acc2: 0.7792   Acc3: 0.8582\n",
            "\n",
            "Epoch 52/99\n",
            "----------\n",
            "lr: 3.125e-05\n",
            "(train)\n",
            "Loss1: 0.1384   Loss2: 0.2970   Loss3: 0.1187\n",
            "Acc1: 0.9922   Acc2: 0.9694   Acc3: 0.9926\n",
            "(val)\n",
            "Loss1: 0.6370   Loss2: 0.9635   Loss3: 0.6980\n",
            "Acc1: 0.8752   Acc2: 0.7792   Acc3: 0.8663\n",
            "\n",
            "Epoch 53/99\n",
            "----------\n",
            "lr: 3.125e-05\n",
            "(train)\n",
            "Loss1: 0.1326   Loss2: 0.2958   Loss3: 0.1197\n",
            "Acc1: 0.9919   Acc2: 0.9671   Acc3: 0.9910\n",
            "(val)\n",
            "Loss1: 0.6310   Loss2: 0.9490   Loss3: 0.6963\n",
            "Acc1: 0.8715   Acc2: 0.7806   Acc3: 0.8619\n",
            "\n",
            "Epoch 54/99\n",
            "----------\n",
            "lr: 3.125e-05\n",
            "(train)\n",
            "Loss1: 0.1376   Loss2: 0.2909   Loss3: 0.1110\n",
            "Acc1: 0.9904   Acc2: 0.9664   Acc3: 0.9926\n",
            "(val)\n",
            "Loss1: 0.6288   Loss2: 0.9585   Loss3: 0.6926\n",
            "Acc1: 0.8730   Acc2: 0.7792   Acc3: 0.8619\n",
            "\n",
            "Epoch 55/99\n",
            "----------\n",
            "lr: 3.125e-05\n",
            "(train)\n",
            "Loss1: 0.1373   Loss2: 0.2928   Loss3: 0.1186\n",
            "Acc1: 0.9902   Acc2: 0.9703   Acc3: 0.9906\n",
            "(val)\n",
            "Loss1: 0.6298   Loss2: 0.9610   Loss3: 0.7018\n",
            "Acc1: 0.8737   Acc2: 0.7814   Acc3: 0.8612\n",
            "\n",
            "Epoch 56/99\n",
            "----------\n",
            "lr: 3.125e-05\n",
            "(train)\n",
            "Loss1: 0.1330   Loss2: 0.2913   Loss3: 0.1163\n",
            "Acc1: 0.9930   Acc2: 0.9688   Acc3: 0.9908\n",
            "(val)\n",
            "Loss1: 0.6437   Loss2: 0.9618   Loss3: 0.6993\n",
            "Acc1: 0.8759   Acc2: 0.7821   Acc3: 0.8656\n",
            "\n",
            "Epoch 57/99\n",
            "----------\n",
            "lr: 3.125e-05\n",
            "(train)\n",
            "Loss1: 0.1263   Loss2: 0.2884   Loss3: 0.1132\n",
            "Acc1: 0.9946   Acc2: 0.9708   Acc3: 0.9934\n",
            "(val)\n",
            "Loss1: 0.6333   Loss2: 0.9686   Loss3: 0.7062\n",
            "Acc1: 0.8737   Acc2: 0.7821   Acc3: 0.8634\n",
            "\n",
            "Epoch 58/99\n",
            "----------\n",
            "lr: 3.125e-05\n",
            "(train)\n",
            "Loss1: 0.1345   Loss2: 0.2897   Loss3: 0.1119\n",
            "Acc1: 0.9919   Acc2: 0.9701   Acc3: 0.9926\n",
            "(val)\n",
            "Loss1: 0.6303   Loss2: 0.9579   Loss3: 0.7005\n",
            "Acc1: 0.8744   Acc2: 0.7821   Acc3: 0.8619\n",
            "\n",
            "Epoch 59/99\n",
            "----------\n",
            "lr: 3.125e-05\n",
            "(train)\n",
            "Loss1: 0.1267   Loss2: 0.2833   Loss3: 0.1137\n",
            "Acc1: 0.9932   Acc2: 0.9690   Acc3: 0.9908\n",
            "(val)\n",
            "Loss1: 0.6240   Loss2: 0.9710   Loss3: 0.6928\n",
            "Acc1: 0.8722   Acc2: 0.7814   Acc3: 0.8641\n",
            "\n",
            "Epoch 60/99\n",
            "----------\n",
            "lr: 1.5625e-05\n",
            "(train)\n",
            "Loss1: 0.1281   Loss2: 0.2872   Loss3: 0.1094\n",
            "Acc1: 0.9922   Acc2: 0.9695   Acc3: 0.9932\n",
            "(val)\n",
            "Loss1: 0.6281   Loss2: 0.9580   Loss3: 0.6975\n",
            "Acc1: 0.8730   Acc2: 0.7799   Acc3: 0.8641\n",
            "\n",
            "Epoch 61/99\n",
            "----------\n",
            "lr: 1.5625e-05\n",
            "(train)\n",
            "Loss1: 0.1329   Loss2: 0.2835   Loss3: 0.1132\n",
            "Acc1: 0.9913   Acc2: 0.9705   Acc3: 0.9921\n",
            "(val)\n",
            "Loss1: 0.6316   Loss2: 0.9737   Loss3: 0.7131\n",
            "Acc1: 0.8715   Acc2: 0.7851   Acc3: 0.8567\n",
            "\n",
            "Epoch 62/99\n",
            "----------\n",
            "lr: 1.5625e-05\n",
            "(train)\n",
            "Loss1: 0.1263   Loss2: 0.2822   Loss3: 0.1136\n",
            "Acc1: 0.9948   Acc2: 0.9719   Acc3: 0.9911\n",
            "(val)\n",
            "Loss1: 0.6409   Loss2: 0.9643   Loss3: 0.7013\n",
            "Acc1: 0.8767   Acc2: 0.7799   Acc3: 0.8671\n",
            "\n",
            "Epoch 63/99\n",
            "----------\n",
            "lr: 1.5625e-05\n",
            "(train)\n",
            "Loss1: 0.1228   Loss2: 0.2879   Loss3: 0.1162\n",
            "Acc1: 0.9946   Acc2: 0.9694   Acc3: 0.9917\n",
            "(val)\n",
            "Loss1: 0.6186   Loss2: 0.9678   Loss3: 0.6999\n",
            "Acc1: 0.8722   Acc2: 0.7806   Acc3: 0.8619\n",
            "\n",
            "Epoch 64/99\n",
            "----------\n",
            "lr: 1.5625e-05\n",
            "(train)\n",
            "Loss1: 0.1276   Loss2: 0.2797   Loss3: 0.1146\n",
            "Acc1: 0.9928   Acc2: 0.9721   Acc3: 0.9911\n",
            "(val)\n",
            "Loss1: 0.6274   Loss2: 0.9654   Loss3: 0.7038\n",
            "Acc1: 0.8722   Acc2: 0.7806   Acc3: 0.8619\n",
            "\n",
            "Epoch 65/99\n",
            "----------\n",
            "lr: 1.5625e-05\n",
            "(train)\n",
            "Loss1: 0.1282   Loss2: 0.2830   Loss3: 0.1138\n",
            "Acc1: 0.9926   Acc2: 0.9721   Acc3: 0.9921\n",
            "(val)\n",
            "Loss1: 0.6339   Loss2: 0.9614   Loss3: 0.6980\n",
            "Acc1: 0.8722   Acc2: 0.7821   Acc3: 0.8626\n",
            "\n",
            "Epoch 66/99\n",
            "----------\n",
            "lr: 1.5625e-05\n",
            "(train)\n",
            "Loss1: 0.1261   Loss2: 0.2809   Loss3: 0.1109\n",
            "Acc1: 0.9937   Acc2: 0.9710   Acc3: 0.9930\n",
            "(val)\n",
            "Loss1: 0.6270   Loss2: 0.9875   Loss3: 0.7036\n",
            "Acc1: 0.8730   Acc2: 0.7784   Acc3: 0.8626\n",
            "\n",
            "Epoch 67/99\n",
            "----------\n",
            "lr: 1.5625e-05\n",
            "(train)\n",
            "Loss1: 0.1214   Loss2: 0.2769   Loss3: 0.1108\n",
            "Acc1: 0.9948   Acc2: 0.9723   Acc3: 0.9934\n",
            "(val)\n",
            "Loss1: 0.6317   Loss2: 0.9750   Loss3: 0.7007\n",
            "Acc1: 0.8737   Acc2: 0.7792   Acc3: 0.8663\n",
            "\n",
            "Epoch 68/99\n",
            "----------\n",
            "lr: 1.5625e-05\n",
            "(train)\n",
            "Loss1: 0.1242   Loss2: 0.2759   Loss3: 0.1108\n",
            "Acc1: 0.9939   Acc2: 0.9690   Acc3: 0.9924\n",
            "(val)\n",
            "Loss1: 0.6355   Loss2: 0.9612   Loss3: 0.7020\n",
            "Acc1: 0.8767   Acc2: 0.7792   Acc3: 0.8641\n",
            "\n",
            "Epoch 69/99\n",
            "----------\n",
            "lr: 1.5625e-05\n",
            "(train)\n",
            "Loss1: 0.1233   Loss2: 0.2752   Loss3: 0.1126\n",
            "Acc1: 0.9922   Acc2: 0.9738   Acc3: 0.9922\n",
            "(val)\n",
            "Loss1: 0.6283   Loss2: 0.9696   Loss3: 0.7096\n",
            "Acc1: 0.8737   Acc2: 0.7814   Acc3: 0.8641\n",
            "\n",
            "Epoch 70/99\n",
            "----------\n",
            "lr: 7.8125e-06\n",
            "(train)\n",
            "Loss1: 0.1341   Loss2: 0.2779   Loss3: 0.1070\n",
            "Acc1: 0.9921   Acc2: 0.9716   Acc3: 0.9941\n",
            "(val)\n",
            "Loss1: 0.6272   Loss2: 0.9945   Loss3: 0.6964\n",
            "Acc1: 0.8737   Acc2: 0.7814   Acc3: 0.8641\n",
            "\n",
            "Epoch 71/99\n",
            "----------\n",
            "lr: 7.8125e-06\n",
            "(train)\n",
            "Loss1: 0.1248   Loss2: 0.2788   Loss3: 0.1105\n",
            "Acc1: 0.9937   Acc2: 0.9699   Acc3: 0.9924\n",
            "(val)\n",
            "Loss1: 0.6251   Loss2: 0.9841   Loss3: 0.7238\n",
            "Acc1: 0.8730   Acc2: 0.7799   Acc3: 0.8582\n",
            "\n",
            "Epoch 72/99\n",
            "----------\n",
            "lr: 7.8125e-06\n",
            "(train)\n",
            "Loss1: 0.1240   Loss2: 0.2733   Loss3: 0.1109\n",
            "Acc1: 0.9926   Acc2: 0.9747   Acc3: 0.9915\n",
            "(val)\n",
            "Loss1: 0.6209   Loss2: 0.9531   Loss3: 0.6948\n",
            "Acc1: 0.8708   Acc2: 0.7799   Acc3: 0.8663\n",
            "\n",
            "Epoch 73/99\n",
            "----------\n",
            "lr: 7.8125e-06\n",
            "(train)\n",
            "Loss1: 0.1288   Loss2: 0.2745   Loss3: 0.1058\n",
            "Acc1: 0.9922   Acc2: 0.9729   Acc3: 0.9948\n",
            "(val)\n",
            "Loss1: 0.6330   Loss2: 0.9699   Loss3: 0.6990\n",
            "Acc1: 0.8737   Acc2: 0.7814   Acc3: 0.8641\n",
            "\n",
            "Epoch 74/99\n",
            "----------\n",
            "lr: 7.8125e-06\n",
            "(train)\n",
            "Loss1: 0.1237   Loss2: 0.2793   Loss3: 0.1113\n",
            "Acc1: 0.9932   Acc2: 0.9705   Acc3: 0.9924\n",
            "(val)\n",
            "Loss1: 0.6271   Loss2: 0.9592   Loss3: 0.7063\n",
            "Acc1: 0.8759   Acc2: 0.7799   Acc3: 0.8604\n",
            "\n",
            "Epoch 75/99\n",
            "----------\n",
            "lr: 7.8125e-06\n",
            "(train)\n",
            "Loss1: 0.1282   Loss2: 0.2736   Loss3: 0.1105\n",
            "Acc1: 0.9943   Acc2: 0.9732   Acc3: 0.9921\n",
            "(val)\n",
            "Loss1: 0.6208   Loss2: 0.9572   Loss3: 0.7038\n",
            "Acc1: 0.8752   Acc2: 0.7806   Acc3: 0.8589\n",
            "\n",
            "Epoch 76/99\n",
            "----------\n",
            "lr: 7.8125e-06\n",
            "(train)\n",
            "Loss1: 0.1305   Loss2: 0.2782   Loss3: 0.1108\n",
            "Acc1: 0.9932   Acc2: 0.9688   Acc3: 0.9932\n",
            "(val)\n",
            "Loss1: 0.6432   Loss2: 0.9814   Loss3: 0.7105\n",
            "Acc1: 0.8744   Acc2: 0.7799   Acc3: 0.8626\n",
            "\n",
            "Epoch 77/99\n",
            "----------\n",
            "lr: 7.8125e-06\n",
            "(train)\n",
            "Loss1: 0.1213   Loss2: 0.2742   Loss3: 0.1065\n",
            "Acc1: 0.9943   Acc2: 0.9732   Acc3: 0.9941\n",
            "(val)\n",
            "Loss1: 0.6570   Loss2: 0.9785   Loss3: 0.7050\n",
            "Acc1: 0.8722   Acc2: 0.7836   Acc3: 0.8663\n",
            "\n",
            "Epoch 78/99\n",
            "----------\n",
            "lr: 7.8125e-06\n",
            "(train)\n",
            "Loss1: 0.1225   Loss2: 0.2739   Loss3: 0.1143\n",
            "Acc1: 0.9943   Acc2: 0.9714   Acc3: 0.9911\n",
            "(val)\n",
            "Loss1: 0.6511   Loss2: 0.9785   Loss3: 0.7209\n",
            "Acc1: 0.8715   Acc2: 0.7792   Acc3: 0.8656\n",
            "\n",
            "Epoch 79/99\n",
            "----------\n",
            "lr: 7.8125e-06\n",
            "(train)\n",
            "Loss1: 0.1270   Loss2: 0.2774   Loss3: 0.1094\n",
            "Acc1: 0.9926   Acc2: 0.9721   Acc3: 0.9934\n",
            "(val)\n",
            "Loss1: 0.6311   Loss2: 0.9837   Loss3: 0.7157\n",
            "Acc1: 0.8752   Acc2: 0.7806   Acc3: 0.8575\n",
            "\n",
            "Epoch 80/99\n",
            "----------\n",
            "lr: 3.90625e-06\n",
            "(train)\n",
            "Loss1: 0.1215   Loss2: 0.2718   Loss3: 0.1078\n",
            "Acc1: 0.9941   Acc2: 0.9747   Acc3: 0.9945\n",
            "(val)\n",
            "Loss1: 0.6440   Loss2: 0.9757   Loss3: 0.7097\n",
            "Acc1: 0.8730   Acc2: 0.7806   Acc3: 0.8656\n",
            "\n",
            "Epoch 81/99\n",
            "----------\n",
            "lr: 3.90625e-06\n",
            "(train)\n",
            "Loss1: 0.1257   Loss2: 0.2760   Loss3: 0.1099\n",
            "Acc1: 0.9924   Acc2: 0.9736   Acc3: 0.9930\n",
            "(val)\n",
            "Loss1: 0.6266   Loss2: 0.9625   Loss3: 0.7035\n",
            "Acc1: 0.8722   Acc2: 0.7806   Acc3: 0.8604\n",
            "\n",
            "Epoch 82/99\n",
            "----------\n",
            "lr: 3.90625e-06\n",
            "(train)\n",
            "Loss1: 0.1215   Loss2: 0.2743   Loss3: 0.1031\n",
            "Acc1: 0.9941   Acc2: 0.9753   Acc3: 0.9945\n",
            "(val)\n",
            "Loss1: 0.6306   Loss2: 0.9870   Loss3: 0.6997\n",
            "Acc1: 0.8759   Acc2: 0.7814   Acc3: 0.8671\n",
            "\n",
            "Epoch 83/99\n",
            "----------\n",
            "lr: 3.90625e-06\n",
            "(train)\n",
            "Loss1: 0.1250   Loss2: 0.2713   Loss3: 0.1100\n",
            "Acc1: 0.9922   Acc2: 0.9749   Acc3: 0.9937\n",
            "(val)\n",
            "Loss1: 0.6251   Loss2: 0.9782   Loss3: 0.7069\n",
            "Acc1: 0.8737   Acc2: 0.7806   Acc3: 0.8656\n",
            "\n",
            "Epoch 84/99\n",
            "----------\n",
            "lr: 3.90625e-06\n",
            "(train)\n",
            "Loss1: 0.1223   Loss2: 0.2720   Loss3: 0.1026\n",
            "Acc1: 0.9935   Acc2: 0.9723   Acc3: 0.9945\n",
            "(val)\n",
            "Loss1: 0.6275   Loss2: 0.9837   Loss3: 0.6995\n",
            "Acc1: 0.8730   Acc2: 0.7799   Acc3: 0.8641\n",
            "\n",
            "Epoch 85/99\n",
            "----------\n",
            "lr: 3.90625e-06\n",
            "(train)\n",
            "Loss1: 0.1219   Loss2: 0.2690   Loss3: 0.1074\n",
            "Acc1: 0.9930   Acc2: 0.9747   Acc3: 0.9937\n",
            "(val)\n",
            "Loss1: 0.6281   Loss2: 0.9708   Loss3: 0.7215\n",
            "Acc1: 0.8715   Acc2: 0.7799   Acc3: 0.8575\n",
            "\n",
            "Epoch 86/99\n",
            "----------\n",
            "lr: 3.90625e-06\n",
            "(train)\n",
            "Loss1: 0.1224   Loss2: 0.2719   Loss3: 0.1065\n",
            "Acc1: 0.9932   Acc2: 0.9747   Acc3: 0.9930\n",
            "(val)\n",
            "Loss1: 0.6293   Loss2: 0.9688   Loss3: 0.7007\n",
            "Acc1: 0.8722   Acc2: 0.7799   Acc3: 0.8656\n",
            "\n",
            "Epoch 87/99\n",
            "----------\n",
            "lr: 3.90625e-06\n",
            "(train)\n",
            "Loss1: 0.1180   Loss2: 0.2731   Loss3: 0.1112\n",
            "Acc1: 0.9954   Acc2: 0.9731   Acc3: 0.9928\n",
            "(val)\n",
            "Loss1: 0.6281   Loss2: 0.9960   Loss3: 0.7194\n",
            "Acc1: 0.8730   Acc2: 0.7799   Acc3: 0.8582\n",
            "\n",
            "Epoch 88/99\n",
            "----------\n",
            "lr: 3.90625e-06\n",
            "(train)\n",
            "Loss1: 0.1241   Loss2: 0.2727   Loss3: 0.1083\n",
            "Acc1: 0.9934   Acc2: 0.9734   Acc3: 0.9930\n",
            "(val)\n",
            "Loss1: 0.6385   Loss2: 0.9577   Loss3: 0.7004\n",
            "Acc1: 0.8737   Acc2: 0.7762   Acc3: 0.8671\n",
            "\n",
            "Epoch 89/99\n",
            "----------\n",
            "lr: 3.90625e-06\n",
            "(train)\n",
            "Loss1: 0.1261   Loss2: 0.2727   Loss3: 0.1012\n",
            "Acc1: 0.9932   Acc2: 0.9742   Acc3: 0.9950\n",
            "(val)\n",
            "Loss1: 0.6843   Loss2: 0.9715   Loss3: 0.7142\n",
            "Acc1: 0.8715   Acc2: 0.7799   Acc3: 0.8641\n",
            "\n",
            "Epoch 90/99\n",
            "----------\n",
            "lr: 1.953125e-06\n",
            "(train)\n",
            "Loss1: 0.1225   Loss2: 0.2702   Loss3: 0.1069\n",
            "Acc1: 0.9928   Acc2: 0.9731   Acc3: 0.9939\n",
            "(val)\n",
            "Loss1: 0.6601   Loss2: 0.9662   Loss3: 0.7067\n",
            "Acc1: 0.8744   Acc2: 0.7799   Acc3: 0.8656\n",
            "\n",
            "Epoch 91/99\n",
            "----------\n",
            "lr: 1.953125e-06\n",
            "(train)\n",
            "Loss1: 0.1226   Loss2: 0.2705   Loss3: 0.1038\n",
            "Acc1: 0.9941   Acc2: 0.9723   Acc3: 0.9945\n",
            "(val)\n",
            "Loss1: 0.6195   Loss2: 0.9675   Loss3: 0.7019\n",
            "Acc1: 0.8752   Acc2: 0.7806   Acc3: 0.8641\n",
            "\n",
            "Epoch 92/99\n",
            "----------\n",
            "lr: 1.953125e-06\n",
            "(train)\n",
            "Loss1: 0.1260   Loss2: 0.2735   Loss3: 0.1101\n",
            "Acc1: 0.9932   Acc2: 0.9723   Acc3: 0.9915\n",
            "(val)\n",
            "Loss1: 0.6321   Loss2: 0.9955   Loss3: 0.7071\n",
            "Acc1: 0.8744   Acc2: 0.7821   Acc3: 0.8634\n",
            "\n",
            "Epoch 93/99\n",
            "----------\n",
            "lr: 1.953125e-06\n",
            "(train)\n",
            "Loss1: 0.1251   Loss2: 0.2671   Loss3: 0.1030\n",
            "Acc1: 0.9926   Acc2: 0.9732   Acc3: 0.9961\n",
            "(val)\n",
            "Loss1: 0.6283   Loss2: 0.9608   Loss3: 0.7021\n",
            "Acc1: 0.8715   Acc2: 0.7806   Acc3: 0.8626\n",
            "\n",
            "Epoch 94/99\n",
            "----------\n",
            "lr: 1.953125e-06\n",
            "(train)\n",
            "Loss1: 0.1210   Loss2: 0.2702   Loss3: 0.1082\n",
            "Acc1: 0.9937   Acc2: 0.9753   Acc3: 0.9943\n",
            "(val)\n",
            "Loss1: 0.6532   Loss2: 0.9666   Loss3: 0.7103\n",
            "Acc1: 0.8722   Acc2: 0.7784   Acc3: 0.8634\n",
            "\n",
            "Epoch 95/99\n",
            "----------\n",
            "lr: 1.953125e-06\n",
            "(train)\n",
            "Loss1: 0.1223   Loss2: 0.2672   Loss3: 0.1030\n",
            "Acc1: 0.9948   Acc2: 0.9731   Acc3: 0.9946\n",
            "(val)\n",
            "Loss1: 0.6467   Loss2: 0.9646   Loss3: 0.7031\n",
            "Acc1: 0.8752   Acc2: 0.7799   Acc3: 0.8612\n",
            "\n",
            "Epoch 96/99\n",
            "----------\n",
            "lr: 1.953125e-06\n",
            "(train)\n",
            "Loss1: 0.1212   Loss2: 0.2719   Loss3: 0.1099\n",
            "Acc1: 0.9945   Acc2: 0.9721   Acc3: 0.9939\n",
            "(val)\n",
            "Loss1: 0.6387   Loss2: 0.9700   Loss3: 0.7181\n",
            "Acc1: 0.8789   Acc2: 0.7806   Acc3: 0.8575\n",
            "\n",
            "Epoch 97/99\n",
            "----------\n",
            "lr: 1.953125e-06\n",
            "(train)\n",
            "Loss1: 0.1265   Loss2: 0.2741   Loss3: 0.1069\n",
            "Acc1: 0.9939   Acc2: 0.9714   Acc3: 0.9935\n",
            "(val)\n",
            "Loss1: 0.6376   Loss2: 0.9684   Loss3: 0.7115\n",
            "Acc1: 0.8759   Acc2: 0.7814   Acc3: 0.8597\n",
            "\n",
            "Epoch 98/99\n",
            "----------\n",
            "lr: 1.953125e-06\n",
            "(train)\n",
            "Loss1: 0.1246   Loss2: 0.2651   Loss3: 0.1022\n",
            "Acc1: 0.9934   Acc2: 0.9727   Acc3: 0.9950\n",
            "(val)\n",
            "Loss1: 0.6337   Loss2: 0.9609   Loss3: 0.6985\n",
            "Acc1: 0.8737   Acc2: 0.7792   Acc3: 0.8626\n",
            "\n",
            "Epoch 99/99\n",
            "----------\n",
            "lr: 1.953125e-06\n",
            "(train)\n",
            "Loss1: 0.1248   Loss2: 0.2700   Loss3: 0.1053\n",
            "Acc1: 0.9946   Acc2: 0.9727   Acc3: 0.9943\n",
            "(val)\n",
            "Loss1: 0.6236   Loss2: 0.9620   Loss3: 0.7153\n",
            "Acc1: 0.8737   Acc2: 0.7814   Acc3: 0.8626\n",
            "\n",
            "Training complete in 68m 45s\n",
            "Best val Acc: 0.842935\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1gN_XDMLzVL"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRFZlK37L5at"
      },
      "source": [
        "### function get_mean_std\n",
        "\n",
        "Used for normalization.\n",
        "\n",
        "For ```np_random_seed = 64``` and ```validation_split = 0.2```:\n",
        "\n",
        "* All data - mean = 0.3933, std = 0.2727\n",
        "* Train - mean = 0.3927, std = 0.2723\n",
        "* Val - mean = 0.3961, std = 0.2743\n",
        "\n",
        "Normalization and denormalization operations should all use training set values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkfzrg_6Nb_3"
      },
      "source": [
        "def get_mean_std(loader):\n",
        "  #VAR[X] = E[X**2] - E[X]**2\n",
        "  channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
        "  for image, labels in loader:\n",
        "    channels_sum += torch.mean(image, dim=[0,2,3])\n",
        "    channels_squared_sum += torch.mean(image**2, dim=[0,2,3])\n",
        "    num_batches = num_batches + 1\n",
        "    print('num_batches: {}'.format(num_batches))\n",
        "  mean = channels_sum / num_batches\n",
        "  std = (channels_squared_sum/num_batches - mean**2)**0.5\n",
        "  return mean, std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGP_OO2pNe45"
      },
      "source": [
        "### function unnormalize\n",
        "\n",
        "Used for visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYLa-ch5NpvE"
      },
      "source": [
        "def unnormalize(img, mean=[0.3927, 0.3927, 0.3927], std=[0.2723, 0.2723, 0.2723]):\n",
        "  for t, m, s in zip(img, mean, std):\n",
        "    t.mul_(s).add_(m)\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0m5VUeQNr-o"
      },
      "source": [
        "### function show_batch\n",
        "\n",
        "Visualizes batch of images in grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO1oThUjYzHm"
      },
      "source": [
        "def show_data(loader, batch_to_visualize):\n",
        "  for i, [images, labels] in enumerate(loader):\n",
        "    if (i == batch_to_visualize):\n",
        "      # lbl = [labels[0][index_to_visualize], labels[1][index_to_visualize], labels[2][index_to_visualize]]\n",
        "      for img in images:\n",
        "        img = unnormalize(img)\n",
        "      grid = utils.make_grid(images)\n",
        "      plt.figure()\n",
        "      plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "      plt.axis('off')\n",
        "      plt.ioff()\n",
        "      plt.show()\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu5E0efwDOk3"
      },
      "source": [
        "# Ax-platform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJAWFk2NDfjo"
      },
      "source": [
        "## Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF6rGyHeDd6j"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pickle\n",
        "import copy\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IEr9BOFD3x3",
        "outputId": "b7591fd8-bb2d-422c-9a23-34d1c76d8fec"
      },
      "source": [
        "! pip install ax-platform"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ax-platform\n",
            "  Downloading ax_platform-0.2.1-py3-none-any.whl (844 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 39.1 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 40.7 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30 kB 42.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40 kB 26.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 51 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 71 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 81 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 92 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 102 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 112 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 122 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 133 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 143 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 153 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 163 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 174 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 184 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 194 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 204 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 215 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 225 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 235 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 245 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 256 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 266 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 276 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 286 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 296 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 307 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 317 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 327 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 337 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 348 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 358 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 368 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 378 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 389 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 399 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 409 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 419 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 430 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 440 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 450 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 460 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 471 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 481 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 491 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 501 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 512 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 522 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 532 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 542 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 552 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 563 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 573 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 583 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 593 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 604 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 614 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 624 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 634 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 645 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 655 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 665 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 675 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 686 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 696 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 706 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 716 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 727 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 737 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 747 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 757 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 768 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 778 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 788 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 798 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 808 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 819 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 829 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 839 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 844 kB 13.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from ax-platform) (1.4.1)\n",
            "Collecting botorch==0.5.0\n",
            "  Downloading botorch-0.5.0-py3-none-any.whl (475 kB)\n",
            "\u001b[K     |████████████████████████████████| 475 kB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard in /usr/local/lib/python3.7/dist-packages (from ax-platform) (2.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ax-platform) (1.1.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from ax-platform) (2.11.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from ax-platform) (4.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from ax-platform) (0.22.2.post1)\n",
            "Collecting gpytorch>=1.5\n",
            "  Downloading gpytorch-1.5.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 61.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from botorch==0.5.0->ax-platform) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->botorch==0.5.0->ax-platform) (3.7.4.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->ax-platform) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ax-platform) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->ax-platform) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->ax-platform) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->ax-platform) (1.15.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->ax-platform) (1.3.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->ax-platform) (1.0.1)\n",
            "Installing collected packages: gpytorch, botorch, ax-platform\n",
            "Successfully installed ax-platform-0.2.1 botorch-0.5.0 gpytorch-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWWycbXiEHlO"
      },
      "source": [
        "from ax.plot.contour import plot_contour\n",
        "from ax.plot.trace import optimization_trace_single_method\n",
        "from ax.service.managed_loop import optimize\n",
        "from ax.utils.notebook.plotting import render\n",
        "from ax.utils.tutorials.cnn_utils import train, evaluate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPzfUPXkSUU1"
      },
      "source": [
        "import tqdm\n",
        "from tqdm import trange\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "from functools import partial\n",
        "tqdm = partial(tqdm, position=0, leave=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P92aLsjiBX3r"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VarHV-SmBf5S"
      },
      "source": [
        "train_test_split = 0.85\n",
        "datafn = '/content/drive/MyDrive/embryo-grading/multi-focal-project/morphology_grading_dataset_0606.pickle'\n",
        "trans = transforms.Compose([transforms.ToTensor(),\n",
        "                            transforms.Normalize([0.3927], [0.2723])])\n",
        "ax_total_trials = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB8FgfelFUfV"
      },
      "source": [
        "## class single_focal_net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-Q61wmtFsFG"
      },
      "source": [
        "class single_focal_net(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(single_focal_net, self).__init__()\n",
        "    self.backbone_model = models.resnet18(pretrained=True)\n",
        "    self.backbone_model.fc = nn.Identity()\n",
        "    self.fc1 = nn.Linear(512, 4)\n",
        "    self.fc2 = nn.Linear(512, 3)\n",
        "    self.fc3 = nn.Linear(512, 3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.backbone_model(x)\n",
        "    out1 = self.fc1(x)\n",
        "    out2 = self.fc2(x)\n",
        "    out3 = self.fc3(x)\n",
        "    return {'out1': out1, 'out2': out2, 'out3': out3}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaeU8fCwFX8R"
      },
      "source": [
        "## class mydata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jR7YVHnFwTw"
      },
      "source": [
        "class mydata(Dataset):\n",
        "\n",
        "  def __init__(self, datafn, multi_image=False, transform=None):\n",
        "    # Load .mat data file\n",
        "    with open(datafn, 'rb') as handle:\n",
        "      self.data = pickle.load(handle)\n",
        "    self.multi_image_flag = multi_image\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "\n",
        "    if not self.multi_image_flag:\n",
        "      image = self.data[idx]['img']['0']\n",
        "      labels = self.data[idx]['label'][1]\n",
        "    else:\n",
        "      # TODO multi image not implemented yet\n",
        "      image = self.data[idx]['img']['0']\n",
        "      labels = self.data[idx]['label'][0]\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    image = image.repeat(3, 1, 1) # turn into RGB because pretrained network expects RGB\n",
        "\n",
        "    return image, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KdRhXj6A8Q0"
      },
      "source": [
        "### Global dataset vars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RxRbDcNN9YQz"
      },
      "source": [
        "# Dataset\n",
        "dataset = mydata(datafn, multi_image=False, transform=trans)\n",
        "\n",
        "train_val_set_len = int(train_test_split * len(dataset))\n",
        "dataset_train_val_indices = range(0, train_val_set_len)\n",
        "dataset_test_indices = range(train_val_set_len, len(dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0IyW80vFfhG"
      },
      "source": [
        "## function focal_loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LD3EzWUJFzpi"
      },
      "source": [
        "def focal_loss(labels, logits, alpha, gamma):\n",
        "  bcloss = F.binary_cross_entropy_with_logits(input=logits, target=labels, reduction='none')\n",
        "  if gamma == 0.0:\n",
        "    modulator = 1.0\n",
        "  else:\n",
        "    modulator = torch.exp(-gamma * labels * logits - gamma * torch.log(1 + torch.exp(-1.0 * logits)))\n",
        "\n",
        "  loss = modulator * bcloss\n",
        "  weighted_loss = alpha * loss\n",
        "\n",
        "  return torch.sum(weighted_loss) / torch.sum(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIMGXc72Fiue"
      },
      "source": [
        "## function cb_loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gDJ1eqs7F2oQ"
      },
      "source": [
        "def cb_loss(labels, logits, samples_per_class, loss_type, beta, gamma, device):\n",
        "  num_class = len(samples_per_class)\n",
        "  labels_oh = F.one_hot(labels, num_class).float()\n",
        "  weights = (1.0 - beta) / (np.array(1.0 - np.power(beta, samples_per_class)))\n",
        "  weights = weights / np.sum(weights) * num_class\n",
        "  weights = torch.tensor(weights).float()\n",
        "  weights = weights.to(device)\n",
        "  weights = weights.unsqueeze(0)\n",
        "  weights = weights.repeat(labels_oh.shape[0], 1) * labels_oh\n",
        "  weights = weights.sum(1)\n",
        "  weights = weights.unsqueeze(1)\n",
        "  weights = weights.repeat(1, num_class)\n",
        "\n",
        "  if loss_type == 'focal':\n",
        "    cb_loss = focal_loss(labels_oh, logits, weights, gamma)\n",
        "  elif loss_type == 'sigmoid':\n",
        "    cb_loss = F.binary_cross_entropy_with_logits(input=logits, target=labels_oh, weights=weights)\n",
        "  elif loss_type == 'softmax':\n",
        "    pred = logits.softmax(dim = 1)\n",
        "    cb_loss = F.binary_cross_entropy(input=pred, target=labels_oh, weight=weights)\n",
        "\n",
        "  return cb_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8CDtU5xFlMJ"
      },
      "source": [
        "## function train_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xQFZz2R4F6HQ"
      },
      "source": [
        "def train_model(model, device, dataloader, hyperparms, samples_per_class):\n",
        "\n",
        "  model = model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  optimizer = optim.SGD(model.parameters(), \n",
        "                        lr=hyperparms.get('lr'), \n",
        "                        momentum=hyperparms.get('momentum'),\n",
        "                        weight_decay=hyperparms.get('weight_decay'))\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
        "                                        step_size=hyperparms.get('step_size'),\n",
        "                                        gamma=hyperparms.get('lr_gamma'))\n",
        "  num_epochs = hyperparms.get('num_epochs')\n",
        "  loss_type = hyperparms.get('loss_type')\n",
        "  loss_beta = hyperparms.get('loss_beta')\n",
        "  loss_gamma = hyperparms.get('loss_gamma')\n",
        "\n",
        "  # Show progress bar for epochs\n",
        "  with tqdm(total=num_epochs) as pbar_epoch:\n",
        "\n",
        "    # Iterate over epochs\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      # Iterate over data.\n",
        "      for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        label1 = torch.from_numpy(np.asarray(labels[0]).astype('long')).to(device)\n",
        "        label2 = torch.from_numpy(np.asarray(labels[1]).astype('long')).to(device)\n",
        "        label3 = torch.from_numpy(np.asarray(labels[2]).astype('long')).to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Combine losses\n",
        "        loss1 = cb_loss(label1, outputs['out1'], samples_per_class[0], loss_type, loss_beta, loss_gamma, device)\n",
        "        loss2 = cb_loss(label2, outputs['out2'], samples_per_class[1], loss_type, loss_beta, loss_gamma, device)\n",
        "        loss3 = cb_loss(label3, outputs['out3'], samples_per_class[2], loss_type, loss_beta, loss_gamma, device)\n",
        "        loss1_weight = hyperparms.get('loss1_weight')\n",
        "        loss2_weight = hyperparms.get('loss2_weight')\n",
        "        loss3_weight = 1.0 - loss1_weight - loss2_weight\n",
        "        loss = loss1_weight*loss1 + loss2_weight*loss2 + loss3_weight*loss3 \n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      scheduler.step()\n",
        "      pbar_epoch.update(1)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKEgynx9XT83"
      },
      "source": [
        "## function val_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uvEE2VMXXWwv"
      },
      "source": [
        "def val_model(model, device, dataloader):\n",
        "\n",
        "  model.eval()\n",
        "  size = len(dataloader.sampler) # since I'm using samplers\n",
        "\n",
        "  running_corrects1 = 0\n",
        "  running_corrects2 = 0\n",
        "  running_corrects3 = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    # Iterate over data.\n",
        "    for inputs, labels in dataloader:\n",
        "      inputs = inputs.to(device)\n",
        "      label1 = torch.from_numpy(np.asarray(labels[0]).astype('long')).to(device)\n",
        "      label2 = torch.from_numpy(np.asarray(labels[1]).astype('long')).to(device)\n",
        "      label3 = torch.from_numpy(np.asarray(labels[2]).astype('long')).to(device)\n",
        "\n",
        "      outputs = model(inputs)\n",
        "      _, preds1 = torch.max(outputs['out1'], 1)\n",
        "      _, preds2 = torch.max(outputs['out2'], 1)\n",
        "      _, preds3 = torch.max(outputs['out3'], 1)\n",
        "\n",
        "      running_corrects1 += torch.sum(preds1 == label1).cpu().numpy()\n",
        "      running_corrects2 += torch.sum(preds2 == label2).cpu().numpy()\n",
        "      running_corrects3 += torch.sum(preds3 == label3).cpu().numpy()\n",
        "\n",
        "  acc1 = running_corrects1 / size\n",
        "  acc2 = running_corrects1 / size\n",
        "  acc3 = running_corrects1 / size\n",
        "\n",
        "  return acc1, acc2, acc3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzxcotRsimKK"
      },
      "source": [
        "## function train_evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VUnnm5c3isi2"
      },
      "source": [
        "def train_evaluate(hyperparms):\n",
        "\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  # Stratified K-fold cross validation\n",
        "  skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=64)\n",
        "\n",
        "  # Helper function for getting an appropriate Y for stratified splitting\n",
        "  def join_label(label):\n",
        "    return int(''.join(map(str, label))) # [0,1,2] -> int(012) -> 12\n",
        "\n",
        "  X = np.array(dataset_train_val_indices)\n",
        "  Y = np.array([join_label(dataset[idx][1]) for idx in dataset_train_val_indices])\n",
        "\n",
        "  avg_acc = []\n",
        "  for train_indices, val_indices in skf.split(X, Y):\n",
        "\n",
        "    # Split train and val set\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    val_sampler = SubsetRandomSampler(val_indices)\n",
        "    batch_size = hyperparms.get('batch_size')\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "    # Get samples per class\n",
        "    samples_per_class = [[0, 0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
        "    for _, labels in train_loader:\n",
        "      for i in range(3):\n",
        "        for j in range(len(samples_per_class[i])):\n",
        "          num_mask = np.asarray(labels[i]) == j\n",
        "          samples_per_class[i][j] += len(labels[i][num_mask])\n",
        "\n",
        "    # Instantiate model and train\n",
        "    untrained_model = single_focal_net()\n",
        "    untrained_model = untrained_model.to(device)\n",
        "    trained_model = train_model(model=untrained_model, \n",
        "                                device=device, \n",
        "                                dataloader=train_loader, \n",
        "                                hyperparms=hyperparms, \n",
        "                                samples_per_class=samples_per_class)\n",
        "\n",
        "    # Do validation\n",
        "    acc1, acc2, acc3 = val_model(model=trained_model, \n",
        "                                 device=device, \n",
        "                                 dataloader=val_loader)\n",
        "    avg_acc.append(np.mean([acc1, acc2, acc3]))\n",
        "\n",
        "  print('average accuracy = {}'.format(np.mean(avg_acc)))\n",
        "  return np.mean(avg_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3HTy45iPIAB"
      },
      "source": [
        "## Ax-platform experiment script\n",
        "\n",
        "Returns:\n",
        "\n",
        "- ```best_parameters```: dictionary of what ax-platform thinks are the best hyperparameters\n",
        "- ```means, covariances = values```: means and covariances of the optimized metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9a48a1f48e4d4f44acdc5a7fab320b96"
          ]
        },
        "id": "40UdNv8NPRzo",
        "outputId": "ffb6f54b-1242-4636-fbe8-8ae2d5f67dd0"
      },
      "source": [
        "best_parameters, values, experiment, model = optimize(\n",
        "  parameters=[\n",
        "    {'name': 'lr', 'type': 'range', 'bounds': [1e-8, 0.1], 'log_scale': True},\n",
        "    {'name': 'weight_decay', 'type': 'range', 'bounds': [1e-8, 0.1], 'log_scale': True},\n",
        "    {'name': 'batch_size', 'type': 'range', 'bounds': [4, 64]},\n",
        "    {'name': 'momentum', 'type': 'range', 'bounds': [0.0, 1.0]},\n",
        "    {'name': 'step_size', 'type': 'range', 'bounds': [1, 10]},\n",
        "    {'name': 'lr_gamma', 'type': 'range', 'bounds': [0.02, 1.0]},\n",
        "    {'name': 'loss_gamma', 'type': 'range', 'bounds': [0.0, 5.0]}, # for focal loss\n",
        "    {'name': 'loss_beta', 'type': 'choice', 'values': [0.9, 0.99, 0.999, 0.9999, 0.99999]}, # for cb loss\n",
        "    {'name': 'num_epochs', 'type': 'range', 'bounds': [5, 15]},\n",
        "    {'name': 'loss_type', 'type': 'choice', 'values': ['softmax', 'focal']},\n",
        "    {'name': 'loss1_weight', 'type': 'range', 'bounds': [0.23, 0.43]},\n",
        "    {'name': 'loss2_weight', 'type': 'range', 'bounds': [0.23, 0.43]},\n",
        "  ],\n",
        "  evaluation_function=train_evaluate,\n",
        "  objective_name='accuracy',\n",
        "  minimize=False,\n",
        "  total_trials=ax_total_trials\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[INFO 09-08 13:21:22] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter lr. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
            "[INFO 09-08 13:21:22] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter weight_decay. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
            "[INFO 09-08 13:21:22] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter batch_size. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
            "[INFO 09-08 13:21:22] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter momentum. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
            "[INFO 09-08 13:21:22] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter step_size. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
            "[INFO 09-08 13:21:22] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter lr_gamma. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
            "[INFO 09-08 13:21:22] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter loss_gamma. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
            "[INFO 09-08 13:21:22] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter loss_beta. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
            "[INFO 09-08 13:21:22] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter num_epochs. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
            "[INFO 09-08 13:21:22] ax.service.utils.instantiation: Inferred value type of ParameterType.STRING for parameter loss_type. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
            "[INFO 09-08 13:21:22] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter loss1_weight. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
            "[INFO 09-08 13:21:22] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter loss2_weight. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
            "[INFO 09-08 13:21:22] ax.modelbridge.dispatch_utils: Using GPEI (Bayesian optimization) since there are more continuous parameters than there are categories for the unordered categorical parameters.\n",
            "[INFO 09-08 13:21:22] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 12 trials, GPEI for subsequent trials]). Iterations after 12 will take longer to generate due to  model-fitting.\n",
            "[INFO 09-08 13:21:22] ax.service.managed_loop: Started full optimization with 25 steps.\n",
            "[INFO 09-08 13:21:22] ax.service.managed_loop: Running optimization trial 1...\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a48a1f48e4d4f44acdc5a7fab320b96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/7 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning:\n",
            "\n",
            "Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\n",
            "100%|██████████| 7/7 [03:10<00:00, 27.26s/it]\n",
            "100%|██████████| 7/7 [03:10<00:00, 27.21s/it]\n",
            "100%|██████████| 7/7 [03:10<00:00, 27.18s/it]\n",
            "100%|██████████| 7/7 [03:10<00:00, 27.24s/it]\n",
            "100%|██████████| 7/7 [03:10<00:00, 27.23s/it]\n",
            "[INFO 09-08 13:38:10] ax.service.managed_loop: Running optimization trial 2...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.3743954532290762\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 14/14 [05:46<00:00, 24.75s/it]\n",
            "100%|██████████| 14/14 [05:47<00:00, 24.80s/it]\n",
            "100%|██████████| 14/14 [05:47<00:00, 24.82s/it]\n",
            "100%|██████████| 14/14 [05:47<00:00, 24.81s/it]\n",
            "100%|██████████| 14/14 [05:47<00:00, 24.84s/it]\n",
            "[INFO 09-08 14:07:56] ax.service.managed_loop: Running optimization trial 3...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.580785578965151\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 10/10 [03:59<00:00, 23.96s/it]\n",
            "100%|██████████| 10/10 [03:59<00:00, 23.96s/it]\n",
            "100%|██████████| 10/10 [03:59<00:00, 23.97s/it]\n",
            "100%|██████████| 10/10 [03:59<00:00, 23.97s/it]\n",
            "100%|██████████| 10/10 [03:59<00:00, 23.99s/it]\n",
            "[INFO 09-08 14:28:43] ax.service.managed_loop: Running optimization trial 4...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.8210576250120669\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 11/11 [04:27<00:00, 24.36s/it]\n",
            "100%|██████████| 11/11 [04:27<00:00, 24.32s/it]\n",
            "100%|██████████| 11/11 [04:27<00:00, 24.34s/it]\n",
            "100%|██████████| 11/11 [04:27<00:00, 24.34s/it]\n",
            "100%|██████████| 11/11 [04:27<00:00, 24.34s/it]\n",
            "[INFO 09-08 14:51:52] ax.service.managed_loop: Running optimization trial 5...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.7691068756636741\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 9/9 [03:41<00:00, 24.56s/it]\n",
            "100%|██████████| 9/9 [03:41<00:00, 24.57s/it]\n",
            "100%|██████████| 9/9 [03:41<00:00, 24.56s/it]\n",
            "100%|██████████| 9/9 [03:41<00:00, 24.57s/it]\n",
            "100%|██████████| 9/9 [03:41<00:00, 24.57s/it]\n",
            "[INFO 09-08 15:11:08] ax.service.managed_loop: Running optimization trial 6...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.1224579773626798\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 15/15 [06:11<00:00, 24.74s/it]\n",
            "100%|██████████| 15/15 [06:11<00:00, 24.76s/it]\n",
            "100%|██████████| 15/15 [06:11<00:00, 24.76s/it]\n",
            "100%|██████████| 15/15 [06:11<00:00, 24.76s/it]\n",
            "100%|██████████| 15/15 [06:11<00:00, 24.75s/it]\n",
            "[INFO 09-08 15:42:57] ax.service.managed_loop: Running optimization trial 7...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.8756054517810599\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 5/5 [02:00<00:00, 24.10s/it]\n",
            "100%|██████████| 5/5 [02:00<00:00, 24.08s/it]\n",
            "100%|██████████| 5/5 [02:00<00:00, 24.06s/it]\n",
            "100%|██████████| 5/5 [02:00<00:00, 24.05s/it]\n",
            "100%|██████████| 5/5 [02:00<00:00, 24.05s/it]\n",
            "[INFO 09-08 15:53:54] ax.service.managed_loop: Running optimization trial 8...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.2241001182546578\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 6/6 [02:24<00:00, 24.01s/it]\n",
            "100%|██████████| 6/6 [02:24<00:00, 24.01s/it]\n",
            "100%|██████████| 6/6 [02:24<00:00, 24.01s/it]\n",
            "100%|██████████| 6/6 [02:24<00:00, 24.02s/it]\n",
            "100%|██████████| 6/6 [02:24<00:00, 24.01s/it]\n",
            "[INFO 09-08 16:06:42] ax.service.managed_loop: Running optimization trial 9...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.340516549618689\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 7/7 [02:49<00:00, 24.18s/it]\n",
            "100%|██████████| 7/7 [02:49<00:00, 24.17s/it]\n",
            "100%|██████████| 7/7 [02:49<00:00, 24.18s/it]\n",
            "100%|██████████| 7/7 [02:49<00:00, 24.17s/it]\n",
            "100%|██████████| 7/7 [02:49<00:00, 24.16s/it]\n",
            "[INFO 09-08 16:21:45] ax.service.managed_loop: Running optimization trial 10...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.2164298556810503\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 8/8 [03:21<00:00, 25.15s/it]\n",
            "100%|██████████| 8/8 [03:21<00:00, 25.17s/it]\n",
            "100%|██████████| 8/8 [03:21<00:00, 25.16s/it]\n",
            "100%|██████████| 8/8 [03:21<00:00, 25.15s/it]\n",
            "100%|██████████| 8/8 [03:21<00:00, 25.14s/it]\n",
            "[INFO 09-08 16:39:20] ax.service.managed_loop: Running optimization trial 11...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.22674863041799403\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 9/9 [03:41<00:00, 24.63s/it]\n",
            "100%|██████████| 9/9 [03:41<00:00, 24.64s/it]\n",
            "100%|██████████| 9/9 [03:29<00:00, 23.32s/it]\n",
            "100%|██████████| 9/9 [03:41<00:00, 24.63s/it]\n",
            "100%|██████████| 9/9 [03:29<00:00, 23.32s/it]\n",
            "[INFO 09-08 16:58:14] ax.service.managed_loop: Running optimization trial 12...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.6063174232551404\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 6/6 [02:20<00:00, 23.37s/it]\n",
            "100%|██████████| 6/6 [02:20<00:00, 23.34s/it]\n",
            "100%|██████████| 6/6 [02:20<00:00, 23.34s/it]\n",
            "100%|██████████| 6/6 [02:20<00:00, 23.35s/it]\n",
            "100%|██████████| 6/6 [02:20<00:00, 23.34s/it]\n",
            "[INFO 09-08 17:10:43] ax.service.managed_loop: Running optimization trial 13...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.19804457476590404\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 15/15 [05:43<00:00, 22.90s/it]\n",
            "100%|██████████| 15/15 [05:43<00:00, 22.91s/it]\n",
            "100%|██████████| 15/15 [05:43<00:00, 22.89s/it]\n",
            "100%|██████████| 15/15 [05:43<00:00, 22.89s/it]\n",
            "100%|██████████| 15/15 [05:43<00:00, 22.93s/it]\n",
            "[INFO 09-08 17:40:14] ax.service.managed_loop: Running optimization trial 14...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.8848167957814461\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 15/15 [06:09<00:00, 24.66s/it]\n",
            "100%|██████████| 15/15 [06:10<00:00, 24.67s/it]\n",
            "100%|██████████| 15/15 [06:09<00:00, 24.67s/it]\n",
            "100%|██████████| 15/15 [06:10<00:00, 24.67s/it]\n",
            "100%|██████████| 15/15 [06:09<00:00, 24.66s/it]\n",
            "[INFO 09-08 18:11:59] ax.service.managed_loop: Running optimization trial 15...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.8818616360169902\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 15/15 [06:09<00:00, 24.65s/it]\n",
            "100%|██████████| 15/15 [06:09<00:00, 24.65s/it]\n",
            "100%|██████████| 15/15 [06:09<00:00, 24.64s/it]\n",
            "100%|██████████| 15/15 [06:09<00:00, 24.65s/it]\n",
            "100%|██████████| 15/15 [06:09<00:00, 24.65s/it]\n",
            "[INFO 09-08 18:43:42] ax.service.managed_loop: Running optimization trial 16...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.886899374939666\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 15/15 [05:44<00:00, 22.95s/it]\n",
            "100%|██████████| 15/15 [05:44<00:00, 22.97s/it]\n",
            "100%|██████████| 15/15 [05:44<00:00, 22.94s/it]\n",
            "100%|██████████| 15/15 [05:44<00:00, 22.94s/it]\n",
            "100%|██████████| 15/15 [05:44<00:00, 22.96s/it]\n",
            "[INFO 09-08 19:13:16] ax.service.managed_loop: Running optimization trial 17...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.8822079532290761\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 15/15 [05:41<00:00, 22.74s/it]\n",
            "100%|██████████| 15/15 [05:41<00:00, 22.76s/it]\n",
            "100%|██████████| 15/15 [05:43<00:00, 22.93s/it]\n",
            "100%|██████████| 15/15 [05:41<00:00, 22.74s/it]\n",
            "100%|██████████| 15/15 [05:41<00:00, 22.74s/it]\n",
            "[INFO 09-08 19:42:34] ax.service.managed_loop: Running optimization trial 18...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.24689853026353897\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 15/15 [05:45<00:00, 23.05s/it]\n",
            "100%|██████████| 15/15 [05:45<00:00, 23.05s/it]\n",
            "100%|██████████| 15/15 [05:45<00:00, 23.06s/it]\n",
            "100%|██████████| 15/15 [05:45<00:00, 23.04s/it]\n",
            "100%|██████████| 15/15 [05:45<00:00, 23.04s/it]\n",
            "[INFO 09-08 20:12:30] ax.service.managed_loop: Running optimization trial 19...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.8589303383531229\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 15/15 [06:10<00:00, 24.70s/it]\n",
            "100%|██████████| 15/15 [06:10<00:00, 24.69s/it]\n",
            "100%|██████████| 15/15 [06:10<00:00, 24.69s/it]\n",
            "100%|██████████| 15/15 [06:10<00:00, 24.68s/it]\n",
            "100%|██████████| 15/15 [06:10<00:00, 24.69s/it]\n",
            "[INFO 09-08 20:44:36] ax.service.managed_loop: Running optimization trial 20...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.8771709684815135\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 15/15 [05:48<00:00, 23.25s/it]\n",
            "100%|██████████| 15/15 [05:48<00:00, 23.24s/it]\n",
            "100%|██████████| 15/15 [05:48<00:00, 23.26s/it]\n",
            "100%|██████████| 15/15 [05:48<00:00, 23.25s/it]\n",
            "100%|██████████| 15/15 [05:48<00:00, 23.24s/it]\n",
            "[INFO 09-08 21:14:55] ax.service.managed_loop: Running optimization trial 21...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.8896804710879428\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 15/15 [06:14<00:00, 24.98s/it]\n",
            "100%|██████████| 15/15 [06:14<00:00, 24.98s/it]\n",
            "100%|██████████| 15/15 [06:14<00:00, 24.98s/it]\n",
            "100%|██████████| 15/15 [06:14<00:00, 24.99s/it]\n",
            "100%|██████████| 15/15 [06:14<00:00, 24.98s/it]\n",
            "[INFO 09-08 21:47:19] ax.service.managed_loop: Running optimization trial 22...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy = 0.8943706861183511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 15/15 [06:10<00:00, 24.68s/it]\n",
            "100%|██████████| 15/15 [06:10<00:00, 24.69s/it]\n",
            "100%|██████████| 15/15 [06:10<00:00, 24.70s/it]\n",
            "100%|██████████| 15/15 [06:10<00:00, 24.68s/it]\n",
            "100%|██████████| 15/15 [06:10<00:00, 24.69s/it]\n",
            "[INFO 09-08 22:19:24] ax.service.managed_loop: Running optimization trial 23...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average accuracy = 0.8912446302731925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 15/15 [05:48<00:00, 23.25s/it]\n",
            "100%|██████████| 15/15 [05:48<00:00, 23.27s/it]\n",
            "100%|██████████| 15/15 [05:48<00:00, 23.26s/it]\n",
            "100%|██████████| 15/15 [05:48<00:00, 23.26s/it]\n",
            "100%|██████████| 15/15 [05:46<00:00, 23.13s/it]\n",
            "[INFO 09-08 22:49:41] ax.service.managed_loop: Running optimization trial 24...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average accuracy = 0.6620735893908678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 15/15 [06:06<00:00, 24.46s/it]\n",
            "100%|██████████| 15/15 [06:07<00:00, 24.47s/it]\n",
            "100%|██████████| 15/15 [06:07<00:00, 24.47s/it]\n",
            "100%|██████████| 15/15 [06:07<00:00, 24.47s/it]\n",
            "100%|██████████| 15/15 [06:07<00:00, 24.47s/it]\n",
            "[INFO 09-08 23:22:01] ax.service.managed_loop: Running optimization trial 25...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average accuracy = 0.8714386342793705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning:\n",
            "\n",
            "The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "\n",
            "100%|██████████| 15/15 [06:09<00:00, 24.62s/it]\n",
            "100%|██████████| 15/15 [06:09<00:00, 24.62s/it]\n",
            "100%|██████████| 15/15 [06:09<00:00, 24.62s/it]\n",
            "100%|██████████| 15/15 [06:09<00:00, 24.62s/it]\n",
            "100%|██████████| 15/15 [06:09<00:00, 24.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average accuracy = 0.8611880369726809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzNQZEQRUujC",
        "outputId": "3d1f52d9-7611-4833-938d-e1fce5d68452"
      },
      "source": [
        "print(best_parameters)\n",
        "means, covars = values\n",
        "print(means)\n",
        "print(covars)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'lr': 0.007139755710616323, 'weight_decay': 1.2049296999405936e-06, 'batch_size': 42, 'momentum': 0.9551203454074805, 'step_size': 3, 'lr_gamma': 0.4965844948817707, 'loss_gamma': 3.550996627942822, 'num_epochs': 15, 'loss1_weight': 0.36443370589740265, 'loss2_weight': 0.32606099465323224, 'loss_beta': 0.999, 'loss_type': 'softmax'}\n",
            "{'accuracy': 0.8950714786582658}\n",
            "{'accuracy': {'accuracy': 6.355516232130471e-05}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEuE0Tm58ZpT"
      },
      "source": [
        "\n",
        "```\n",
        "{'lr': 0.007139755710616323, 'weight_decay': 1.2049296999405936e-06, 'batch_size': 42, 'momentum': 0.9551203454074805, 'step_size': 3, 'lr_gamma': 0.4965844948817707, 'loss_gamma': 3.550996627942822, 'num_epochs': 15, 'loss1_weight': 0.36443370589740265, 'loss2_weight': 0.32606099465323224, 'loss_beta': 0.999, 'loss_type': 'softmax'}\n",
        "{'accuracy': 0.8950714786582658}\n",
        "{'accuracy': {'accuracy': 6.355516232130471e-05}}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFsBT0ggPmCl"
      },
      "source": [
        "## Plot ax results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IC9-7yz3Prxh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "outputId": "cd0c5d85-f2f8-466a-9d83-101c2269ec77"
      },
      "source": [
        "best_objectives = np.array([[trial.objective_mean for trial in experiment.trials.values()]])\n",
        " \n",
        "best_objective_plot = optimization_trace_single_method(\n",
        "    y=np.maximum.accumulate(best_objectives, axis=1),\n",
        "    title=\"Model performance vs. # of iterations\",\n",
        "    ylabel=\"Accuracy, %\",\n",
        ")\n",
        "render(best_objective_plot)\n",
        " \n",
        "render(plot_contour(model=model, param_x='batch_size', param_y='lr', metric_name='accuracy'))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"23627b1f-b3b8-4e88-9725-2674f7ade4af\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"23627b1f-b3b8-4e88-9725-2674f7ade4af\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '23627b1f-b3b8-4e88-9725-2674f7ade4af',\n",
              "                        [{\"hoverinfo\": \"none\", \"legendgroup\": \"\", \"line\": {\"width\": 0}, \"mode\": \"lines\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], \"y\": [0.3743954532290762, 0.580785578965151, 0.8210576250120669, 0.8210576250120669, 0.8210576250120669, 0.8756054517810599, 0.8756054517810599, 0.8756054517810599, 0.8756054517810599, 0.8756054517810599, 0.8756054517810599, 0.8756054517810599, 0.8848167957814461, 0.8848167957814461, 0.886899374939666, 0.886899374939666, 0.886899374939666, 0.886899374939666, 0.886899374939666, 0.8896804710879428, 0.8943706861183511, 0.8943706861183511, 0.8943706861183511, 0.8943706861183511, 0.8943706861183511]}, {\"fill\": \"tonexty\", \"fillcolor\": \"rgba(128,177,211,0.3)\", \"legendgroup\": \"objective value\", \"line\": {\"color\": \"rgba(128,177,211,1)\"}, \"mode\": \"lines\", \"name\": \"objective value\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], \"y\": [0.3743954532290762, 0.580785578965151, 0.8210576250120669, 0.8210576250120669, 0.8210576250120669, 0.8756054517810599, 0.8756054517810599, 0.8756054517810599, 0.8756054517810599, 0.8756054517810599, 0.8756054517810599, 0.8756054517810599, 0.8848167957814461, 0.8848167957814461, 0.886899374939666, 0.886899374939666, 0.886899374939666, 0.886899374939666, 0.886899374939666, 0.8896804710879428, 0.8943706861183511, 0.8943706861183511, 0.8943706861183511, 0.8943706861183511, 0.8943706861183511]}, {\"fill\": \"tonexty\", \"fillcolor\": \"rgba(128,177,211,0.3)\", \"hoverinfo\": \"none\", \"legendgroup\": \"\", \"line\": {\"width\": 0}, \"mode\": \"lines\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], \"y\": [0.3743954532290762, 0.580785578965151, 0.8210576250120669, 0.8210576250120669, 0.8210576250120669, 0.8756054517810599, 0.8756054517810599, 0.8756054517810599, 0.8756054517810599, 0.8756054517810599, 0.8756054517810599, 0.8756054517810599, 0.8848167957814461, 0.8848167957814461, 0.886899374939666, 0.886899374939666, 0.886899374939666, 0.886899374939666, 0.886899374939666, 0.8896804710879428, 0.8943706861183511, 0.8943706861183511, 0.8943706861183511, 0.8943706861183511, 0.8943706861183511]}],\n",
              "                        {\"showlegend\": true, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Model performance vs. # of iterations\"}, \"xaxis\": {\"title\": {\"text\": \"Iteration\"}}, \"yaxis\": {\"title\": {\"text\": \"Accuracy, %\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('23627b1f-b3b8-4e88-9725-2674f7ade4af');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"47cda26f-8ed0-442a-855a-77ae797cc425\" class=\"plotly-graph-div\" style=\"height:450px; width:950px;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"47cda26f-8ed0-442a-855a-77ae797cc425\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '47cda26f-8ed0-442a-855a-77ae797cc425',\n",
              "                        [{\"autocolorscale\": false, \"autocontour\": true, \"colorbar\": {\"tickfont\": {\"size\": 8}, \"ticksuffix\": \"\", \"x\": 0.45, \"y\": 0.5}, \"colorscale\": [[0.0, \"rgb(247,252,253)\"], [0.125, \"rgb(229,245,249)\"], [0.25, \"rgb(204,236,230)\"], [0.375, \"rgb(153,216,201)\"], [0.5, \"rgb(102,194,164)\"], [0.625, \"rgb(65,174,118)\"], [0.75, \"rgb(35,139,69)\"], [0.875, \"rgb(0,109,44)\"], [1.0, \"rgb(0,68,27)\"]], \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"x+y+z\", \"ncontours\": 25, \"type\": \"contour\", \"x\": [4.0, 5.224489795918368, 6.448979591836735, 7.673469387755102, 8.89795918367347, 10.122448979591837, 11.346938775510203, 12.571428571428571, 13.795918367346939, 15.020408163265307, 16.244897959183675, 17.46938775510204, 18.693877551020407, 19.918367346938776, 21.142857142857142, 22.367346938775512, 23.591836734693878, 24.816326530612244, 26.040816326530614, 27.26530612244898, 28.48979591836735, 29.714285714285715, 30.93877551020408, 32.163265306122454, 33.38775510204081, 34.61224489795919, 35.83673469387755, 37.06122448979592, 38.285714285714285, 39.51020408163265, 40.734693877551024, 41.95918367346939, 43.183673469387756, 44.40816326530612, 45.63265306122449, 46.85714285714286, 48.08163265306123, 49.30612244897959, 50.53061224489796, 51.755102040816325, 52.9795918367347, 54.204081632653065, 55.42857142857143, 56.6530612244898, 57.87755102040816, 59.10204081632653, 60.3265306122449, 61.55102040816327, 62.775510204081634, 64.0], \"xaxis\": \"x\", \"y\": [1e-08, 1.3894954943731388e-08, 1.9306977288832496e-08, 2.682695795279727e-08, 3.7275937203149377e-08, 5.1794746792312124e-08, 7.196856730011513e-08, 1e-07, 1.389495494373136e-07, 1.9306977288832497e-07, 2.6826957952797275e-07, 3.727593720314938e-07, 5.179474679231213e-07, 7.196856730011514e-07, 1e-06, 1.389495494373136e-06, 1.9306977288832498e-06, 2.6826957952797274e-06, 3.727593720314938e-06, 5.179474679231202e-06, 7.196856730011514e-06, 1e-05, 1.3894954943731361e-05, 1.9306977288832496e-05, 2.6826957952797274e-05, 3.727593720314938e-05, 5.179474679231202e-05, 7.196856730011514e-05, 0.0001, 0.0001389495494373136, 0.00019306977288832496, 0.0002682695795279722, 0.0003727593720314938, 0.0005179474679231213, 0.0007196856730011514, 0.001, 0.001389495494373136, 0.0019306977288832496, 0.002682695795279722, 0.003727593720314938, 0.005179474679231202, 0.007196856730011514, 0.01, 0.01389495494373136, 0.019306977288832496, 0.026826957952797218, 0.03727593720314938, 0.05179474679231202, 0.07196856730011514, 0.1], \"yaxis\": \"y\", \"z\": [[0.4297485640096341, 0.4297438432681691, 0.42973914814033914, 0.42973448567147565, 0.4297298630532691, 0.4297252876087305, 0.4297207667760764, 0.4297163080915547, 0.42971191917123774, 0.4297076076918147, 0.4297033813704245, 0.4296992479435767, 0.42969521514521825, 0.4296912906840098, 0.42968748221988723, 0.4296837973399873, 0.4296802435340312, 0.4296768281692622, 0.4296735584650452, 0.429670441467242, 0.42966748402248395, 0.42966469275246943, 0.4296620740284198, 0.4296596339458332, 0.42965737829967743, 0.42965531256016964, 0.4296534418492889, 0.4296517709181713, 0.4296503041255354, 0.4296490454172838, 0.42964799830742556, 0.42964716586045615, 0.4296465506753294, 0.4296461548711468, 0.42964598007468024, 0.42964602740983593, 0.4296462974891556, 0.42964679040743825, 0.4296475057375524, 0.42964844252849504, 0.4296495993057391, 0.42965097407389224, 0.429652564321678, 0.4296543670292302, 0.4296563786776773, 0.42965859526097366, 0.4296610122999218, 0.429663624858311, 0.4296664275610826, 0.42966941461441815], [0.42972517115846687, 0.42971967922593934, 0.429714205278636, 0.42970875742579095, 0.42970334398675913, 0.42969797347459193, 0.4296926545781491, 0.4296873961427553, 0.42968220714941946, 0.4296770966926465, 0.42967207395687845, 0.4296671481916175, 0.42966232868529153, 0.429657624737936, 0.4296530456327786, 0.4296486006068241, 0.4296442988205497, 0.4296401493268335, 0.4296361610392481, 0.4296323426998665, 0.4296287028467338, 0.429625249781171, 0.4296219915350835, 0.42961893583845834, 0.42961609008723556, 0.42961346131174927, 0.4296110561459344, 0.4296088807974981, 0.42960694101925523, 0.4296052420818264, 0.4296037887478915, 0.42960258524818984, 0.4296016352594467, 0.4296009418844018, 0.42960050763409907, 0.4296003344125894, 0.4296004235041798, 0.4296007755633482, 0.42960139060742497, 0.4296022680121247, 0.4296034065099892, 0.42960480419178504, 0.42960645851087476, 0.4296083662905609, 0.4296105237343779, 0.4296129264392866, 0.42961556941170204, 0.42961844708626606, 0.4296215533472536, 0.429624881552482], [0.42970830714537755, 0.42970217979084496, 0.4296960611770392, 0.42968996025731165, 0.4296838862577152, 0.42967784865976394, 0.42967185718135725, 0.42966592175586427, 0.42966005250937866, 0.4296542597361659, 0.4296485538723384, 0.4296429454678087, 0.4296374451565852, 0.4296320636254917, 0.42962681158140525, 0.4296216997171245, 0.4296167386759963, 0.42961193901544337, 0.42960731116955053, 0.42960286541088416, 0.4295986118117301, 0.42959456020495124, 0.42959072014467625, 0.42958710086704116, 0.4295837112512163, 0.42958055978095583, 0.4295776545069151, 0.4295750030099813, 0.4295726123658677, 0.429570489111217, 0.42956863921145827, 0.42956706803065603, 0.4295657803035789, 0.4295647801102095, 0.4295640708528987, 0.42956365523635687, 0.4295635352506534, 0.42956371215737854, 0.4295641864791003, 0.4295649579922232, 0.4295660257233359, 0.429567387949104, 0.42956904219974246, 0.42957098526607074, 0.42957321321013, 0.42957572137931066, 0.42957850442391554, 0.42958155631805267, 0.4295848703837303, 0.4295884393179994], [0.4296998208793361, 0.42969327301958593, 0.42968672454523454, 0.42968018493120147, 0.42967366397697093, 0.4296671717891388, 0.4296607187618232, 0.42965431555492406, 0.42964797307023417, 0.42964170242541533, 0.42963551492587115, 0.42962942203456633, 0.4296234353398565, 0.42961756652141336, 0.4296118273143483, 0.4296062294716544, 0.4296007847251078, 0.42959550474478725, 0.429590401097388, 0.42958548520352535, 0.42958076829423864, 0.42957626136692295, 0.4295719751409297, 0.4295679200130898, 0.4295641060134252, 0.42956054276132194, 0.42955723942244617, 0.42955420466668764, 0.42955144662741856, 0.42954897286235294, 0.4295467903162903, 0.42954490528601974, 0.42954332338765155, 0.429542049526632, 0.4295410878706818, 0.42954044182588236, 0.42954011401611225, 0.4295401062660154, 0.4295404195876591, 0.42954105417100996, 0.42954200937833265, 0.4295432837425829, 0.4295448749698383, 0.42954677994577795, 0.4295489947461919, 0.4295515146514674, 0.4295543341649708, 0.42955744703521115, 0.4295608462816436, 0.42956452422394087], [0.4297006367348192, 0.4296939384991884, 0.42968723171438816, 0.42968052598833617, 0.4296738312862522, 0.42966715791355403, 0.4296605164964396, 0.42965391796013175, 0.42964737350478155, 0.42964089457903876, 0.4296344928513188, 0.429628180178811, 0.4296219685742933, 0.4296158701708408, 0.42960989718452985, 0.4296040618752667, 0.4295983765058849, 0.42959285329967933, 0.4295875043965634, 0.42958234180805455, 0.42957737737131363, 0.42957262270247865, 0.42956808914955036, 0.4295637877451014, 0.4295597291590919, 0.42955592365208506, 0.42955238102916404, 0.42954911059485534, 0.4295461211093681, 0.429543420746456, 0.4295410170532076, 0.4295389169120605, 0.42953712650533066, 0.4295356512825307, 0.4295344959307371, 0.4295336643482506, 0.4295331596217674, 0.4295329840072617, 0.42953313891474754, 0.4295336248970663, 0.42953444164281074, 0.42953558797346914, 0.4295370618448391, 0.42953886035272715, 0.4295409797429174, 0.4295434154253592, 0.42954616199248863, 0.42954921324156903, 0.4295525622009021, 0.42955620115973137], [0.4297105055035185, 0.4297039432790218, 0.4296973671038088, 0.42969078631823215, 0.4296842106287959, 0.4296776500918795, 0.4296711150951169, 0.42966461633640707, 0.42965816480054697, 0.42965177173349495, 0.4296454486142889, 0.42963920712466475, 0.42963305911643823, 0.4296270165767325, 0.4296210915911574, 0.42961529630506323, 0.4296096428830154, 0.4296041434666563, 0.42959881013113943, 0.4295936548403415, 0.42958868940107825, 0.4295839254165618, 0.4295793742393606, 0.42957504692412973, 0.42957095418039776, 0.4295671063257014, 0.4295635132393708, 0.4295601843172704, 0.4295571284278048, 0.42955435386949686, 0.4295518683304441, 0.42954967884994977, 0.4295477917826193, 0.42954621276519744, 0.4295449466864076, 0.4295439976600357, 0.429543369001482, 0.42954306320797697, 0.42954308194263635, 0.4295434260224994, 0.42954409541066485, 0.42954508921261125, 0.4295464056767518, 0.4295480421992446, 0.4295499953330435, 0.42955226080114306, 0.4295548335139371, 0.42955770759057843, 0.4295608763841967, 0.4295643325108004], [0.42972799366202896, 0.4297218252886893, 0.4297156405003682, 0.42970944801689104, 0.42970325690950095, 0.4296970765857818, 0.42969091677235205, 0.42968478749530703, 0.4296786990584017, 0.42967266201898224, 0.42966668716169004, 0.42966078546998204, 0.42965496809552606, 0.4296492463255524, 0.42964363154825974, 0.4296381352163931, 0.42963276880913237, 0.4296275437924477, 0.4296224715780981, 0.4296175634814654, 0.429612830678437, 0.42960828416156205, 0.42960393469572433, 0.4295997927735853, 0.4295958685710646, 0.4295921719031319, 0.42958871218019357, 0.4295854983653588, 0.42958253893287696, 0.42957984182803155, 0.4295774144287775, 0.42957526350940045, 0.4295733952064665, 0.4295718149873238, 0.429570527621397, 0.42956953715450386, 0.4295688468863998, 0.42956845935173893, 0.42956837630461053, 0.4295685987067899, 0.4295691267198106, 0.42956995970093986, 0.4295710962031062, 0.4295725339788004, 0.42957426998793846, 0.42957630040964495, 0.4295786206578849, 0.42958122540084176, 0.42958410858391083, 0.42958726345615006], [0.4297507378037515, 0.4297451539038498, 0.4297395537416312, 0.42973394516166563, 0.4297283363266915, 0.4297227357039909, 0.4297171520497647, 0.42971159439148776, 0.4297060720082406, 0.4297005944090282, 0.4296951713091085, 0.4296898126043739, 0.4296845283438403, 0.429679328700318, 0.42967422393935517, 0.42966922438656047, 0.4296643403934304, 0.42965958230182166, 0.4296549604072274, 0.42965048492102953, 0.4296461659319164, 0.4296420133666675, 0.4296380369505199, 0.4296342461673427, 0.4296306502198548, 0.42962725799012946, 0.429624078000634, 0.4296211183760591, 0.42961838680619024, 0.429615890510076, 0.42961363620174375, 0.42961163005770775, 0.4296098776865057, 0.4296083841004924, 0.4296071536901032, 0.4296061902007868, 0.42960549671279014, 0.4296050756239581, 0.4296049286356909, 0.4296050567421777, 0.42960546022300505, 0.4296061386392088, 0.429607090832816, 0.4296083149298954, 0.4296098083471084, 0.4296115678017257, 0.4296135893250488, 0.429615868279147, 0.4296183993768009, 0.42962117670451233], [0.42977590744728766, 0.4297710091677958, 0.4297660965863733, 0.42976117654485635, 0.42975625615947327, 0.42975134280879745, 0.4297464441199864, 0.42974156795329665, 0.4297367223848739, 0.42973191568783103, 0.429727156311639, 0.4297224528598691, 0.42971781406633813, 0.42971324876972267, 0.42970876588672213, 0.42970437438386605, 0.4297000832480722, 0.42969590145607933, 0.4296918379428897, 0.42968790156936965, 0.42968410108916943, 0.4296804451151338, 0.4296769420853862, 0.42967360022927614, 0.4296704275333907, 0.4296674317078323, 0.42966462015297446, 0.42966199992690624, 0.429659577713778, 0.42965735979326203, 0.4296553520113358, 0.4296535597525931, 0.4296519879142803, 0.4296506408822457, 0.42964952250898236, 0.4296486360939278, 0.4296479843661729, 0.4296475694697165, 0.42964739295138277, 0.4296474557515023, 0.4296477581974364, 0.4296483000000043, 0.4296490802528519, 0.42965009743477744, 0.42965134941500926, 0.42965283346140726, 0.42965454625153854, 0.4296564838865564, 0.429658641907789, 0.4296610153159289], [0.4298007521967615, 0.4297965501389144, 0.42979233660544486, 0.4297881174360887, 0.42978389869904793, 0.4297796866805035, 0.4297754878727089, 0.42977130896066185, 0.42976715680735594, 0.42976303843762903, 0.4297589610206312, 0.4297549318509494, 0.429750958328434, 0.42974704793678653, 0.4297432082209767, 0.42973944676357057, 0.42973577116006, 0.42973218899329846, 0.42972870780715466, 0.4297253350795086, 0.4297220781947222, 0.4297189444157272, 0.4297159408558796, 0.42971307445073526, 0.4297103519299119, 0.4297077797892015, 0.4297053642631036, 0.4297031112979519, 0.42970102652580605, 0.4296991152392785, 0.4296973823674667, 0.4296958324531532, 0.42969446963143376, 0.42969329760992386, 0.4296923196506863, 0.42969153855401376, 0.42969095664418494, 0.42969057575730646, 0.42969039723133107, 0.4296904218983351, 0.42969065007911744, 0.4296910815801683, 0.42969171569303843, 0.42969255119612204, 0.42969358635884847, 0.4296948189482618, 0.42969624623794855, 0.429697865019257, 0.4296996716147356, 0.4297016618937012], [0.4298230874258643, 0.42981951975383503, 0.42981594354044256, 0.4298123637264204, 0.4298087854394248, 0.4298052139849572, 0.42980165483613053, 0.4297981136222786, 0.4297945961164149, 0.4297911082215583, 0.4297876559559479, 0.42978424543718124, 0.4297808828653152, 0.4297775745049828, 0.4297743266665828, 0.42977114568661223, 0.42976803790721807, 0.42976500965505365, 0.4297620672195349, 0.42975921683059654, 0.42975646463605766, 0.4297538166787131, 0.42975127887327025, 0.42974885698325926, 0.42974655659804656, 0.42974438311008534, 0.4297423416925402, 0.42974043727742156, 0.4297386745343684, 0.4297370578502149, 0.4297355913094737, 0.4297342786758678, 0.4297331233750349, 0.4297321284785234, 0.4297312966891944, 0.42973063032813164, 0.4297301313231555, 0.4297298011990265, 0.42972964106941114, 0.42972965163067334, 0.4297298331575403, 0.4297301855006803, 0.42973070808621644, 0.42973139991718456, 0.42973225957693295, 0.42973328523444615, 0.42973447465155984, 0.42973582519202447, 0.429737333832358, 0.42973899717441827], [0.42984160233936985, 0.42983856477064164, 0.4298355212976797, 0.4298324761245429, 0.42982943360850684, 0.429826398252157, 0.42982337469452936, 0.4298203677013034, 0.4298173821540554, 0.42981442303858947, 0.42981149543236696, 0.4298086044910666, 0.42980575543431077, 0.42980295353060194, 0.4298002040815229, 0.429797512405257, 0.42979488381949477, 0.4297923236238005, 0.42978983708151514, 0.4297874294012829, 0.4297851057182893, 0.4297828710753091, 0.42978073040366027, 0.4297786885041721, 0.42977675002826976, 0.42977491945928775, 0.42977320109412065, 0.4297715990253233, 0.42977011712377033, 0.42976875902198497, 0.42976752809824437, 0.42976642746156524, 0.4297654599376696, 0.4297646280560268, 0.4297639340380592, 0.4297633797865956, 0.42976296687664656, 0.42976269654756727, 0.4297625696966685, 0.42976258687432034, 0.42976274828059025, 0.42976305376344104, 0.4297635028185068, 0.4297640945904539, 0.42976482787592063, 0.42976570112802176, 0.42976671246238973, 0.4297678596647172, 0.4297691401997518, 0.4297705512216877], [0.42985593871039945, 0.42985331692388296, 0.4298506915196927, 0.4298480661388481, 0.4298454445500456, 0.42984283064265116, 0.42984022841888947, 0.4298376419852359, 0.42983507554302086, 0.42983253337826244, 0.4298300198507482, 0.42982753938239515, 0.42982509644492006, 0.4298226955468608, 0.4298203412199938, 0.42981803800519935, 0.42981579043783247, 0.42981360303266136, 0.42981148026844285, 0.4298094265722061, 0.42980744630332346, 0.42980554373744995, 0.42980372305041503, 0.429801988302156, 0.429800343420783, 0.4297987921868661, 0.42979733821803934, 0.4297959849540127, 0.4297947356420857, 0.4297935933232529, 0.4297925608189902, 0.42979164071880804, 0.42979083536865414, 0.42979014686024264, 0.42978957702138365, 0.4297891274073786, 0.4297887992935435, 0.42978859366891187, 0.42978851123116446, 0.4297885523828229, 0.42978871722873657, 0.4297890055748842, 0.42978941692850114, 0.4297899504995353, 0.42979060520342605, 0.42979137966518943, 0.42979227222478644, 0.4297932809437408, 0.42979440361296484, 0.4297956377617435], [0.4298665610024699, 0.4298642570616399, 0.42986195160874097, 0.429859647866668, 0.42985734916668217, 0.42985505894207443, 0.429852780721133, 0.4298505181194203, 0.4298482748313696, 0.4298460546212165, 0.429843861313286, 0.42984169878166073, 0.4298395709392612, 0.42983748172637404, 0.4298354350986706, 0.42983343501476157, 0.4298314854233394, 0.42982959024996525, 0.4298277533835614, 0.4298259786626724, 0.42982426986156685, 0.4298226306762485, 0.42982106471045356, 0.4298195754617123, 0.429818166307551, 0.42981684049191793, 0.4298156011119125, 0.429814451104898, 0.429813393236079, 0.4298124300866217, 0.4298115640423933, 0.4298107972833942, 0.4298101317739538, 0.42980956925375474, 0.4298091112297492, 0.4298087589690204, 0.42980851349264265, 0.4298083755705816, 0.4298083457176734, 0.4298084241907122, 0.429808610986669, 0.4298089058420569, 0.4298093082334501, 0.42980981737915613, 0.4298104322420324, 0.42981115153343186, 0.42981197371825186, 0.4298128970210573, 0.429813919433237, 0.4298150387211492], [0.42987449487521556, 0.42987244257728585, 0.42987039089839374, 0.42986834273582125, 0.429866301079265, 0.4298642690050378, 0.42986224966965936, 0.42986024630284064, 0.4298582621998731, 0.4298563007134355, 0.4298543652448389, 0.4298524592347331, 0.42985058615330246, 0.42984874948998536, 0.4298469527427544, 0.4298451994070007, 0.4298434929640673, 0.42984183686948585, 0.42984023454096837, 0.4298386893462152, 0.42983720459059993, 0.42983578350479634, 0.42983442923241555, 0.4298331448177214, 0.42983193319349605, 0.42983079716912687, 0.42982973941898694, 0.42982876247118085, 0.42982786869672684, 0.4298270602992448, 0.4298263393052174, 0.42982570755488914, 0.42982516669386445, 0.4298247181654634, 0.4298243632038865, 0.42982410282823835, 0.42982393783745243, 0.42982386880615414, 0.42982389608149263, 0.42982401978096696, 0.429824239791263, 0.429824555768113, 0.4298249671371811, 0.4298254730959714, 0.4298260726167488, 0.4298267644504551, 0.42982754713159577, 0.429828418984068, 0.42982937812789146, 0.4298304224868003], [0.4298810359568006, 0.4298792041753076, 0.42987737523329916, 0.4298755517374847, 0.42987373637191084, 0.4298719318926867, 0.42987014112217875, 0.42986836694268055, 0.4298666122895666, 0.42986488014394336, 0.4298631735248147, 0.4298614954807829, 0.4298598490813114, 0.4298582374075778, 0.4298566635429528, 0.4298551305631416, 0.42985364152603067, 0.4298521994612844, 0.42985080735974235, 0.4298494681626679, 0.4298481847509052, 0.42984695993400046, 0.4298457964393494, 0.4298446969014312, 0.42984366385119277, 0.42984269970564676, 0.42984180675774675, 0.42984098716660457, 0.42984024294811085, 0.42983957596602124, 0.4298389879235679, 0.42983848035565186, 0.4298380546216713, 0.4298377118990355, 0.42983745317741107, 0.42983727925374093, 0.429837190728075, 0.4298371880002425, 0.4298372712673918, 0.4298374405224197, 0.4298376955533013, 0.4298380359433315, 0.4298384610722768, 0.4298389701184345, 0.42983956206158724, 0.4298402356868355, 0.4298409895892852, 0.429841822179561, 0.42984273169010967, 0.42984371618225503], [0.42988751949536064, 0.4298859062156152, 0.4298842978774855, 0.4298826967844569, 0.42988110530201246, 0.42987952585297373, 0.42987796091239916, 0.4298764130020444, 0.4298748846843914, 0.42987337855625884, 0.4298718972420062, 0.4298704433863517, 0.42986901964682384, 0.4298676286858727, 0.4298662731626701, 0.42986495572463035, 0.42986367899868694, 0.42986244558236597, 0.429861258034696, 0.42986011886700115, 0.42985903053362384, 0.4298579954226269, 0.4298570158465277, 0.4298560940331152, 0.42985523211640686, 0.42985443212779817, 0.4298536959874607, 0.4298530254960438, 0.42985242232673315, 0.4298518880177202, 0.42985142396513265, 0.42985103141647596, 0.42985071146463083, 0.42985046504245183, 0.4298502929180048, 0.42985019569048033, 0.429850173786814, 0.4298502274590411, 0.429850356782406, 0.4298505616542452, 0.4298508417936542, 0.4298511967419446, 0.42985162586389186, 0.42985212834976905, 0.4298527032181559, 0.4298533493195067, 0.429854065340457, 0.4298548498088415, 0.429855701099393, 0.4298566174400863], [0.429895200951153, 0.42989382281570954, 0.4298924510697095, 0.4298910876700578, 0.42988973462068714, 0.42988839396868583, 0.4298870678000767, 0.4298857582352483, 0.42988446742404485, 0.4298831975405223, 0.4298819507773806, 0.4298807293400862, 0.429879535440701, 0.4298783712914376, 0.4298772390979617, 0.42987614105247013, 0.42987507932656877, 0.4298740560639847, 0.4298730733731444, 0.42987213331965435, 0.42987123791872317, 0.42987038912756387, 0.42986958883782034, 0.4298688388680595, 0.4298681409563746, 0.42986749675314473, 0.42986690781399606, 0.4298663755930093, 0.4298659014362197, 0.42986548657545287, 0.42986513212253863, 0.4298648390639455, 0.4298646082558727, 0.4298644404198381, 0.4298643361387942, 0.42986429585380304, 0.42986431986129714, 0.42986440831094885, 0.4298645612041677, 0.42986477839324, 0.42986505958112065, 0.42986540432188436, 0.42986581202183527, 0.4298662819412725, 0.42986681319690356, 0.4298674047648925, 0.4298680554845251, 0.42986876406247, 0.42986952907761017, 0.42987034898641485], [0.42990524564724875, 0.4299041280831526, 0.4299030168573831, 0.4299019135219648, 0.4299008196636369, 0.42989973690098904, 0.4298986668813334, 0.4298976112773143, 0.42989657178325574, 0.42989555011125036, 0.4298945479869951, 0.429893567145383, 0.42989260932585927, 0.4298916762675558, 0.4298907697042191, 0.42988989135894795, 0.4298890429387623, 0.429888226129025, 0.42988744258774036, 0.42988669393975787, 0.4298859817709073, 0.42988530762209887, 0.4298846729834179, 0.4298840792882489, 0.42988352790746365, 0.42988302014370866, 0.4298825572258279, 0.4298821403034576, 0.42988177044182874, 0.42988144861681343, 0.4298811757102503, 0.42988095250558234, 0.42988077968383964, 0.42988065781999796, 0.42988058737974166, 0.42988056871665686, 0.429880602069878, 0.4298806875622092, 0.4298808251987368, 0.42988101486594754, 0.4298812563313627, 0.42988154924369526, 0.42988189313353276, 0.42988228741454526, 0.4298827313852146, 0.4298832242310756, 0.4298837650274584, 0.42988435274271586, 0.429884986241917, 0.42988566429098496], [0.42991878567025127, 0.42991795745940753, 0.4299171329748529, 0.42991631329847557, 0.4299154995406196, 0.42991469283848577, 0.42991389435433025, 0.429913105273456, 0.4299123268019932, 0.4299115601644671, 0.4299108066011531, 0.4299100673652198, 0.42990934371966494, 0.429908636934048, 0.42990794828102924, 0.4299072790327228, 0.42990663045687755, 0.4299060038128988, 0.4299054003477274, 0.42990482129159513, 0.4299042678536752, 0.4299037412176516, 0.42990324253723033, 0.4299027729316175, 0.42990233348099194, 0.4299019252219999, 0.42990154914329903, 0.4299012061811841, 0.4299008972153203, 0.42990062306461774, 0.4299003844832727, 0.42990018215700865, 0.42990001669954164, 0.42989988864929995, 0.4298997984664219, 0.4298997465300567, 0.4298997331359905, 0.42989975849461765, 0.4298998227292743, 0.4298999258749508, 0.4299000678773937, 0.42990024859260817, 0.42990046778676705, 0.42990072513653005, 0.42990102022977406, 0.4299013525667314, 0.4299017215615314, 0.4299021265441356, 0.4299025667626567, 0.42990304138604496], [0.42993699367570726, 0.4299364881273108, 0.42993598000760047, 0.42993546985272113, 0.4299349582307399, 0.4299344457416232, 0.42993393301702504, 0.42993342071987506, 0.42993290954375674, 0.42993240021206675, 0.4299318934769486, 0.4299313901179951, 0.429930890940717, 0.42993039677477585, 0.4299299084719824, 0.42992942690406366, 0.4299289529602043, 0.4299284875443691, 0.4299280315724181, 0.4299275859690259, 0.4299271516644191, 0.42992672959095024, 0.4299263206795254, 0.42992592585590717, 0.4299255460369144, 0.4299251821265442, 0.4299248350120388, 0.42992450555992673, 0.42992419461206255, 0.429923902981695, 0.4299236314495901, 0.4299233807602383, 0.4299231516181724, 0.42992294468442477, 0.4299227605731493, 0.42992259984843384, 0.4299224630213272, 0.42992235054710315, 0.42992226282278234, 0.4299222001849306, 0.42992216290774987, 0.4299221512014769, 0.42992216521110116, 0.42992220501540945, 0.42992227062636756, 0.4299223619888389, 0.4299224789806433, 0.42992262141295556, 0.42992278903103676, 0.42992298151529446], [0.42996115358709774, 0.4299610174515097, 0.42996086778222986, 0.4299607044672108, 0.42996052744239943, 0.4299603366937048, 0.4299601322587291, 0.4299599142282372, 0.42995968274735086, 0.42995943801644676, 0.4299591802917459, 0.4299589098855814, 0.42995862716633393, 0.4299583325580283, 0.4299580265395861, 0.42995770964373237, 0.4299573824555577, 0.4299570456107401, 0.4299566997934324, 0.429956345733827, 0.4299559842054094, 0.429955616021916, 0.42995524203401586, 0.42995486312573583, 0.42995448021065247, 0.42995409422787667, 0.42995370613785655, 0.4299533169180293, 0.4299529275583501, 0.4299525390567305, 0.4299521524144172, 0.4299517686313443, 0.42995138870149074, 0.42995101360827537, 0.42995064432002184, 0.4299502817855235, 0.4299499269297392, 0.429949580649647, 0.4299492438102852, 0.4299489172410036, 0.42994860173194904, 0.4299482980308072, 0.4299480068398175, 0.42994772881307847, 0.429947464554157, 0.42994721461401186, 0.42994697948924065, 0.4299467596206546, 0.42994655539218474, 0.42994636713012024], [0.4299927723198776, 0.4299930843047457, 0.4299933665439125, 0.4299936181152401, 0.42999383817401665, 0.4299940259575307, 0.4299941807892905, 0.4299943020828544, 0.4299943893452426, 0.42999444217990246, 0.42999446028920263, 0.4299944434764361, 0.4299943916473129, 0.42999430481093015, 0.42999418308020954, 0.42999402667179615, 0.4299938359054169, 0.4299936112027024, 0.42999335308547765, 0.4299930621735331, 0.4299927391818915, 0.4299923849175876, 0.42999200027598516, 0.42999158623665545, 0.4299911438588471, 0.42999067427657894, 0.42999017869339035, 0.4299896583767858, 0.4299891146524127, 0.42998854889801186, 0.42998796253718263, 0.42998735703300484, 0.42998673388155884, 0.42998609460538795, 0.4299854407469435, 0.4299847738620556, 0.42998409551346783, 0.4299834072644766, 0.4299827106727113, 0.42998200728409, 0.429981298626985, 0.4299805862066268, 0.42997987149977596, 0.4299791559496865, 0.42997844096138393, 0.4299777278972763, 0.42997701807311417, 0.42997631275431347, 0.42997561315264954, 0.42997492042333024], [0.4300338632037082, 0.43003476730870444, 0.4300356202922728, 0.43003642014250115, 0.4300371649644764, 0.4300378529884421, 0.4300384825774379, 0.4300390522343722, 0.4300395606084808, 0.4300400065011278, 0.43004038887091256, 0.43004070683804807, 0.4300409596879833, 0.43004114687424744, 0.4300412680204986, 0.4300413229217668, 0.430041311544885, 0.4300412340281103, 0.4300410906799408, 0.4300408819771412, 0.4300406085619946, 0.4300402712388042, 0.43003987096967256, 0.430039408869593, 0.4300388862008886, 0.4300383043670425, 0.430037664905963, 0.4300369694827332, 0.43003621988189483, 0.43003541799932055, 0.4300345658337287, 0.43003366547789734, 0.4300327191096336, 0.4300317289825557, 0.43003069741674405, 0.4300296267893177, 0.4300285195249903, 0.43002737808665925, 0.43002620496607913, 0.4300250026746687, 0.430023773734497, 0.4300225206694931, 0.43002124599691904, 0.4300199522191439, 0.43001864181575244, 0.43001731723601766, 0.4300159808917655, 0.43001463515065347, 0.430013282329882, 0.43001192469035565], [0.4300876297595879, 0.43008939558881365, 0.43009108482046365, 0.430092693847273, 0.4300942192185578, 0.43009565765358937, 0.43009700605431844, 0.4300982615173715, 0.4300994213452512, 0.4301004830566722, 0.4301014443959751, 0.4301023033415631, 0.43010305811331595, 0.4301037071789416, 0.43010424925923185, 0.4301046833322004, 0.4301050086360836, 0.4301052246711966, 0.43010533120064426, 0.4301053282498908, 0.4301052161052049, 0.43010499531100044, 0.4301046666660997, 0.43010423121895613, 0.430103690261876, 0.430103045324285, 0.4301022981650933, 0.43010145076421424, 0.4301005053132968, 0.4300994642057363, 0.43009833002603115, 0.4300971055385524, 0.43009579367579964, 0.4300943975262131, 0.43009292032161595, 0.4300913654243593, 0.4300897363142402, 0.43008803657526695, 0.4300862698823378, 0.43008443998790324, 0.4300825507086761, 0.43008060591245234, 0.43007860950510224, 0.4300765654177886, 0.43007447759446493, 0.4300723499797024, 0.43007018650689205, 0.4300679910868628, 0.4300657675969528, 0.4300635198705689], [0.4301598781684627, 0.4301630122361472, 0.43016604257698127, 0.43016896304924573, 0.43017176768498266, 0.43017445071145927, 0.4301770065720417, 0.4301794299463645, 0.43018171576968933, 0.43018385925134794, 0.4301858558921723, 0.43018770150081964, 0.4301893922089084, 0.43019092448488644, 0.4301922951465633, 0.43019350137224477, 0.4301945407104171, 0.4301954110879384, 0.430196110816702, 0.4301966385987468, 0.43019699352979945, 0.430197175101241, 0.43019718320050115, 0.43019701810989064, 0.43019668050389326, 0.4301961714449444, 0.43019549237773297, 0.43019464512207195, 0.4301936318643862, 0.43019245514787774, 0.4301911178614309, 0.4301896232273272, 0.4301879747878449, 0.43018617639082163, 0.4301842321742634, 0.4301821465500857, 0.43017992418707685, 0.4301775699931729, 0.430175089097138, 0.4301724868297433, 0.43016976870453927, 0.4301669403983137, 0.4301640077313312, 0.43016097664744346, 0.4301578531941643, 0.4301546435027952, 0.43015135376868924, 0.43014799023173583, 0.43014455915714667, 0.43014106681661934], [0.43026157321149644, 0.430267023892002, 0.4302723466549053, 0.4302775310532535, 0.4302825667632222, 0.4302874436188945, 0.4302921516470583, 0.4302966811018538, 0.4303010224991056, 0.4303051666501696, 0.43030910469512695, 0.43031282813515803, 0.43031632886393045, 0.43031959919784113, 0.43032263190495224, 0.4303254202324707, 0.4303279579326242, 0.43033023928679354, 0.4303322591277713, 0.4303340128600237, 0.4303354964778422, 0.43033670658128376, 0.43033764038980765, 0.4303382957535291, 0.43033867116202396, 0.4303387657506291, 0.4303385793041989, 0.43033811225829033, 0.4303373656977648, 0.4303363413528074, 0.43033504159238023, 0.43033346941513995, 0.4303316284378651, 0.4303295228814504, 0.4303271575545431, 0.43032453783490576, 0.43032166964860585, 0.4303185594471427, 0.4303152141826362, 0.43031164128120997, 0.43030784861471416, 0.4303038444709395, 0.4302996375224847, 0.4302952367944435, 0.43029065163108515, 0.4302858916617063, 0.43028096676583294, 0.4302758870379576, 0.43027066275199133, 0.43026530432561544], [0.4304129960341517, 0.4304225223620175, 0.4304319080284784, 0.4304411349386865, 0.4304501849116954, 0.43045903973814903, 0.43046768123989393, 0.43047609133127007, 0.43048425208181906, 0.4304921457801272, 0.4304997549985018, 0.43050706265816674, 0.43051405209464155, 0.43052070712296153, 0.4305270121023818, 0.4305329520002002, 0.43053851245432734, 0.43054367983423014, 0.43054844129987213, 0.4305527848582775, 0.4305566994173522, 0.43056017483660025, 0.4305632019743916, 0.4305657727314452, 0.4305678800902165, 0.4305695181498953, 0.4305706821567459, 0.4305713685295497, 0.4305715748799396, 0.4305713000274478, 0.430570544009124, 0.43056930808361804, 0.43056759472965833, 0.43056540763889617, 0.43056275170312885, 0.43055963299595124, 0.430556058748929, 0.4305520373224249, 0.4305475781712496, 0.4305426918053445, 0.43053738974574235, 0.4305316844760833, 0.4305255893899963, 0.4305191187346839, 0.4305122875510741, 0.4305051116109253, 0.43049760735128717, 0.43048979180673747, 0.43048168253982394, 0.4304732975701474], [0.4306499065690827, 0.4306667187408648, 0.4306834055770491, 0.4306999351083283, 0.43071627472879503, 0.43073239129376456, 0.4307482512242995, 0.43076382061809465, 0.43077906536631383, 0.4307939512759028, 0.43080844419683484, 0.4308225101536811, 0.43083611548083545, 0.43084922696066263, 0.43086181196378637, 0.4308738385906803, 0.4308852758136843, 0.430896093618529, 0.43090626314442393, 0.43091575682174077, 0.4309245485063139, 0.4309326136093752, 0.43093992922214797, 0.4309464742341419, 0.43095222944421774, 0.43095717766352803, 0.43096130380948755, 0.43096459498998557, 0.4309670405771163, 0.43096863226978177, 0.43096936414460263, 0.4309692326946645, 0.4309682368557223, 0.43096637801958776, 0.43096366003453024, 0.4309600891926292, 0.4309556742041244, 0.43095042615892254, 0.43094435847552254, 0.4309374868377305, 0.43092982911963396, 0.43092140529940337, 0.4309122373625762, 0.430902349195564, 0.430891766470194, 0.43088051652016474, 0.4308686282103494, 0.4308561317999253, 0.4308430588003438, 0.43082944182917754], [0.43103185211354145, 0.4310616759920751, 0.43109145190598686, 0.43112112271040925, 0.4311506293640822, 0.43117991109647835, 0.43120890559258385, 0.4312375491949202, 0.43126577712221315, 0.4312935237039331, 0.43132072262974386, 0.43134730721271364, 0.431373210664958, 0.4313983663842078, 0.4314227082496266, 0.4314461709250399, 0.43146869016759887, 0.4314902031397647, 0.43151064872239353, 0.4315299678266089, 0.431548103702082, 0.43156500223929484, 0.4315806122633466, 0.43159488581687216, 0.43160777842968, 0.43161924937278295, 0.43162926189459094, 0.43163778343715486, 0.43164478583050186, 0.43165024546327435, 0.43165414342808395, 0.43165646564020665, 0.4316572029284859, 0.43165635109755696, 0.43165391096077366, 0.4316498883434867, 0.43164429405660265, 0.43163714384062746, 0.4316284582806754, 0.4316182626931929, 0.4316065869854046, 0.4315934654887349, 0.4315789367676895, 0.431563043405887, 0.43154583177112416, 0.431527351761518, 0.4315076565349102, 0.43148680222382807, 0.4314648476383848, 0.4314418539595518], [0.4316520951288888, 0.4317048486634727, 0.43175776343246297, 0.43181073735099196, 0.43186366373845936, 0.4319164315993593, 0.431968925945643, 0.43202102816036003, 0.4320726164018939, 0.43212356604766144, 0.4321737501756848, 0.4322230400819743, 0.4322713058311918, 0.43231841683759686, 0.43236424247282595, 0.43240865269661966, 0.4324515187062068, 0.43249271359968133, 0.432532113048378, 0.43256959597296946, 0.4326050452177792, 0.4326383482176407, 0.43266939765152784, 0.4326980920771504, 0.43272433654074877, 0.4327480431564334, 0.4327691316496021, 0.4327875298592284, 0.4328031741941435, 0.43281601003883635, 0.4328259921047515, 0.43283308472358617, 0.43283726207965373, 0.4328385083789887, 0.43283681795350926, 0.4328321952992211, 0.432824655048121, 0.43281422187414353, 0.4328009303341692, 0.43278482464577006, 0.4327659584040032, 0.43274439424016065, 0.432720203425937, 0.43269346542698217, 0.4326642674102514, 0.4326327037099502, 0.432598875257189, 0.4325628889787092, 0.43252485717022404, 0.43248489685001557], [0.4326472319665149, 0.4327394861070891, 0.4328323811541811, 0.4329257371497818, 0.4330193640007859, 0.43311306192833243, 0.4332066220087193, 0.4332998268067063, 0.43339245110106256, 0.4334842627011938, 0.43357502335260856, 0.4336644897278691, 0.4337524144985287, 0.433838547482415, 0.4339226368594735, 0.43400443044828124, 0.4340836770342791, 0.4341601277397759, 0.43423353742488, 0.43430366610771276, 0.4343702803915945, 0.4344331548863636, 0.434492073610623, 0.434546831361508, 0.4345972350385508, 0.4346431049083824, 0.4346842757973677, 0.4347205981998101, 0.43475193929009026, 0.434778183828001, 0.43479923494760886, 0.43481501482118273, 0.43482546519107546, 0.4348305477638974, 0.43483024446286106, 0.43482455753577465, 0.4348135095178023, 0.4347971430497488, 0.43477552055425617, 0.43474872377387586, 0.43471685317649383, 0.4346800272349996, 0.4346383815893919, 0.4345920681006806, 0.43454125380696146, 0.43448611979289536, 0.4344268599845057, 0.4343636798817153, 0.43429679524137066, 0.434226430723649], [0.4342020977137552, 0.43436034232911336, 0.4345202371981592, 0.4346814729262106, 0.43484371906007613, 0.4350066247407006, 0.43516981954784073, 0.43533291454147116, 0.4354955035026061, 0.43565716437401314, 0.4358174608989177, 0.4359759444532648, 0.43613215606447686, 0.4362856286069312, 0.4364358891616459, 0.4365824615249466, 0.43672486884823314, 0.4368626363884347, 0.4369952943463925, 0.4371223807682723, 0.4372434444832618, 0.4373580480492822, 0.4374657706772847, 0.43756621110394966, 0.4376589903822943, 0.43774375455983805, 0.43782017721458594, 0.4378879618201851, 0.4379468439131609, 0.43799659303714983, 0.43803701444146936, 0.43806795051418246, 0.4380892819329583, 0.43810092852046795, 0.4381028497947065, 0.43809504520844234, 0.43807755407589344, 0.4380504551886375, 0.4380138661266198, 0.4379679422738497, 0.4379128755519096, 0.4378488928876888, 0.4377762544347248, 0.43769525157016476, 0.43760620469158856, 0.4375094608397517, 0.43740539117468524, 0.4372943883335205, 0.43717686369889297, 0.4370532446068339], [0.43654209971375046, 0.4368064155578534, 0.4370743659996534, 0.4373454377788816, 0.4376190759956983, 0.4378946848917001, 0.4381716290140323, 0.43844923477867975, 0.43872679244506657, 0.43900355850962486, 0.4392787585210106, 0.43955159031422036, 0.4398212276550574, 0.44008682428028234, 0.44034751831248065, 0.44060243702227087, 0.4408507019041124, 0.4410914340257538, 0.44132375960545167, 0.44154681576561444, 0.4417597564066179, 0.4419617581403509, 0.4421520262196828, 0.4423298003976326, 0.44249436064863146, 0.44264503268400013, 0.44278119319465054, 0.4429022747560738, 0.4430077703339246, 0.44309723733286954, 0.44317030113682027, 0.4432266580950862, 0.4432660779162668, 0.4432884054397058, 0.4432935617628856, 0.4432815447120938, 0.4432524286528331, 0.4432063636456107, 0.44314357396172693, 0.443064355982312, 0.4429690755119686, 0.4428581645458057, 0.44273211753526825, 0.44259148720386665, 0.4424368799686036, 0.44226895102652664, 0.442088399168371, 0.44189596138269455, 0.44169240731427106, 0.44147853363983075], [0.4399021674916898, 0.4403291080102256, 0.4407633251744762, 0.44120400547238536, 0.4416502570916784, 0.4421011104509116, 0.44255551945247185, 0.4430123635024222, 0.44347045033511157, 0.4439285196721239, 0.4443852477354737, 0.44483925262403523, 0.4452891005501399, 0.44573331292023244, 0.44617037422965916, 0.4465987407272889, 0.44701684979104056, 0.44742312994079825, 0.4478160114009755, 0.44819393711151656, 0.4485553740737297, 0.4488988249064013, 0.4492228394784963, 0.449526026477706, 0.4498070647694419, 0.45006471439881546, 0.45029782708886246, 0.45050535609183096, 0.45068636525680505, 0.45084003718618454, 0.4509656803654696, 0.45106273516515194, 0.45113077863002937, 0.4511695279895477, 0.4511788428424387, 0.45115872598950324, 0.4511093229094013, 0.45103091989329014, 0.4509239408745691, 0.4507889430094486, 0.45062661108207713, 0.4504377508241991, 0.4502232812534541, 0.44998422614619726, 0.44972170476998463, 0.4494369220074864, 0.4491311580075595, 0.44880575750057283, 0.44846211891393783, 0.4481016834203024], [0.44446279955595747, 0.44512522639658614, 0.4458011196161274, 0.44648925777350934, 0.4471882801894217, 0.44789668623455114, 0.4486128358897745, 0.4493349516874031, 0.45006112213181215, 0.45078930668389566, 0.4515173423767172, 0.45224295210951326, 0.45296375464400757, 0.4536772763010053, 0.45438096432686903, 0.455072201869137, 0.45574832446887137, 0.45640663794493336, 0.45704443751309487, 0.4576590279515367, 0.45824774459472273, 0.45880797491082137, 0.45933718039463156, 0.45983291848923447, 0.4602928642360797, 0.46071483134557967, 0.4610967923790332, 0.46143689773811836, 0.46173349317044377, 0.4619851355185832, 0.4621906064653728, 0.4623489240594623, 0.4624593518414592, 0.46252140543160974, 0.46253485648374093, 0.4624997339560233, 0.46241632269575983, 0.46228515938169734, 0.4621070259119925, 0.4618829403679315, 0.4616141457217202, 0.46130209649028414, 0.4609484435653729, 0.4605550174728144, 0.4601238103302549, 0.45965695678299523, 0.459156714201705, 0.45862544242412223, 0.4580655833157107, 0.45747964041219563], [0.45025398783270937, 0.45123493672664305, 0.4522389827211424, 0.453264398518443, 0.45430922426042536, 0.4553712636479733, 0.4564480821378164, 0.45753700745117787, 0.45863513261507716, 0.4597393217371908, 0.4608462186883736, 0.4619522588331084, 0.46305368390729373, 0.4641465600951131, 0.465226799302804, 0.4662901835676967, 0.4673323924770597, 0.4683490334043774, 0.46933567430234546, 0.4702878787239908, 0.4712012426778801, 0.47207143286256503, 0.47289422577141726, 0.4736655471138874, 0.474381510965016, 0.47503845803338013, 0.47563299242998, 0.4761620163277114, 0.4766227619235524, 0.4770128201531948, 0.47733016565998815, 0.47757317758542983, 0.4777406558253322, 0.4778318324820432, 0.4778463783360972, 0.477784404257715, 0.47764645757666024, 0.47743351352527363, 0.47714696196115947, 0.47678858966053317, 0.4763605585484163, 0.4758653802958587, 0.47530588776593263, 0.47468520382846835, 0.4740067080881323, 0.4732740020815652, 0.4724908734975222, 0.47166125996021097, 0.4707892128916, 0.4698788619347667], [0.45704590468835615, 0.4584244092773444, 0.45983957524584484, 0.4612891044722517, 0.4627703356825805, 0.4642802344838798, 0.4658153865416735, 0.4673719943495942, 0.46894587802764864, 0.47053248056177127, 0.4721268778603021, 0.473723793951894, 0.4753176215836046, 0.4769024483974559, 0.47847208876913977, 0.48002012128483096, 0.4815399317131726, 0.4830247612017654, 0.4844677592942083, 0.48586204122868526, 0.4872007488464965, 0.4884771143136023, 0.4896845257449626, 0.49081659372512443, 0.4918672176436062, 0.4928306507142377, 0.4937015625269572, 0.4944750979907815, 0.4951469315689542, 0.4957133157811948, 0.49617112305210087, 0.4965178801160903, 0.49675179434372807, 0.49687177152686207, 0.4968774248447475, 0.49676907492405586, 0.4965477410958692, 0.49621512413623103, 0.4957735809477306, 0.49522609179291904, 0.4945762208219857, 0.4938280707439334, 0.4929862325706538, 0.4920557314159152, 0.49104196935674943, 0.48995066636418505, 0.48878780028584357, 0.4875595468173284, 0.48627222033569517, 0.48493221639004336], [0.46425384898225885, 0.4660830789878241, 0.4679661198037964, 0.4699000884854698, 0.47188157221810956, 0.473906608700387, 0.4759706709761367, 0.47806865748786453, 0.4801948881237502, 0.4823431070076737, 0.48450649273633006, 0.48667767669634643, 0.48884876999568294, 0.4910113994165717, 0.4931567526418907, 0.4952756328245895, 0.49735852236336864, 0.49939565552151155, 0.5013770992854343, 0.503292841612527, 0.5051328859726211, 0.5068873508535711, 0.5085465726890032, 0.5101012104853344, 0.5115423502851952, 0.5128616075137741, 0.5140512252199113, 0.5151041662491714, 0.5160141974734507, 0.5167759643492826, 0.5173850542810099, 0.51783804751802, 0.5181325546081017, 0.5182672397503199, 0.5182418297281421, 0.5180571084440505, 0.5177148974078267, 0.5172180228406705, 0.5165702703359405, 0.5157763282567869, 0.5148417212453021, 0.5137727353635133, 0.5125763364825462, 0.5112600835832901, 0.5098320386328556, 0.5083006746599618, 0.5066747835745751, 0.5049633851685558, 0.5031756386010926, 0.5013207575215939], [0.47086389849264976, 0.4731426581398605, 0.47549447735895534, 0.4779159581002136, 0.48040298031478595, 0.4829506689365267, 0.48555336676234717, 0.4882046144536847, 0.4908971389029019, 0.4936228511975421, 0.4963728553667036, 0.4991374690021831, 0.5019062567081622, 0.5046680771442018, 0.5074111441861012, 0.5101231024390114, 0.5127911170011537, 0.515401977001456, 0.5179422120305999, 0.5203982201657329, 0.522756405870159, 0.5250033256490305, 0.5271258389795592, 0.5291112617292011, 0.5309475190455827, 0.5326232945636012, 0.5341281727397592, 0.5354527711982163, 0.5365888601583756, 0.5375294663046946, 0.5382689588451128, 0.5388031159682516, 0.5391291704308794, 0.5392458335626624, 0.5391532975404637, 0.5388532163354605, 0.5383486662513921, 0.5376440874329459, 0.5367452081153647, 0.5356589537002542, 0.5343933429737076, 0.532957373930146, 0.5313609017326092, 0.5296145113332238, 0.5277293872050319, 0.5257171825077953, 0.5235898898363145, 0.5213597154907146, 0.5190389589742366, 0.5166398991745837], [0.47538228311865527, 0.47801771603236487, 0.48074438364651273, 0.48355859463117146, 0.4864557498012487, 0.48943029282375966, 0.4924756680638638, 0.49558428732263105, 0.4987475072823965, 0.501955619493468, 0.5051978546991117, 0.5084624031946788, 0.5117364527429827, 0.5150062453152356, 0.5182571535913684, 0.5214737777352376, 0.5246400624639049, 0.527739433865416, 0.5307549548018087, 0.5336694970847448, 0.5364659279560192, 0.5391273077753664, 0.5416370952462298, 0.5439793560304389, 0.5461389702469642, 0.5481018341452742, 0.549855051210863, 0.5513871081099468, 0.5526880312124544, 0.5537495199359862, 0.5545650538063237, 0.5551299709003843, 0.5554415161867399, 0.5554988591640829, 0.5553030810760732, 0.5548571328112702, 0.5541657653439718, 0.5532354352089512, 0.5520741880110718, 0.5506915233404954, 0.5490982446950121, 0.5473062981085892, 0.5453286031625917, 0.5431788799286533, 0.5408714751782623, 0.5384211909129843, 0.5358431179389258, 0.5331524768468942, 0.5303644683811344, 0.5274941347972283], [0.4759595090477854, 0.4787265056955229, 0.48159585749924677, 0.48456396919703926, 0.48762622449948134, 0.4907769224744294, 0.4940092215069039, 0.49731509300937793, 0.5006852871716831, 0.504109313100649, 0.5075754356937963, 0.5110706915051548, 0.5145809256815628, 0.5180908517643513, 0.5215841357564476, 0.5250435053461345, 0.5284508845592817, 0.5317875533920928, 0.5350343311748865, 0.5381717815603236, 0.5411804361517258, 0.5440410329296854, 0.5467347648442527, 0.5492435332635633, 0.5515502004545006, 0.5536388349581334, 0.5554949436440526, 0.5571056844027611, 0.5584600538676716, 0.5595490452352889, 0.5603657721443066, 0.5609055556375103, 0.5611659724092699, 0.5611468637729535, 0.5608503060030867, 0.5602805438548273, 0.5594438900852925, 0.5583485946554406, 0.5570046879496537, 0.5554238027983915, 0.5536189803279649, 0.5516044647017545, 0.5493954916808871, 0.5470080756470741, 0.5444587993273344, 0.5417646099723539, 0.5389426251979913, 0.5360099511313292, 0.532983514933165, 0.5298799132175749], [0.4710417908506418, 0.47358224738604016, 0.4762212747984392, 0.4789557879965703, 0.481781721707101, 0.4846939642251341, 0.4876862981079856, 0.49075135000999914, 0.4938805519905197, 0.49706411671013584, 0.5002910289475597, 0.5035490558063299, 0.506824777822901, 0.5101036429238939, 0.5133700448011993, 0.5166074267750356, 0.5197984115986066, 0.5229249569321749, 0.5259685353963774, 0.5289103372295644, 0.5317314926562248, 0.5344133101645923, 0.5369375260388694, 0.5392865597457259, 0.5414437691862432, 0.54339369943959, 0.5451223184809874, 0.5466172334798051, 0.5478678816837915, 0.5488656905650835, 0.5496042028171917, 0.5500791629068764, 0.5502885631444359, 0.5502326485731233, 0.5499138813225719, 0.5493368663515286, 0.5485082416581731, 0.5474365370091661, 0.546132005993287, 0.5446064367198851, 0.5428729467515176, 0.5409457678936573, 0.5388400262851354, 0.5365715228733328, 0.5341565188559712, 0.531611530067938, 0.528953133626436, 0.52619778945814, 0.5233616786500337, 0.5204605599170585], [0.4605525188366774, 0.46247650925712835, 0.4644769233004947, 0.46655149253238515, 0.4686971948860998, 0.47091020350949603, 0.47318584103485717, 0.47551854095662743, 0.4779018178970444, 0.4803282485947621, 0.4827894654560273, 0.4852761644513409, 0.4877781290129306, 0.4902842713816499, 0.4927826925601631, 0.4952607616499022, 0.49770521488423664, 0.5001022741261827, 0.5024377839888338, 0.5046973660787182, 0.5068665881815412, 0.5089311455359373, 0.510877050707939, 0.5126908280231472, 0.5143597080711662, 0.5158718175013464, 0.5172163592078312, 0.5183837780751737, 0.5193659077320844, 0.5201560942370219, 0.5207492932792609, 0.5211421382941023, 0.5213329778224165, 0.5213218814448477, 0.5211106146388309, 0.5207025838893103, 0.520102754282934, 0.5193175425884934, 0.5183546894412722, 0.5172231146847547, 0.5159327601716936, 0.5144944243908514, 0.5129195931798363, 0.5112202705304447, 0.509408813118611, 0.5074977717279903, 0.5054997422156802, 0.5034272281212993, 0.5012925164734172, 0.49910756782296617], [0.44663819991908743, 0.44770240707850073, 0.44880945169217146, 0.44995810720187285, 0.45114673923981746, 0.4523732800732412, 0.45363520623762144, 0.45492952020457306, 0.45625273695891744, 0.4576008763658622, 0.4589694621894048, 0.4603535285729278, 0.46174763470899294, 0.4631458883051971, 0.46454197829537136, 0.46592921705076296, 0.467300592116658, 0.46864882724048773, 0.4699664521749016, 0.47124588044241195, 0.4724794939484249, 0.47365973303962783, 0.47477919033882, 0.47583070645987147, 0.47680746553145315, 0.47770308834805303, 0.478511720931328, 0.47922811633076967, 0.4798477076222003, 0.48036667027359276, 0.48078197233314957, 0.48109141124279964, 0.4812936364762639, 0.4813881576257882, 0.48137533799602, 0.48125637418672357, 0.48103326253880335, 0.4807087536636401, 0.48028629656039795, 0.4797699740402167, 0.479164431315254, 0.47847479967370377, 0.4777066171529419, 0.47686574804889037, 0.47595830297017033, 0.4749905609723831, 0.4739688951029475, 0.47289970246269564, 0.471789339658453, 0.4706440642910583], [0.43290504896971793, 0.4331297731908793, 0.4333657948923862, 0.4336130422683485, 0.43387136852039027, 0.4341405476794765, 0.43442027107808395, 0.4347101445726288, 0.4350096866059666, 0.4353183271861729, 0.435635407840924, 0.4359601825871601, 0.4362918199338594, 0.4366294059127389, 0.4369719481084238, 0.43731838063731915, 0.43766757000429773, 0.43801832174944677, 0.43836938778461454, 0.4387194743119409, 0.439067250214463, 0.4394113558121292, 0.43975041188464836, 0.44008302887456985, 0.4404078161984369, 0.44072339160922475, 0.44102839056755805, 0.4413214755908521, 0.4416013455568354, 0.44186674493987527, 0.4421164729545204, 0.4423493925708144, 0.4425644393510796, 0.4427606300392192, 0.4429370708132363, 0.44309296509164653, 0.4432276207670507, 0.4433404567273942, 0.44343100851917994, 0.4434989330080885, 0.4435440119018334, 0.44356615401733424, 0.4435653961987166, 0.44354190282293005, 0.4434959638641059, 0.4434279915243948, 0.4433385154757463, 0.443228176792168, 0.44309772068376385, 0.4429479881709006], [0.422549222546284, 0.42217525829041747, 0.4217929026845242, 0.4214030942292301, 0.42100691056650186, 0.4206055734478682, 0.42020045233718156, 0.4197930663675721, 0.41938508436451893, 0.418978322646986, 0.41857474032802466, 0.4181764318566925, 0.4177856165759495, 0.41740462511746057, 0.41703588251455725, 0.4166818879889742, 0.4163451914547074, 0.41602836688181083, 0.41573398277176266, 0.4154645701106697, 0.4152225882828202, 0.41501038953985864, 0.41483018272442546, 0.4146839970356927, 0.41457364669184443, 0.4145006973859968, 0.4144664354426536, 0.4144718405585661, 0.414517562953123, 0.4146039056595865, 0.4147308125619589, 0.41489786262736794, 0.4151042706068651, 0.4153488942859421, 0.4156302481686943, 0.4159465232853615, 0.4162956126309674, 0.41667514158106944, 0.4170825024962385, 0.417514892624941, 0.41796935434837573, 0.4184428167817066, 0.4189321377533097, 0.41943414522447076, 0.41994567728234977, 0.42046361993376735, 0.42098494204042447, 0.4215067268613876, 0.42202619979985395, 0.4225407520826607], [0.41694231052614494, 0.41628170949336174, 0.41560591422937665, 0.41491646483023564, 0.414215115792103, 0.41350384055533373, 0.41278483377632624, 0.41206051100039665, 0.41133350541833924, 0.4106066614089126, 0.4098830246003573, 0.4091658282271312, 0.4084584756138349, 0.40776451868699604, 0.4070876324964972, 0.40643158582100325, 0.40580020803404826, 0.4051973525170909, 0.40462685701974743, 0.4040925014818031, 0.4035979639422044, 0.40314677526237275, 0.40274227347994396, 0.4023875586796615, 0.4020854493160847, 0.40183844094426646, 0.4016486683065448, 0.40151787168441955, 0.4014473683536909, 0.4014380298797342, 0.40149026586061654, 0.40160401457281325, 0.4017787408029744, 0.4020134409659244, 0.4023066554210024, 0.4026564877133607, 0.4030606302912612, 0.4035163960914233, 0.40402075524805525, 0.4045703760718469, 0.40516166936626274, 0.4057908351015418, 0.40645391045211765, 0.4071468182193646, 0.40786541470606785, 0.40860553617826034, 0.4093630431395714, 0.4101338617482329, 0.4109140218222994, 0.41169969099954185], [0.4155818393261589, 0.41488584776819026, 0.4141760226925691, 0.4134540292921369, 0.41272172893164144, 0.41198118016860397, 0.4112346375907299, 0.4104845482466605, 0.40973354546582696, 0.4089844398896776, 0.4082402075709489, 0.4075039750401015, 0.4067790012883702, 0.4060686566746715, 0.40537639882805376, 0.40470574568731665, 0.4040602458933248, 0.4034434468254975, 0.4028588606497684, 0.40230992881843275, 0.40179998553011814, 0.40133222071778735, 0.40090964318152256, 0.4005350445182636, 0.40021096452039273, 0.39993965871735, 0.39972306871798613, 0.3995627959756797, 0.39946007954351015, 0.3994157783140341, 0.3994303581492734, 0.39950388420391403, 0.39963601863167036, 0.39982602374494347, 0.4000727705753623, 0.4003747526616901, 0.4007301047760409, 0.40113662619319956, 0.40159180801451577, 0.40209286398013566, 0.40263676414334504, 0.4032202707398616, 0.40383997556350704, 0.4044923381575757, 0.4051737241473442, 0.4058804430719397, 0.40660878512102916, 0.40735505624100254, 0.40811561114375516, 0.4088868838259724], [0.41698334331361875, 0.4163921793376466, 0.41579145616958113, 0.4151826235391223, 0.41456727316245556, 0.41394713718243625, 0.41332408499181283, 0.41270011832708375, 0.41207736453854005, 0.41145806796417894, 0.41084457936143115, 0.410239343380859, 0.4096448840998306, 0.4090637886711683, 0.40849868918126786, 0.407952242853368, 0.40742711077355687, 0.406925935358621, 0.4064513168247913, 0.40600578895348427, 0.40559179448302873, 0.4052116604827577, 0.40486757408654656, 0.404561558975823, 0.40429545300630987, 0.404070887367731, 0.4038892676509459, 0.4037517571725239, 0.40365926287287734, 0.4036124240614064, 0.4036116042316521, 0.40365688611250955, 0.40374807005966384, 0.4038846758263518, 0.40406594768621407, 0.40429086281533494, 0.40455814277751057, 0.40486626789813984, 0.40521349425956155, 0.4055978730055587, 0.4060172716062058, 0.406469396707047, 0.406951818169175, 0.407461993899206, 0.40799729507017357, 0.40855503134542026, 0.40913247573682476, 0.40972688875514796, 0.4103355415426827, 0.4109557377154997]], \"zauto\": true, \"zmax\": 0.5611659724092699, \"zmin\": -0.5611659724092699}, {\"autocolorscale\": false, \"autocontour\": true, \"colorbar\": {\"tickfont\": {\"size\": 8}, \"ticksuffix\": \"\", \"x\": 1, \"y\": 0.5}, \"colorscale\": [[0.0, \"rgb(255,247,251)\"], [0.14285714285714285, \"rgb(236,231,242)\"], [0.2857142857142857, \"rgb(208,209,230)\"], [0.42857142857142855, \"rgb(166,189,219)\"], [0.5714285714285714, \"rgb(116,169,207)\"], [0.7142857142857143, \"rgb(54,144,192)\"], [0.8571428571428571, \"rgb(5,112,176)\"], [1.0, \"rgb(3,78,123)\"]], \"contours\": {\"coloring\": \"heatmap\"}, \"hoverinfo\": \"x+y+z\", \"ncontours\": 25, \"type\": \"contour\", \"x\": [4.0, 5.224489795918368, 6.448979591836735, 7.673469387755102, 8.89795918367347, 10.122448979591837, 11.346938775510203, 12.571428571428571, 13.795918367346939, 15.020408163265307, 16.244897959183675, 17.46938775510204, 18.693877551020407, 19.918367346938776, 21.142857142857142, 22.367346938775512, 23.591836734693878, 24.816326530612244, 26.040816326530614, 27.26530612244898, 28.48979591836735, 29.714285714285715, 30.93877551020408, 32.163265306122454, 33.38775510204081, 34.61224489795919, 35.83673469387755, 37.06122448979592, 38.285714285714285, 39.51020408163265, 40.734693877551024, 41.95918367346939, 43.183673469387756, 44.40816326530612, 45.63265306122449, 46.85714285714286, 48.08163265306123, 49.30612244897959, 50.53061224489796, 51.755102040816325, 52.9795918367347, 54.204081632653065, 55.42857142857143, 56.6530612244898, 57.87755102040816, 59.10204081632653, 60.3265306122449, 61.55102040816327, 62.775510204081634, 64.0], \"xaxis\": \"x2\", \"y\": [1e-08, 1.3894954943731388e-08, 1.9306977288832496e-08, 2.682695795279727e-08, 3.7275937203149377e-08, 5.1794746792312124e-08, 7.196856730011513e-08, 1e-07, 1.389495494373136e-07, 1.9306977288832497e-07, 2.6826957952797275e-07, 3.727593720314938e-07, 5.179474679231213e-07, 7.196856730011514e-07, 1e-06, 1.389495494373136e-06, 1.9306977288832498e-06, 2.6826957952797274e-06, 3.727593720314938e-06, 5.179474679231202e-06, 7.196856730011514e-06, 1e-05, 1.3894954943731361e-05, 1.9306977288832496e-05, 2.6826957952797274e-05, 3.727593720314938e-05, 5.179474679231202e-05, 7.196856730011514e-05, 0.0001, 0.0001389495494373136, 0.00019306977288832496, 0.0002682695795279722, 0.0003727593720314938, 0.0005179474679231213, 0.0007196856730011514, 0.001, 0.001389495494373136, 0.0019306977288832496, 0.002682695795279722, 0.003727593720314938, 0.005179474679231202, 0.007196856730011514, 0.01, 0.01389495494373136, 0.019306977288832496, 0.026826957952797218, 0.03727593720314938, 0.05179474679231202, 0.07196856730011514, 0.1], \"yaxis\": \"y2\", \"z\": [[0.24289428513638084, 0.24289428281583741, 0.2428942804359389, 0.2428942779991501, 0.24289427550833795, 0.24289427296678814, 0.24289427037821973, 0.24289426774679768, 0.24289426507714257, 0.24289426237433734, 0.24289425964393063, 0.24289425689193628, 0.2428942541248287, 0.24289425134953394, 0.2428942485734157, 0.24289424580425695, 0.24289424305023558, 0.24289424031989568, 0.24289423762211287, 0.2428942349660545, 0.24289423236113486, 0.24289422981696496, 0.24289422734329802, 0.2428942249499701, 0.24289422264683716, 0.24289422044370834, 0.24289421835027658, 0.24289421637604697, 0.2428942145302638, 0.2428942128218368, 0.2428942112592674, 0.2428942098505766, 0.24289420860323419, 0.24289420752409094, 0.24289420661931493, 0.24289420589433178, 0.24289420535377074, 0.2428942050014167, 0.2428942048401692, 0.2428942048720087, 0.24289420509797094, 0.24289420551812974, 0.24289420613158813, 0.2428942069364789, 0.24289420792997346, 0.2428942091083001, 0.24289421046677037, 0.24289421199981445, 0.24289421370102393, 0.2428942155632022], [0.24289427336228558, 0.24289427023996246, 0.24289426702261657, 0.2428942637129975, 0.24289426031446984, 0.24289425683104332, 0.24289425326739975, 0.2428942496289164, 0.2428942459216854, 0.24289424215252828, 0.24289423832900486, 0.24289423445941655, 0.24289423055280288, 0.2428942266189309, 0.2428942226682774, 0.24289421871200292, 0.24289421476191786, 0.2428942108304401, 0.24289420693054423, 0.2428942030757024, 0.24289419927981679, 0.2428941955571444, 0.24289419192221387, 0.24289418838973567, 0.24289418497450577, 0.24289418169130375, 0.2428941785547866, 0.24289417557937865, 0.24289417277915937, 0.2428941701677498, 0.24289416775819947, 0.24289416556287435, 0.242894163593348, 0.24289416186029703, 0.24289416037340167, 0.24289415914125403, 0.2428941581712739, 0.24289415746963428, 0.24289415704119724, 0.24289415688946112, 0.24289415701651995, 0.24289415742303558, 0.2428941581082231, 0.2428941590698498, 0.24289416030424751, 0.24289416180633874, 0.24289416356967578, 0.24289416558649243, 0.2428941678477681, 0.2428941703433027], [0.24289426342237114, 0.2428942595389116, 0.24289425552266763, 0.2428942513765036, 0.24289424710412055, 0.24289424271010077, 0.24289423819994863, 0.24289423358012646, 0.24289422885808454, 0.2428942240422846, 0.2428942191422157, 0.242894214168402, 0.24289420913240117, 0.2428942040467931, 0.24289419892515832, 0.24289419378204524, 0.24289418863292622, 0.24289418349414177, 0.2428941783828334, 0.24289417331686422, 0.2428941683147286, 0.2428941633954501, 0.24289415857846955, 0.24289415388352256, 0.24289414933050896, 0.24289414493935393, 0.24289414072986307, 0.24289413672157215, 0.24289413293359385, 0.24289412938446242, 0.24289412609197866, 0.24289412307305666, 0.24289412034357458, 0.24289411791823093, 0.24289411581040868, 0.24289411403204852, 0.24289411259353363, 0.24289411150358683, 0.2428941107691822, 0.24289411039547204, 0.24289411038573047, 0.2428941107413145, 0.24289411146164283, 0.2428941125441934, 0.2428941139845192, 0.24289411577628253, 0.2428941179113068, 0.24289412037964625, 0.24289412316967127, 0.24289412626816928], [0.2428942572309644, 0.24289425278530546, 0.24289424817555202, 0.24289424340453236, 0.24289423847609226, 0.24289423339515218, 0.24289422816775977, 0.24289422280113618, 0.24289421730371547, 0.24289421168517566, 0.2428942059564607, 0.24289420012979196, 0.24289419421866898, 0.24289418823785752, 0.24289418220336526, 0.24289417613240372, 0.24289417004333638, 0.24289416395561222, 0.24289415788968505, 0.24289415186691815, 0.242894145909475, 0.24289414004019605, 0.24289413428246262, 0.24289412866004886, 0.24289412319696252, 0.24289411791727653, 0.24289411284495233, 0.2428941080036573, 0.24289410341657772, 0.2428940991062297, 0.24289409509427015, 0.24289409140131, 0.24289408804673202, 0.24289408504851565, 0.24289408242307103, 0.24289408018508454, 0.24289407834737764, 0.24289407692078155, 0.24289407591402906, 0.242894075333665, 0.2428940751839772, 0.2428940754669483, 0.2428940761822295, 0.24289407732713678, 0.24289407889666945, 0.24289408088355086, 0.2428940832782909, 0.2428940860692694, 0.24289408924283917, 0.24289409278344748], [0.2428942561910916, 0.24289425151937288, 0.2428942466669899, 0.24289424163664095, 0.24289423643213548, 0.2428942310584577, 0.2428942255218252, 0.24289421982974074, 0.2428942139910361, 0.24289420801590722, 0.24289420191593894, 0.2428941957041187, 0.2428941893948376, 0.24289418300387855, 0.24289417654839027, 0.2428941700468461, 0.24289416351898824, 0.24289415698575548, 0.24289415046919585, 0.24289414399236292, 0.24289413757919698, 0.24289413125439113, 0.24289412504324331, 0.24289411897149513, 0.24289411306515876, 0.2428941073503338, 0.2428941018530149, 0.2428940965988936, 0.2428940916131545, 0.24289408692027012, 0.24289408254379505, 0.24289407850616332, 0.24289407482849004, 0.24289407153038142, 0.24289406862975418, 0.2428940661426679, 0.24289406408317177, 0.24289406246316841, 0.24289406129229626, 0.24289406057783255, 0.2428940603246182, 0.24289406053500526, 0.2428940612088286, 0.2428940623434014, 0.2428940639335351, 0.2428940659715834, 0.24289406844750958, 0.24289407134897673, 0.24289407466145888, 0.24289407836837243], [0.2428942605907497, 0.24289425608562573, 0.24289425140232462, 0.24289424654337996, 0.24289424151241917, 0.2428942363142258, 0.24289423095479648, 0.24289422544139128, 0.24289421978257614, 0.2428942139882565, 0.242894208069701, 0.24289420203955372, 0.2428941959118348, 0.24289418970192764, 0.24289418342655253, 0.24289417710372577, 0.2428941707527039, 0.24289416439391248, 0.24289415804885975, 0.24289415174003476, 0.24289414549079072, 0.2428941393252135, 0.24289413326797707, 0.24289412734418567, 0.2428941215792051, 0.24289411599848357, 0.24289411062736488, 0.24289410549089469, 0.24289410061362296, 0.24289409601940404, 0.2428940917311969, 0.2428940877708683, 0.2428940841590006, 0.24289408091470746, 0.24289407805545896, 0.24289407559691917, 0.24289407355279816, 0.2428940719347202, 0.24289407075211025, 0.2428940700121005, 0.2428940697194578, 0.24289406987653347, 0.2428940704832362, 0.24289407153702802, 0.24289407303294427, 0.2428940749636363, 0.2428940773194372, 0.2428940800884493, 0.242894083256652, 0.2428940868080291], [0.24289426937963574, 0.24289426538086212, 0.24289426122378505, 0.24289425691077188, 0.24289425244516627, 0.2428942478313416, 0.24289424307474994, 0.24289423818196432, 0.2428942331607144, 0.242894228019914, 0.24289422276967954, 0.24289421742133888, 0.24289421198742922, 0.24289420648168364, 0.2428942009190055, 0.24289419531543013, 0.24289418968807358, 0.24289418405506788, 0.24289417843548322, 0.24289417284923656, 0.24289416731698757, 0.24289416186002197, 0.24289415650012322, 0.24289415125943345, 0.24289414616030444, 0.2428941412251407, 0.24289413647623503, 0.24289413193559947, 0.24289412762479226, 0.2428941235647435, 0.2428941197755813, 0.24289411627646043, 0.24289411308539538, 0.2428941102191005, 0.24289410769283865, 0.24289410552028065, 0.24289410371337763, 0.24289410228224742, 0.2428941012350769, 0.24289410057804214, 0.24289410031524616, 0.24289410044867704, 0.24289410097818492, 0.24289410190148017, 0.24289410321415142, 0.24289410490970398, 0.2428941069796177, 0.2428941094134238, 0.24289411219879956, 0.24289411532167948], [0.2428942805175274, 0.24289427722481952, 0.24289427380442571, 0.24289427025855284, 0.24289426659020508, 0.24289426280322451, 0.24289425890232744, 0.2428942548931356, 0.24289425078220162, 0.24289424657702788, 0.24289424228607787, 0.24289423791877998, 0.24289423348552225, 0.24289422899763813, 0.24289422446738265, 0.24289421990789853, 0.24289421533317193, 0.24289421075797787, 0.24289420619781535, 0.24289420166883183, 0.2428941971877381, 0.24289419277171348, 0.24289418843830185, 0.24289418420529985, 0.2428941800906376, 0.24289417611225306, 0.24289417228796162, 0.24289416863532182, 0.24289416517149856, 0.24289416191312588, 0.24289415887616994, 0.2428941560757949, 0.24289415352623248, 0.24289415124065694, 0.24289414923106772, 0.24289414750818028, 0.24289414608132737, 0.24289414495837172, 0.24289414414563132, 0.2428941436478184, 0.2428941434679929, 0.24289414360753123, 0.24289414406611068, 0.2428941448417095, 0.24289414593062317, 0.2428941473274963, 0.24289414902536965, 0.24289415101574224, 0.24289415328864666, 0.24289415583273805], [0.24289429173383736, 0.24289428917996853, 0.24289428653143774, 0.242894283790297, 0.2428942809592034, 0.2428942780414461, 0.24289427504097008, 0.2428942719623956, 0.24289426881103313, 0.24289426559289354, 0.24289426231469236, 0.24289425898384845, 0.24289425560847594, 0.24289425219736963, 0.24289424875998342, 0.2428942453064012, 0.24289424184730063, 0.2428942383939095, 0.2428942349579545, 0.24289423155160308, 0.24289422818739803, 0.24289422487818577, 0.2428942216370384, 0.24289421847717016, 0.2428942154118493, 0.24289421245430556, 0.24289420961763478, 0.24289420691470107, 0.2428942043580379, 0.24289420195974898, 0.2428941997314103, 0.24289419768397394, 0.24289419582767563, 0.2428941941719462, 0.242894192725329, 0.24289419149540353, 0.2428941904887169, 0.2428941897107235, 0.24289418916573396, 0.24289418885687442, 0.2428941887860556, 0.24289418895395365, 0.24289418936000157, 0.24289419000239226, 0.24289419087809297, 0.24289419198287082, 0.24289419331132914, 0.24289419485695418, 0.24289419661217182, 0.24289419856841313], [0.2428943012859126, 0.24289429937206955, 0.24289429739235854, 0.24289429534868615, 0.24289429324339717, 0.2428942910792904, 0.24289428885963135, 0.24289428658816223, 0.24289428426910836, 0.24289428190718101, 0.242894279507576, 0.24289427707596847, 0.24289427461850266, 0.2428942721417774, 0.24289426965282654, 0.2428942671590948, 0.2428942646684082, 0.24289426218893995, 0.24289425972917153, 0.24289425729784894, 0.2428942549039346, 0.24289425255655528, 0.24289425026494602, 0.242894248038391, 0.2428942458861615, 0.24289424381745164, 0.24289424184131236, 0.24289423996658469, 0.24289423820183226, 0.24289423655527503, 0.2428942350347233, 0.2428942336475143, 0.24289423240045122, 0.24289423129974555, 0.24289423035096394, 0.24289422955897924, 0.2428942289279273, 0.2428942284611699, 0.24289422816126363, 0.242894228029936, 0.24289422806806865, 0.24289422827568788, 0.24289422865196283, 0.24289422919521117, 0.24289422990291243, 0.2428942307717287, 0.24289423179753225, 0.24289423297544038, 0.24289423429985615, 0.24289423576451522], [0.24289430834998368, 0.24289430691429184, 0.24289430543411042, 0.2428943039112153, 0.24289430234769738, 0.24289430074597015, 0.24289429910877514, 0.24289429743918495, 0.24289429574060398, 0.2428942940167661, 0.24289429227172976, 0.2428942905098699, 0.24289428873586683, 0.24289428695469178, 0.2428942851715895, 0.24289428339205732, 0.24289428162182125, 0.24289427986680873, 0.24289427813311867, 0.24289427642698833, 0.2428942747547579, 0.24289427312283243, 0.24289427153764187, 0.24289427000559924, 0.24289426853305743, 0.24289426712626513, 0.24289426579132198, 0.2428942645341339, 0.24289426336036846, 0.2428942622754113, 0.24289426128432381, 0.2428942603918025, 0.24289425960214087, 0.24289425891919367, 0.24289425834634457, 0.24289425788647703, 0.24289425754194968, 0.24289425731457512, 0.24289425720560362, 0.24289425721571165, 0.24289425734499484, 0.24289425759296623, 0.2428942579585592, 0.24289425844013562, 0.24289425903549872, 0.24289425974191056, 0.2428942605561142, 0.24289426147436022, 0.2428942624924365, 0.2428942636057026], [0.24289431294754607, 0.24289431183037402, 0.2428943106827525, 0.2428943095063379, 0.2428943083030209, 0.24289430707492915, 0.24289430582442859, 0.24289430455412234, 0.24289430326684808, 0.24289430196567321, 0.24289430065388765, 0.24289429933499468, 0.24289429801269957, 0.2428942966908956, 0.24289429537364818, 0.24289429406517657, 0.2428942927698337, 0.2428942914920835, 0.24289429023647693, 0.24289428900762594, 0.24289428781017577, 0.2428942866487762, 0.24289428552805162, 0.24289428445257025, 0.2428942834268127, 0.24289428245514064, 0.24289428154176512, 0.24289428069071556, 0.24289427990580942, 0.24289427919062279, 0.24289427854846266, 0.24289427798234034, 0.24289427749494757, 0.24289427708863423, 0.24289427676538902, 0.2428942765268228, 0.24289427637415462, 0.24289427630820143, 0.24289427632937052, 0.24289427643765563, 0.24289427663263638, 0.242894276913481, 0.24289427727895296, 0.24289427772742006, 0.24289427825686766, 0.24289427886491433, 0.24289427954883072, 0.24289428030556098, 0.24289428113174658, 0.24289428202375246], [0.24289431556688482, 0.24289431465020064, 0.24289431371183182, 0.24289431275331144, 0.24289431177635595, 0.24289431078286577, 0.24289430977492413, 0.24289430875479465, 0.24289430772491705, 0.24289430668790138, 0.24289430564652043, 0.24289430460370032, 0.24289430356250957, 0.24289430252614608, 0.24289430149792285, 0.24289430048125177, 0.24289429947962582, 0.2428942984966003, 0.24289429753577207, 0.24289429660075826, 0.24289429569517354, 0.242894294822607, 0.2428942939865981, 0.2428942931906125, 0.2428942924380177, 0.2428942917320586, 0.24289429107583385, 0.24289429047227254, 0.24289428992411166, 0.24289428943387492, 0.24289428900385285, 0.24289428863608445, 0.24289428833234053, 0.2428942880941094, 0.24289428792258413, 0.2428942878186528, 0.24289428778289052, 0.2428942878155546, 0.2428942879165817, 0.24289428808558816, 0.2428942883218725, 0.24289428862442064, 0.24289428899191406, 0.24289428942273972, 0.24289428991500278, 0.24289429046654137, 0.24289429107494329, 0.24289429173756474, 0.24289429245155034, 0.2428942932138549], [0.24289431673006862, 0.2428943159473528, 0.24289431514914206, 0.2428943143368234, 0.24289431351193047, 0.24289431267614287, 0.24289431183128463, 0.2428943109793216, 0.24289431012235713, 0.24289430926262637, 0.24289430840248957, 0.24289430754442348, 0.2428943066910115, 0.24289430584493257, 0.24289430500894837, 0.2428943041858895, 0.24289430337864049, 0.24289430259012318, 0.2428943018232799, 0.24289430108105495, 0.242894300366376, 0.24289429968213452, 0.24289429903116613, 0.24289429841623061, 0.24289429783999186, 0.24289429730499826, 0.24289429681366334, 0.24289429636824728, 0.24289429597083886, 0.2428942956233389, 0.2428942953274447, 0.24289429508463586, 0.24289429489616202, 0.2428942947630319, 0.24289429468600457, 0.24289429466558238, 0.24289429470200632, 0.24289429479525332, 0.24289429494503553, 0.24289429515080227, 0.24289429541174357, 0.2428942957267963, 0.24289429609465207, 0.2428942965137672, 0.2428942969823745, 0.24289429749849684, 0.2428942980599621, 0.2428942986644197, 0.24289429930935813, 0.2428942999921238], [0.24289431669685507, 0.24289431602262254, 0.2428943153389643, 0.24289431464708727, 0.2428943139483074, 0.24289431324404942, 0.24289431253584576, 0.2428943118253343, 0.2428943111142551, 0.24289431040444623, 0.24289430969783826, 0.2428943089964476, 0.24289430830236902, 0.24289430761776656, 0.24289430694486372, 0.2428943062859324, 0.24289430564328088, 0.24289430501924097, 0.2428943044161541, 0.24289430383635682, 0.24289430328216588, 0.2428943027558624, 0.24289430225967615, 0.2428943017957693, 0.24289430136622053, 0.24289430097300893, 0.24289430061799863, 0.2428943003029235, 0.2428943000293731, 0.24289429979877886, 0.24289429961240178, 0.24289429947132113, 0.24289429937642446, 0.24289429932839898, 0.2428942993277247, 0.242894299374669, 0.24289429946928315, 0.2428942996114004, 0.24289429980063595, 0.2428943000363888, 0.24289430031784537, 0.24289430064398476, 0.2428943010135859, 0.2428943014252363, 0.24289430187734215, 0.24289430236814014, 0.2428943028957101, 0.24289430345798937, 0.24289430405278767, 0.24289430467780293], [0.24289431538879458, 0.24289431481993978, 0.24289431424942065, 0.24289431367823072, 0.24289431310742524, 0.24289431253812158, 0.2428943119714988, 0.24289431140879683, 0.24289431085131485, 0.24289431030040914, 0.24289430975749002, 0.242894309224018, 0.24289430870149928, 0.24289430819148036, 0.24289430769554168, 0.2428943072152907, 0.24289430675235402, 0.24289430630836883, 0.24289430588497365, 0.24289430548379845, 0.24289430510645418, 0.24289430475452192, 0.24289430442954163, 0.2428943041330007, 0.24289430386632224, 0.24289430363085363, 0.242894303427855, 0.2428943032584881, 0.24289430312380553, 0.24289430302474055, 0.24289430296209774, 0.24289430293654413, 0.24289430294860162, 0.24289430299864026, 0.2428943030868727, 0.24289430321335, 0.24289430337795856, 0.2428943035804185, 0.2428943038202835, 0.24289430409694196, 0.2428943044096195, 0.24289430475738316, 0.24289430513914648, 0.2428943055536764, 0.2428943059996009, 0.2428943064754183, 0.24289430697950698, 0.24289430751013674, 0.24289430806548032, 0.24289430864362602], [0.24289431252708651, 0.24289431206548162, 0.2428943116126984, 0.2428943111694981, 0.24289431073664167, 0.2428943103148905, 0.2428943099050076, 0.24289430950775825, 0.24289430912391094, 0.2428943087542376, 0.24289430839951415, 0.2428943080605202, 0.24289430773803886, 0.24289430743285556, 0.24289430714575688, 0.24289430687752878, 0.2428943066289541, 0.24289430640080958, 0.24289430619386276, 0.24289430600886763, 0.24289430584656033, 0.24289430570765427, 0.24289430559283462, 0.2428943055027527, 0.24289430543801996, 0.2428943053992016, 0.2428943053868104, 0.24289430540130005, 0.24289430544305904, 0.2428943055124043, 0.24289430560957526, 0.24289430573472826, 0.24289430588793143, 0.2428943060691601, 0.24289430627829278, 0.24289430651510796, 0.2428943067792817, 0.24289430707038592, 0.2428943073878877, 0.2428943077311496, 0.2428943080994306, 0.24289430849188842, 0.2428943089075825, 0.24289430934547795, 0.24289430980445045, 0.242894310283292, 0.24289431078071738, 0.24289431129537145, 0.24289431182583684, 0.24289431237064257], [0.24289430790849326, 0.24289430754961192, 0.24289430721293984, 0.24289430689901087, 0.2428943066082812, 0.24289430634113118, 0.24289430609786777, 0.24289430587872735, 0.2428943056838794, 0.24289430551343036, 0.24289430536742782, 0.242894305245865, 0.24289430514868543, 0.2428943050757875, 0.24289430502702924, 0.24289430500223255, 0.24289430500118767, 0.24289430502365697, 0.24289430506937854, 0.24289430513806942, 0.24289430522942823, 0.24289430534313733, 0.24289430547886443, 0.24289430563626374, 0.24289430581497665, 0.24289430601463155, 0.24289430623484365, 0.242894306475214, 0.2428943067353283, 0.24289430701475523, 0.2428943073130446, 0.24289430762972536, 0.24289430796430347, 0.24289430831625952, 0.24289430868504672, 0.24289430907008863, 0.2428943094707775, 0.24289430988647256, 0.2428943103164986, 0.24289431076014523, 0.24289431121666621, 0.24289431168527953, 0.24289431216516774, 0.24289431265547903, 0.2428943131553286, 0.2428943136638007, 0.24289431417995114, 0.24289431470281014, 0.242894315231386, 0.2428943157646686], [0.24289430167608594, 0.24289430140418367, 0.24289430117006106, 0.24289430097406023, 0.24289430081635777, 0.2428943006969675, 0.24289430061574438, 0.24289430057238978, 0.24289430056645803, 0.24289430059736392, 0.24289430066439166, 0.242894300766704, 0.2428943009033528, 0.24289430107328946, 0.24289430127537645, 0.2428943015083986, 0.24289430177107466, 0.24289430206206872, 0.24289430238000173, 0.2428943027234623, 0.24289430309101726, 0.2428943034812217, 0.24289430389262825, 0.24289430432379555, 0.2428943047732962, 0.24289430523972358, 0.24289430572169796, 0.24289430621787156, 0.242894306726933, 0.24289430724761057, 0.24289430777867477, 0.2428943083189402, 0.2428943088672665, 0.24289430942255874, 0.2428943099837671, 0.24289431054988622, 0.2428943111199538, 0.24289431169304918, 0.24289431226829128, 0.24289431284483678, 0.24289431342187767, 0.24289431399863914, 0.2428943145743774, 0.24289431514837742, 0.24289431571995104, 0.2428943162884351, 0.24289431685318985, 0.24289431741359746, 0.24289431796906125, 0.2428943185190045], [0.24289429439516302, 0.2428942941825932, 0.2428942940241045, 0.24289429391992282, 0.24289429387001868, 0.24289429387411055, 0.2428942939316697, 0.2428942940419279, 0.24289429420388645, 0.24289429441632757, 0.24289429467782717, 0.24289429498676912, 0.24289429534136117, 0.24289429573965152, 0.2428942961795466, 0.24289429665882944, 0.2428942971751784, 0.24289429772618612, 0.24289429830937867, 0.2428942989222341, 0.24289429956220102, 0.24289430022671613, 0.24289430091322128, 0.24289430161917958, 0.24289430234209022, 0.2428943030795023, 0.2428943038290275, 0.24289430458835112, 0.24289430535524198, 0.2428943061275609, 0.24289430690326777, 0.242894307680427, 0.24289430845721213, 0.24289430923190872, 0.2428943100029161, 0.24289431076874807, 0.24289431152803262, 0.24289431227951036, 0.24289431302203243, 0.24289431375455747, 0.2428943144761481, 0.24289431518596657, 0.2428943158832704, 0.24289431656740734, 0.24289431723781021, 0.24289431789399174, 0.24289431853553908, 0.24289431916210852, 0.24289431977342038, 0.24289432036925365], [0.24289428678678737, 0.24289428659376996, 0.24289428647026298, 0.2428942864165077, 0.2428942864324073, 0.24289428651752948, 0.24289428667111243, 0.2428942868920729, 0.24289428717901768, 0.24289428753025705, 0.24289428794382098, 0.24289428841747723, 0.24289428894875148, 0.24289428953494904, 0.2428942901731781, 0.2428942908603737, 0.242894291593323, 0.24289429236869067, 0.24289429318304478, 0.2428942940328824, 0.242894294914655, 0.2428942958247933, 0.2428942967597312, 0.24289429771592871, 0.24289429868989357, 0.2428942996782017, 0.24289430067751566, 0.2428943016846018, 0.24289430269634554, 0.2428943037097648, 0.24289430472202142, 0.2428943057304309, 0.24289430673247028, 0.24289430772578402, 0.2428943087081882, 0.24289430967767306, 0.24289431063240385, 0.2428943115707202, 0.24289431249113427, 0.24289431339232725, 0.24289431427314526, 0.24289431513259377, 0.2428943159698315, 0.24289431678416343, 0.24289431757503332, 0.24289431834201572, 0.2428943190848077, 0.24289431980322024, 0.24289432049716966, 0.24289432116666906], [0.24289427915139422, 0.24289427892305196, 0.24289427877801303, 0.2428942787167465, 0.24289427873931232, 0.24289427884536235, 0.24289427903414457, 0.24289427930451088, 0.24289427965492835, 0.24289428008349342, 0.24289428058794907, 0.24289428116570522, 0.24289428181386102, 0.24289428252922984, 0.2428942833083659, 0.2428942841475927, 0.24289428504303276, 0.2428942859906379, 0.24289428698622065, 0.24289428802548552, 0.24289428910406036, 0.24289429021752723, 0.24289429136145257, 0.24289429253141634, 0.24289429372304003, 0.2428942949320131, 0.24289429615411762, 0.2428942973852511, 0.24289429862144754, 0.24289429985889593, 0.24289430109395685, 0.24289430232317663, 0.24289430354329938, 0.2428943047512767, 0.24289430594427508, 0.24289430711968113, 0.24289430827510497, 0.2428943094083812, 0.24289431051756832, 0.24289431160094643, 0.24289431265701314, 0.24289431368447806, 0.24289431468225625, 0.2428943156494603, 0.24289431658539162, 0.24289431748953064, 0.24289431836152695, 0.24289431920118815, 0.24289432000846922, 0.2428943207834609], [0.24289427076362635, 0.24289427042255413, 0.24289427017794474, 0.24289427003082156, 0.24289426998173505, 0.2428942700307592, 0.24289427017749177, 0.242894270421059, 0.24289427076012352, 0.24289427119289703, 0.24289427171715608, 0.24289427233026142, 0.24289427302918098, 0.24289427381051526, 0.24289427467052585, 0.2428942756051661, 0.24289427661011362, 0.2428942776808046, 0.24289427881246922, 0.2428942800001677, 0.24289428123882711, 0.24289428252327777, 0.24289428384828982, 0.24289428520860842, 0.24289428659898868, 0.2428942880142285, 0.24289428944920033, 0.24289429089888082, 0.2428942923583783, 0.24289429382295818, 0.24289429528806578, 0.2428942967493466, 0.24289429820266423, 0.24289429964411524, 0.24289430107004173, 0.24289430247704105, 0.24289430386197305, 0.2428943052219649, 0.24289430655441316, 0.2428943078569842, 0.24289430912761187, 0.24289431036449358, 0.24289431156608454, 0.24289431273109036, 0.2428943138584581, 0.24289431494736627, 0.24289431599721373, 0.24289431700760758, 0.24289431797835054, 0.24289431890942775], [0.24289425961370723, 0.2428942590455376, 0.24289425858914632, 0.2428942582465915, 0.2428942580193878, 0.24289425790849434, 0.24289425791430744, 0.24289425803665807, 0.24289425827481392, 0.24289425862748668, 0.24289425909284337, 0.24289425966852257, 0.24289426035165496, 0.2428942611388877, 0.24289426202641268, 0.24289426300999825, 0.2428942640850238, 0.24289426524651708, 0.24289426648919374, 0.24289426780749837, 0.24289426919564716, 0.24289427064767108, 0.24289427215745946, 0.24289427371880357, 0.24289427532543953, 0.24289427697109034, 0.2428942786495065, 0.24289428035450492, 0.2428942820800058, 0.2428942838200672, 0.24289428556891704, 0.24289428732098237, 0.24289428907091545, 0.24289429081361724, 0.2428942925442573, 0.2428942942582907, 0.24289429595147186, 0.24289429761986514, 0.24289429925985234, 0.2428943008681374, 0.24289430244174817, 0.24289430397803546, 0.2428943054746699, 0.2428943069296361, 0.24289430834122508, 0.2428943097080247, 0.2428943110289084, 0.24289431230302266, 0.24289431352977306, 0.24289431470880968], [0.2428942425060136, 0.2428942415224165, 0.24289424067087753, 0.24289423995519693, 0.2428942393785435, 0.24289423894342924, 0.2428942386516897, 0.24289423850446984, 0.24289423850221578, 0.2428942386446727, 0.24289423893088832, 0.24289423935922286, 0.24289423992736425, 0.24289424063234918, 0.24289424147058958, 0.24289424243790372, 0.2428942435295523, 0.24289424474027835, 0.24289424606435084, 0.24289424749561167, 0.24289424902752485, 0.24289425065322792, 0.24289425236558476, 0.24289425415723892, 0.24289425602066791, 0.2428942579482363, 0.24289425993224856, 0.2428942619650004, 0.2428942640388283, 0.24289426614615656, 0.242894268279542, 0.24289427043171555, 0.24289427259562052, 0.24289427476444753, 0.2428942769316654, 0.24289427909104885, 0.24289428123670173, 0.24289428336307672, 0.24289428546499098, 0.24289428753763823, 0.24289428957659676, 0.2428942915778344, 0.24289429353770992, 0.24289429545297092, 0.24289429732074963, 0.24289429913855523, 0.24289430090426414, 0.24289430261610845, 0.24289430427266168, 0.2428943058728239], [0.2428942143573613, 0.24289421256606455, 0.242894210930433, 0.24289420945720808, 0.24289420815242502, 0.24289420702136846, 0.2428942060685337, 0.24289420529759467, 0.2428942047113787, 0.2428942043118487, 0.24289420410009233, 0.24289420407631898, 0.24289420423986385, 0.24289420458919936, 0.24289420512195384, 0.2428942058349368, 0.24289420672417059, 0.24289420778492846, 0.2428942090117773, 0.24289421039862621, 0.2428942119387784, 0.24289421362498742, 0.24289421544951603, 0.24289421740419725, 0.2428942194804972, 0.2428942216695783, 0.2428942239623635, 0.242894226349599, 0.24289422882191675, 0.24289423136989488, 0.2428942339841159, 0.24289423665522253, 0.24289423937397037, 0.24289424213127683, 0.24289424491826703, 0.2428942477263151, 0.24289425054708186, 0.24289425337254789, 0.24289425619504276, 0.2428942590072697, 0.24289426180232623, 0.2428942645737205, 0.24289426731538385, 0.2428942700216795, 0.24289427268740743, 0.24289427530780636, 0.24289427787855203, 0.24289428039575292, 0.24289428285594328, 0.24289428525607373], [0.24289416287472243, 0.24289415920302793, 0.2428941556977057, 0.24289415237120518, 0.2428941492353846, 0.2428941463014379, 0.24289414357982655, 0.24289414108021667, 0.2428941388114221, 0.24289413678135396, 0.24289413499697707, 0.24289413346427324, 0.24289413218821218, 0.24289413117272965, 0.24289413042071317, 0.24289412993399517, 0.2428941297133533, 0.24289412975851804, 0.2428941300681869, 0.2428941306400453, 0.24289413147079356, 0.2428941325561792, 0.24289413389103492, 0.24289413546932112, 0.24289413728417247, 0.2428941393279484, 0.24289414159228662, 0.2428941440681595, 0.2428941467459324, 0.24289414961542377, 0.24289415266596642, 0.2428941558864697, 0.24289415926548186, 0.24289416279125237, 0.24289416645179385, 0.2428941702349434, 0.24289417412842254, 0.242894178119896, 0.24289418219702885, 0.2428941863475417, 0.24289419055926384, 0.2428941948201841, 0.24289419911849905, 0.2428942034426589, 0.24289420778141027, 0.24289421212383613, 0.24289421645939274, 0.24289422077794343, 0.242894225069789, 0.24289422932569507], [0.2428940485172941, 0.24289403957393738, 0.2428940307347714, 0.24289402202616625, 0.2428940134748577, 0.2428940051078294, 0.2428939969521894, 0.24289398903504184, 0.2428939813833541, 0.24289397402382026, 0.24289396698272128, 0.242893960285783, 0.24289395395803254, 0.2428939480236537, 0.2428939425058431, 0.24289393742666718, 0.24289393280692148, 0.2428939286659935, 0.24289392502172982, 0.24289392189030914, 0.24289391928612217, 0.2428939172216594, 0.24289391570740876, 0.24289391475176347, 0.2428939143609416, 0.2428939145389188, 0.2428939152873746, 0.24289391660565388, 0.24289391849074352, 0.242893920937266, 0.24289392393748932, 0.24289392748135447, 0.2428939315565201, 0.24289393614842464, 0.24289394124036542, 0.24289394681359458, 0.24289395284743076, 0.24289395931938595, 0.2428939662053065, 0.24289397347952663, 0.24289398111503335, 0.24289398908364124, 0.2428939973561749, 0.2428940059026578, 0.2428940146925053, 0.24289402369471996, 0.2428940328780871, 0.24289404221136893, 0.2428940516634946, 0.24289406120374565], [0.24289374372241382, 0.24289371853938369, 0.24289369312330322, 0.24289366754059502, 0.2428936418621099, 0.24289361616293134, 0.24289359052213072, 0.2428935650224716, 0.24289353975006345, 0.24289351479396387, 0.24289349024573156, 0.24289346619893043, 0.2428934427485883, 0.24289341999061279, 0.24289339802116922, 0.24289337693602436, 0.2428933568298631, 0.2428933377955831, 0.24289331992357543, 0.24289330330099887, 0.24289328801105511, 0.2428932741322747, 0.24289326173782153, 0.24289325089482505, 0.24289324166374945, 0.2428932340978077, 0.24289322824242926, 0.24289322413478923, 0.24289322180340553, 0.24289322126781107, 0.2428932225383058, 0.24289322561579293, 0.24289323049170272, 0.2428932371480054, 0.2428932455573144, 0.2428932556830784, 0.24289326747986129, 0.24289328089370565, 0.24289329586257627, 0.24289331231687789, 0.2428933301800398, 0.24289334936916113, 0.242893369795707, 0.2428933913662479, 0.24289341398323192, 0.24289343754578044, 0.24289346195049774, 0.24289348709228425, 0.2428935128651437, 0.24289353916297585], [0.24289286524667397, 0.24289278937068462, 0.24289271197244383, 0.24289263323923013, 0.242892553377585, 0.24289247261301142, 0.24289239118945966, 0.2428923093685919, 0.24289222742881783, 0.24289214566409772, 0.24289206438251276, 0.24289198390460462, 0.24289190456149182, 0.24289182669277296, 0.24289175064423146, 0.24289167676536058, 0.24289160540673058, 0.24289153691722462, 0.24289147164117258, 0.24289140991541552, 0.2428913520663358, 0.24289129840689008, 0.24289124923368402, 0.24289120482412827, 0.24289116543371495, 0.24289113129345422, 0.2428911026075081, 0.24289107955105754, 0.24289106226843493, 0.24289105087155163, 0.24289104543864562, 0.24289104601336936, 0.24289105260423355, 0.24289106518441655, 0.2428910836919435, 0.24289110803023373, 0.2428911380690091, 0.24289117364555013, 0.24289121456628168, 0.24289126060866478, 0.24289131152336688, 0.24289136703667813, 0.24289142685313936, 0.24289149065834345, 0.24289155812187058, 0.24289162890031657, 0.24289170264037355, 0.2428917789819219, 0.24289185756109444, 0.24289193801327436], [0.24289034057230635, 0.24289011062247753, 0.24288987470155315, 0.24288963335316158, 0.2428893871908467, 0.24288913689788305, 0.24288888322631666, 0.24288862699518532, 0.242888369087879, 0.2428881104486141, 0.24288785207800678, 0.2428875950277443, 0.24288734039436924, 0.24288708931220568, 0.24288684294547408, 0.2428866024796568, 0.24288636911219358, 0.2428861440426012, 0.24288592846212656, 0.24288572354305596, 0.24288553042781474, 0.24288535021800176, 0.24288518396350905, 0.2428850326518827, 0.2428848971980818, 0.24288477843479103, 0.24288467710343764, 0.2428845938460551, 0.24288452919812642, 0.2428844835825239, 0.24288445730464742, 0.24288445054884406, 0.24288446337617045, 0.24288449572353804, 0.2428845474042576, 0.24288461810997636, 0.2428847074139768, 0.24288481477578439, 0.24288493954700957, 0.24288508097832892, 0.24288523822749303, 0.2428854103682338, 0.24288559639993032, 0.24288579525788348, 0.2428860058240435, 0.24288622693803072, 0.2428864574082908, 0.2428866960232274, 0.24288694156216356, 0.24288719280598892], [0.24288336379106618, 0.24288268579281053, 0.24288198738482672, 0.2428812701066849, 0.24288053573182802, 0.242879786269323, 0.24287902396300995, 0.24287825128784413, 0.24287747094325712, 0.24287668584339991, 0.242875899104176, 0.24287511402702072, 0.24287433407943776, 0.24287356287236295, 0.24287280413448614, 0.24287206168372552, 0.24287133939611283, 0.24287064117240914, 0.24286997090283197, 0.24286933243032724, 0.2428687295128706, 0.24286816578532158, 0.2428676447213866, 0.24286716959626864, 0.2428667434505889, 0.24286636905616543, 0.24286604888421695, 0.2428657850765318, 0.24286557942010228, 0.24286543332567112, 0.24286534781057387, 0.24286532348618903, 0.24286536055022606, 0.24286545878399599, 0.2428656175547196, 0.24286583582283597, 0.24286611215418422, 0.24286644473684355, 0.24286683140233423, 0.24286726965080793, 0.2428677566797883, 0.24286828941596897, 0.2428688645495309, 0.24286947857040983, 0.2428701278059266, 0.24287080845918646, 0.2428715166476611, 0.24287224844138522, 0.24287299990023012, 0.2428737671097551], [0.2428651082382363, 0.24286319663885436, 0.2428612205365562, 0.24285918406877208, 0.24285709210762738, 0.24285495027258652, 0.24285276493490657, 0.2428505432130957, 0.2428482929586658, 0.24284602273158268, 0.2428437417649621, 0.2428414599187231, 0.24283918762209772, 0.2428369358051011, 0.2428347158192861, 0.24283253934833626, 0.24283041830928637, 0.2428283647453936, 0.24282639071190834, 0.2428245081562065, 0.24282272879393657, 0.24282106398299863, 0.24281952459730216, 0.24281812090234353, 0.24281686243468917, 0.2428157578874535, 0.2428148150038133, 0.24281404048050154, 0.24281343988307927, 0.2428130175745922, 0.2428127766589845, 0.2428127189403725, 0.2428128448989813, 0.24281315368422504, 0.2428136431250773, 0.2428143097575387, 0.2428151488686741, 0.24281615455637306, 0.24281731980368884, 0.2428186365663453, 0.24282009587177023, 0.24282168792782768, 0.24282340223928062, 0.2428252277299224, 0.24282715286827575, 0.24282916579476158, 0.24283125444829673, 0.24283340669037434, 0.24283561042481733, 0.24283785371156394], [0.24282033727823787, 0.242815241708186, 0.2428099561015259, 0.24280449084152414, 0.24279885847593097, 0.24279307377550385, 0.24278715376847493, 0.24278111774806543, 0.2427749872503932, 0.24276878600045543, 0.24276253982429155, 0.24275627652594814, 0.24275002572846532, 0.24274381867878025, 0.24273768801718237, 0.2427316675127435, 0.2427257917669645, 0.2427200958887083, 0.24271461514430198, 0.24270938458746394, 0.24270443867442018, 0.2426998108701897, 0.24269553325251922, 0.24269163612031042, 0.24268814761358817, 0.24268509335209434, 0.24268249609944598, 0.24268037545947124, 0.24267874761082694, 0.24267762508532623, 0.2426770165945721, 0.24267692690852824, 0.2426773567885878, 0.24267830297655557, 0.24267975823977106, 0.24268171147140644, 0.2426841478438104, 0.24268704901166782, 0.2426903933607407, 0.24269415629707528, 0.24269831057082625, 0.24270282662827775, 0.24270767298524865, 0.24271281661485553, 0.24271822334257465, 0.24272385824168355, 0.24272968602246495, 0.24273567140899935, 0.2427417794979412, 0.24274797609434137], [0.2427185447109651, 0.242705834164859, 0.24269260445297014, 0.24267887960883833, 0.2426646896011637, 0.2426500705536219, 0.2426350648992718, 0.24261972145999341, 0.24260409544194886, 0.24258824833891068, 0.24257224773646824, 0.24255616701160318, 0.24254008492391235, 0.24252408509682924, 0.24250825538952192, 0.2424926871626745, 0.2424774744440363, 0.24246271300236982, 0.24244849934116672, 0.24243492962613875, 0.24242209856293215, 0.24241009824366566, 0.24239901698266003, 0.242388938163029, 0.24237993911656341, 0.24237209005950947, 0.2423654531063817, 0.24236008138285015, 0.24235601825701056, 0.2423532967060252, 0.2423519388322716, 0.2423519555398381, 0.24235334637856756, 0.24235609955898593, 0.24236019213748902, 0.24236559036723349, 0.24237225020640743, 0.24238011797207246, 0.24238913112467186, 0.242399219165685, 0.24241030462884286, 0.24242230414385102, 0.2424351295507172, 0.24244868904254896, 0.24246288831504828, 0.24247763170184436, 0.24249282327620503, 0.24250836790148755, 0.24252417221483685, 0.2425401455310361], [0.24250693147986496, 0.24247758823047239, 0.2424469427576668, 0.24241504571843342, 0.24238196276053878, 0.24234777523035173, 0.2423125807179001, 0.24227649341041974, 0.24223964422661795, 0.2422021807057514, 0.2421642666284763, 0.2421260813503056, 0.2420878188334089, 0.24204968636837956, 0.2420119029843869, 0.24197469755369652, 0.2419383066046989, 0.24190297186610416, 0.24186893757356773, 0.2418364475783998, 0.24180574230585222, 0.24177705561743085, 0.2417506116374203, 0.24172662160802638, 0.24170528083998194, 0.2416867658259246, 0.24167123158222134, 0.2416588092811477, 0.24164960422949494, 0.24164369424192572, 0.24164112844798097, 0.2416419265608791, 0.24164607862453746, 0.2416535452430176, 0.241664258284311, 0.241678122038491, 0.24169501479919317, 0.2417147908275297, 0.2417372826492137, 0.2417623036291038, 0.2417896507627343, 0.24181910762173425, 0.241850447389342, 0.24188343592338704, 0.24191783478697657, 0.24195340419146577, 0.24198990580185048, 0.24202710536122077, 0.2420647750980577, 0.2421026958876681], [0.24211207694209516, 0.2420501885219257, 0.24198533932887364, 0.2419176250614371, 0.24184717581820311, 0.24177415808064914, 0.2416987763318921, 0.24162127423362686, 0.24154193528421036, 0.24146108288403978, 0.24137907974033276, 0.2412963265523644, 0.24121325993027407, 0.24113034951572127, 0.24104809429080676, 0.24096701808246154, 0.24088766429246586, 0.24081058990771745, 0.24073635887049377, 0.24066553491326337, 0.2405986739860091, 0.2405363164248909, 0.24047897902825888, 0.24042714721848202, 0.24038126747487604, 0.2403417402235312, 0.24030891336366741, 0.24028307659723508, 0.2402644567091374, 0.24025321392034696, 0.24024943940631047, 0.24025315403965256, 0.24026430838076154, 0.2402827839039333, 0.24030839541193189, 0.24034089455956978, 0.24037997437850195, 0.24042527467189573, 0.24047638812970057, 0.24053286700328974, 0.2405942301723411, 0.2406599704367221, 0.24072956187134784, 0.24080246709179215, 0.2408781442920123, 0.24095605393199876, 0.2410356649715666, 0.2411164605659893, 0.24119794315895263, 0.24127963892768348], [0.2414677576301084, 0.24135037017567826, 0.24122698320855743, 0.24109775581881687, 0.2409629174523247, 0.24082277270161503, 0.24067770538387442, 0.24052818172168455, 0.24037475243913406, 0.2402180535886308, 0.24005880593319837, 0.23989781272609442, 0.23973595575491025, 0.2395741895512744, 0.2394135337098758, 0.23925506331125304, 0.2390998975006269, 0.23894918633834633, 0.23880409610402506, 0.23866579330336624, 0.23853542769072317, 0.2384141146780575, 0.23830291754846047, 0.23820282992633018, 0.23811475897363393, 0.23803950978019583, 0.2379777713944271, 0.2379301048993652, 0.23789693387864266, 0.23787853754069224, 0.23787504668090126, 0.237886442565234, 0.23791255872026099, 0.23795308551890493, 0.2380075773635504, 0.23807546219278417, 0.23815605297821227, 0.2382485608355963, 0.23835210935069134, 0.23846574971414417, 0.23858847627001162, 0.23871924210643758, 0.23885697435181188, 0.23900058888211712, 0.23914900419200708, 0.23930115423060616, 0.2394560000506725, 0.23961254016478498, 0.23976981954330556, 0.23992693722528027], [0.24058330078866771, 0.24038685903732268, 0.24017983045444996, 0.2399624465300693, 0.23973506387148, 0.23949817386538677, 0.23925241113872195, 0.2389985604507951, 0.23873756163495174, 0.2384705122042978, 0.23819866724574632, 0.23792343625211065, 0.2376463765853733, 0.23736918332727924, 0.23709367535695813, 0.2368217775992267, 0.2365554995100915, 0.23629691000470546, 0.23604810918283348, 0.23581119736119371, 0.2355882420726778, 0.2353812438300018, 0.23519210156569997, 0.23502257874162177, 0.2348742711603715, 0.23474857750171915, 0.23464667354525165, 0.23456949092657753, 0.23451770111263498, 0.23449170508063685, 0.23449162895713255, 0.23451732563340957, 0.23456838213707087, 0.23464413232284836, 0.23474367426232431, 0.23486589157274337, 0.23500947783576057, 0.23517296321939585, 0.23535474242778665, 0.2355531031568646, 0.23576625432043752, 0.23599235341947, 0.2362295325466294, 0.23647592263837985, 0.23672967569989048, 0.23698898482795164, 0.23725210194060095, 0.23751735318821046, 0.23778315207020262, 0.238048010316542], [0.23962514568067245, 0.23934111593706803, 0.23904127225636912, 0.23872592220432043, 0.23839556221830743, 0.23805089308385244, 0.2376928336133427, 0.23732253193318226, 0.2369413737568447, 0.2365509870097083, 0.23615324218153058, 0.235750247818457, 0.23534434063280085, 0.23493806980905768, 0.23453417522115913, 0.23413555944952227, 0.23374525369534507, 0.23336637792921566, 0.23300209587336315, 0.23265556569000292, 0.23232988751682135, 0.2320280492363019, 0.2317528720679828, 0.23150695771137395, 0.2312926388236002, 0.23111193457577633, 0.23096651288834139, 0.23085766069949554, 0.23078626328372062, 0.23075279323003156, 0.23075730924123938, 0.23079946446065303, 0.2308785236075265, 0.2309933878407641, 0.2311426259988442, 0.2313245106996834, 0.23153705773257902, 0.2317780672289889, 0.23204516524292018, 0.23233584458088288, 0.23264750396763653, 0.232977484889306, 0.23332310569522796, 0.23368169274541464, 0.23405060855029394, 0.2344272769593401, 0.2348092055177824, 0.23519400513319477, 0.23557940718685297, 0.2359632781998452], [0.23891014137680375, 0.2385612105673784, 0.23819274477859298, 0.23780514147880663, 0.23739903620192152, 0.23697532145812478, 0.23653516314114714, 0.23608001366103618, 0.235611621005874, 0.2351320329356671, 0.23464359554237954, 0.23414894547710094, 0.23365099525327868, 0.23315291118719916, 0.2326580837350054, 0.232170090228427, 0.23169265029469385, 0.2312295745614472, 0.23078470758205474, 0.2303618662531948, 0.22996477531331347, 0.2295970017828134, 0.22926189040838935, 0.22896250227967993, 0.22870155877532966, 0.2284813928541439, 0.2283039094322741, 0.22817055618911622, 0.2280823056463613, 0.22803964880251198, 0.22804260002487764, 0.22809071235288292, 0.22818310189921162, 0.2283184796895999, 0.22849518908504335, 0.22871124689148845, 0.22896438637286268, 0.22925210061786533, 0.22957168503098704, 0.229920278078862, 0.23029489977934667, 0.23069248773381557, 0.23110993074484734, 0.23154410021666194, 0.23199187960263293, 0.23245019215283147, 0.2329160271431961, 0.23338646465952975, 0.23385869888781663, 0.23433005974859364], [0.23872585383667005, 0.23836274256623968, 0.23797965978405464, 0.23757707658076452, 0.2371557117939778, 0.23671654970595055, 0.23626085474050793, 0.2357901823713031, 0.2353063854533174, 0.23481161522090793, 0.23430831626364862, 0.2337992148996329, 0.23328730051720334, 0.23277579965047654, 0.2322681427890687, 0.23176792419227435, 0.2312788552732025, 0.23080471242572323, 0.23034928046961542, 0.22991629316711826, 0.229509372495341, 0.22913196852165849, 0.22878730180319368, 0.22847831020104548, 0.22820760185627753, 0.22797741581823505, 0.22778959145768674, 0.22764554735989867, 0.2275462699084796, 0.22749231127965183, 0.22748379611177308, 0.22752043573793548, 0.22760154860458742, 0.2277260853683632, 0.2278926571732184, 0.22809956575024382, 0.22834483422738597, 0.22862623784823782, 0.2289413341341795, 0.2292874923387698, 0.22966192229976148, 0.2300617029658392, 0.23048381094934828, 0.23092514943432055, 0.23138257766513307, 0.23185294107846544, 0.23233310194822457, 0.2328199702186864, 0.23331053403092952, 0.23380188932129475], [0.23910537211489463, 0.23877950767737532, 0.23843611496174272, 0.23807563853003438, 0.23769873661868002, 0.23730629501417705, 0.23689943823787019, 0.23647953742259697, 0.23604821428083544, 0.2356073406072035, 0.2351590328304123, 0.2347056412326545, 0.23424973358794843, 0.23379407313329062, 0.23334159097355583, 0.23289535322643945, 0.23245852342855097, 0.23203432093697784, 0.23162597625947778, 0.23123668441722459, 0.23086955757303212, 0.23052757823288264, 0.2302135543397242, 0.22993007752026634, 0.22967948561752266, 0.22946383044970997, 0.2292848514916396, 0.22914395589534317, 0.2290422049741393, 0.22898030699269326, 0.22895861585853397, 0.22897713511844792, 0.22903552654084844, 0.22913312251934204, 0.22926894156108119, 0.22944170621509524, 0.22964986293227607, 0.22989160350695906, 0.2301648879054033, 0.23046746841626586, 0.23079691514464987, 0.23115064290400644, 0.2315259395368741, 0.23191999562187274, 0.23232993541296004, 0.23275284872441512, 0.2331858233398973, 0.23362597740389093, 0.23407049116358405, 0.2345166373789258], [0.23981438530695956, 0.23955229436148548, 0.239276164701278, 0.23898631330308429, 0.23868321846889087, 0.23836753072585637, 0.23804008196047208, 0.23770189237225783, 0.23735417483861926, 0.23699833630333822, 0.23663597583791157, 0.23626887907842703, 0.23589900881105047, 0.23552849156584946, 0.23515960018007168, 0.23479473240561077, 0.23443638575775014, 0.2340871289289615, 0.23374957021731346, 0.23342632353814544, 0.2331199726939425, 0.2328330346647999, 0.23256792274493682, 0.23232691038483091, 0.23211209660051502, 0.23192537378002542, 0.23176839865263324, 0.2316425670922315, 0.2315489933071209, 0.23148849383124934, 0.2314615765847584, 0.23146843512301246, 0.23150894805132566, 0.23158268345440308, 0.23168890808022588, 0.231826600930614, 0.23199447084542255, 0.23219097762249793, 0.23241435618767944, 0.23266264331392178, 0.23293370638153973, 0.2332252736688444, 0.23353496566149512, 0.2338603268687049, 0.2341988576355699, 0.2345480454450002, 0.23490539521260798, 0.23526845809619018, 0.23563485837049972, 0.23600231795935364], [0.2405616083564794, 0.24036296673946453, 0.2401534142259548, 0.23993312512307383, 0.23970239124290324, 0.239461631425561, 0.23921140010458528, 0.23895239460890438, 0.2386854608829528, 0.2384115973012484, 0.23813195625752387, 0.23784784322283345, 0.2375607129933281, 0.2372721628877799, 0.23698392270827215, 0.23669784134508493, 0.23641586998840833, 0.23614004200403016, 0.23587244963558313, 0.23561521780933126, 0.23537047543481782, 0.23514032471102178, 0.23492680905720612, 0.23473188038408047, 0.23455736649779543, 0.2344049394805164, 0.23427608591166518, 0.23417207977954504, 0.2340939588820885, 0.23404250542824934, 0.23401823143092398, 0.23402136933342263, 0.23405186814169213, 0.23410939515255455, 0.2341933431838192, 0.23430284303498483, 0.2344367807463403, 0.23459381908714108, 0.2347724225957542, 0.23497088541957398, 0.23518736116111827, 0.2354198939279343, 0.23566644980488172, 0.23592494801381358, 0.23619329109264933, 0.2364693935079679, 0.23675120820732828, 0.23703675071470165, 0.23732412047046542, 0.23761151921288717], [0.24118918103716938, 0.24104198806311025, 0.2408863409204864, 0.24072231382781056, 0.24055006835150525, 0.240369861945608, 0.24018205597660405, 0.2399871229783735, 0.2397856528602848, 0.23957835777469622, 0.23936607534035564, 0.2391497699173463, 0.23893053163922656, 0.2387095729306515, 0.2384882222755671, 0.2382679150530951, 0.23805018132591518, 0.23783663054889365, 0.2376289332625155, 0.2374287999438339, 0.2372379573034701, 0.23705812243584723, 0.23689097534549938, 0.23673813047842035, 0.23660110797710443, 0.2364813054444591, 0.2363799710390935, 0.23629817872785708, 0.2362368064879752, 0.23619651817998202, 0.23617774970565886, 0.23618069992663537, 0.23620532665580374, 0.2362513478537456, 0.23631824797572057, 0.23640528923168364, 0.2365115273522262, 0.2366358313061196, 0.23677690629737463, 0.23693331928630024, 0.23710352623233333, 0.23728590024636434, 0.23747875986454473, 0.23768039670992314, 0.23788910188712512, 0.23810319055226023, 0.23832102420865273, 0.23854103039240984, 0.2387617195244584, 0.2389816988125984], [0.241685457765255, 0.2415795616194928, 0.24146737751476763, 0.24134893606862368, 0.24122433225218223, 0.2410937322523485, 0.24095737999957065, 0.24081560315705297, 0.24066881834784384, 0.24051753538103174, 0.24036236022895086, 0.24020399650545302, 0.2400432452025434, 0.2398810024604617, 0.2397182551758167, 0.23955607429443354, 0.2393956056903778, 0.23923805859971778, 0.23908469165568816, 0.2389367966588653, 0.23879568030867115, 0.23866264421705258, 0.2385389636168756, 0.23842586526127352, 0.2383245050805365, 0.23823594621497382, 0.23816113807093608, 0.23810089704931758, 0.23805588956921397, 0.2380266179535336, 0.23801340965967405, 0.23801641023018522, 0.2380355802106856, 0.23807069614165335, 0.23812135558450556, 0.23818698599845686, 0.2382668571506832, 0.23836009662520932, 0.23846570790135127, 0.23858259040449825, 0.23870956089269468, 0.23884537553217092, 0.2389887520321879, 0.23913839125129885, 0.23929299774921958, 0.23945129883599803, 0.23961206175781738, 0.23977410875132854, 0.2399363297910516, 0.24009769294291886], [0.2420839202060437, 0.24201289622372696, 0.24193766718647125, 0.2418582587882801, 0.24177474056320172, 0.24168723040657009, 0.24159589882985752, 0.24150097280993216, 0.24140273908247736, 0.24130154672163054, 0.24119780884450995, 0.2410920032811888, 0.2409846720587413, 0.24087641956298023, 0.24076790926397068, 0.24065985892158093, 0.24055303422508897, 0.24044824086563327, 0.2403463150910231, 0.2402481128475509, 0.24015449767092328, 0.24006632754576893, 0.23998444100756874, 0.23990964280928337, 0.23984268951439738, 0.23978427540574765, 0.23973501911293943, 0.23969545135860898, 0.23966600420430448, 0.23964700214031168, 0.23963865531135878, 0.23964105510381367, 0.2396541722426619, 0.2396778574619137, 0.2397118447242987, 0.23975575687957454, 0.2398091135698031, 0.23987134111845612, 0.23994178408147065, 0.24001971809481018, 0.24010436362612694, 0.24019490022819664, 0.24029048089832422, 0.24039024616946297, 0.24049333759318764, 0.24059891031925162, 0.2407061445282928, 0.24081425553030072, 0.24092250239882107, 0.24103019506692183], [0.24239546691671354, 0.24235258682782496, 0.24230727234844524, 0.24225955222682222, 0.24220948141266282, 0.2421571434316466, 0.24210265255790236, 0.2420461557073663, 0.2419878339710916, 0.2419279037058307, 0.24186661709999058, 0.24180426213677572, 0.24174116188331005, 0.24167767304500784, 0.24161418373854618, 0.24155111045442387, 0.24148889420103223, 0.24142799584596722, 0.24136889069632295, 0.2413120623870632, 0.24125799617422242, 0.2412071717564487, 0.24116005577297522, 0.24111709414718205, 0.2410787044612139, 0.24104526855752603, 0.24101712556681687, 0.24099456555796286, 0.24097782399402826, 0.24096707715932403, 0.24096243869638667, 0.24096395735959597, 0.24097161605526193, 0.24098533219799087, 0.2410049593717755, 0.2410302902434267, 0.24106106063751376, 0.24109695464760358, 0.24113761062972972, 0.24118262790179965, 0.24123157395778483, 0.24128399199835326, 0.24133940858000205, 0.24139734119225387, 0.24145730558630554, 0.2415188226976047, 0.2415814250279696, 0.24164466237875323, 0.2417081068538757, 0.24177135707907474], [0.24261656209965388, 0.24259353984280813, 0.24256930215646702, 0.24254387394881335, 0.24251729363255556, 0.24248961412699102, 0.24246090373436677, 0.24243124685589168, 0.24240074451221982, 0.24236951463371395, 0.2423376920874555, 0.24230542841090244, 0.24227289122639092, 0.24224026331635173, 0.24220774134614512, 0.24217553422970844, 0.2421438611425974, 0.24211294919724619, 0.24208303080606844, 0.24205434076900004, 0.24202711313282768, 0.24200157787969917, 0.24197795751110454, 0.24195646360089756, 0.24193729339616774, 0.24192062654762667, 0.24190662205135957, 0.24189541548116067, 0.24188711658518489, 0.24188180731240622, 0.241879540323618, 0.24188033802880526, 0.24188419217814724, 0.2418910640182384, 0.2419008850089775, 0.2419135580806238, 0.24192895939540107, 0.24194694056434254, 0.24196733125833664, 0.24198994214297234, 0.24201456806008534, 0.24204099137503807, 0.24206898540774427, 0.2420983178671721, 0.2421287542132984, 0.24216006087692163, 0.24219200827597565, 0.24222437357657634, 0.24225694315751262, 0.24228951474780144]]}, {\"hoverinfo\": \"text\", \"legendgroup\": \"In-sample\", \"marker\": {\"color\": \"black\", \"opacity\": 0.5, \"symbol\": 1}, \"mode\": \"markers\", \"name\": \"In-sample\", \"text\": [\"0_0\", \"1_0\", \"2_0\", \"3_0\", \"4_0\", \"5_0\", \"6_0\", \"7_0\", \"8_0\", \"9_0\", \"10_0\", \"11_0\", \"12_0\", \"13_0\", \"14_0\", \"15_0\", \"16_0\", \"17_0\", \"18_0\", \"19_0\", \"20_0\", \"21_0\", \"22_0\", \"23_0\"], \"type\": \"scatter\", \"x\": [5, 43, 23, 45, 49, 43, 56, 34, 54, 18, 15, 38, 41, 42, 42, 41, 41, 40, 44, 38, 43, 42, 35, 46], \"xaxis\": \"x\", \"y\": [1.1034355229703196e-05, 4.105364690785442e-07, 0.014388260567730632, 0.024145430511910773, 1.053971740060733e-06, 0.004819812341693652, 3.616503292267924e-08, 6.533405868094214e-08, 3.886436376748688e-08, 1.2430719133522317e-08, 0.00012921158968318136, 9.683017012324612e-08, 0.009404069104362969, 0.007606856020702582, 0.0063379649975360474, 0.007421384415851708, 0.024050292831800598, 0.0062498785396539, 0.006671234132972485, 0.006993732048536658, 0.007270225627520383, 0.007139755710616323, 0.006940223310507879, 0.007321189680891077], \"yaxis\": \"y\"}, {\"hoverinfo\": \"text\", \"legendgroup\": \"In-sample\", \"marker\": {\"color\": \"black\", \"opacity\": 0.5, \"symbol\": 1}, \"mode\": \"markers\", \"name\": \"In-sample\", \"showlegend\": false, \"text\": [\"0_0\", \"1_0\", \"2_0\", \"3_0\", \"4_0\", \"5_0\", \"6_0\", \"7_0\", \"8_0\", \"9_0\", \"10_0\", \"11_0\", \"12_0\", \"13_0\", \"14_0\", \"15_0\", \"16_0\", \"17_0\", \"18_0\", \"19_0\", \"20_0\", \"21_0\", \"22_0\", \"23_0\"], \"type\": \"scatter\", \"x\": [5, 43, 23, 45, 49, 43, 56, 34, 54, 18, 15, 38, 41, 42, 42, 41, 41, 40, 44, 38, 43, 42, 35, 46], \"xaxis\": \"x2\", \"y\": [1.1034355229703196e-05, 4.105364690785442e-07, 0.014388260567730632, 0.024145430511910773, 1.053971740060733e-06, 0.004819812341693652, 3.616503292267924e-08, 6.533405868094214e-08, 3.886436376748688e-08, 1.2430719133522317e-08, 0.00012921158968318136, 9.683017012324612e-08, 0.009404069104362969, 0.007606856020702582, 0.0063379649975360474, 0.007421384415851708, 0.024050292831800598, 0.0062498785396539, 0.006671234132972485, 0.006993732048536658, 0.007270225627520383, 0.007139755710616323, 0.006940223310507879, 0.007321189680891077], \"yaxis\": \"y2\"}],\n",
              "                        {\"annotations\": [{\"font\": {\"size\": 14}, \"showarrow\": false, \"text\": \"Mean\", \"x\": 0.25, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 1, \"yanchor\": \"bottom\", \"yref\": \"paper\"}, {\"font\": {\"size\": 14}, \"showarrow\": false, \"text\": \"Standard Error\", \"x\": 0.8, \"xanchor\": \"center\", \"xref\": \"paper\", \"y\": 1, \"yanchor\": \"bottom\", \"yref\": \"paper\"}], \"autosize\": false, \"height\": 450, \"hovermode\": \"closest\", \"legend\": {\"orientation\": \"h\", \"x\": 0, \"y\": -0.25}, \"margin\": {\"b\": 100, \"l\": 35, \"pad\": 0, \"r\": 35, \"t\": 35}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"width\": 950, \"xaxis\": {\"anchor\": \"y\", \"autorange\": false, \"domain\": [0.05, 0.45], \"exponentformat\": \"e\", \"range\": [4.0, 64.0], \"tickfont\": {\"size\": 11}, \"tickmode\": \"auto\", \"title\": {\"text\": \"batch_size\"}, \"type\": \"linear\"}, \"xaxis2\": {\"anchor\": \"y2\", \"autorange\": false, \"domain\": [0.6, 1], \"exponentformat\": \"e\", \"range\": [4.0, 64.0], \"tickfont\": {\"size\": 11}, \"tickmode\": \"auto\", \"title\": {\"text\": \"batch_size\"}, \"type\": \"linear\"}, \"yaxis\": {\"anchor\": \"x\", \"autorange\": false, \"domain\": [0, 1], \"exponentformat\": \"e\", \"range\": [-8.0, -1.0], \"tickfont\": {\"size\": 11}, \"tickmode\": \"auto\", \"title\": {\"text\": \"lr\"}, \"type\": \"log\"}, \"yaxis2\": {\"anchor\": \"x2\", \"autorange\": false, \"domain\": [0, 1], \"exponentformat\": \"e\", \"range\": [-8.0, -1.0], \"tickfont\": {\"size\": 11}, \"tickmode\": \"auto\", \"type\": \"log\"}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('47cda26f-8ed0-442a-855a-77ae797cc425');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpPflZ0bUwcC"
      },
      "source": [
        "# Ax-platform Optimized Run"
      ]
    }
  ]
}